[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT112 Notebook",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "bw/bw-uni.html",
    "href": "bw/bw-uni.html",
    "title": "\n1  Univariate Viz\n",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary (sf)\nlibrary (maps)\nlibrary (RColorBrewer)\nlibrary (gplots)\nlibrary(socviz)\nlibrary (leaflet)\nlibrary (devtools)\nlibrary(ggthemes)\nlibrary(viridis)\n\n\n\nCodesurvey&lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/survey.csv\")\n\n\n\nCode#|fig_alt: \"This visualization is of a bar graph that represents students' favorite temperature in degrees Celcius. The biggest takeaway is that students most like the temperature between `15-25 degrees. Data from class survey- https://hash-mac.github.io/stat112site-s25/data/survey.csv\"\n\nggplot(survey, aes(x=fav_temp_c,))+\n  geom_bar(alpha= 0.75, width=2,fill=\"black\", color=\"white\")+\n  labs(x=\"Temperature (C)\", y=\"Frequency\", title= \"STAT 112 Students' Favorite Temperature\", caption= \"Data from: TidyTuesday Github Repository, Created by: Martha Miller, Date: 02/23/2025\")",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-bi.html",
    "href": "bw/bw-bi.html",
    "title": "\n2  Bivariate Viz\n",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary (sf)\nlibrary (maps)\nlibrary (RColorBrewer)\nlibrary (gplots)\nlibrary(socviz)\nlibrary (leaflet)\nlibrary (devtools)\nlibrary(ggthemes)\nlibrary(viridis)\n\n\n\nCodeelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n\n\nCode#|fig_alt: \"This visualization is of a scatter plot of republican vote percentage in 2016 and 2020. The biggest takeaway is that the two variables are have strong positive correlation. Data from elections-https://mac-stat.github.io/data/election_2020_county.csv\"\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()+\n  labs(x=\"Republican % Vote (2016)\", y=\"Republican % Vote (2020)\", title= \"Relationship between Republican Vote %\", caption= \"Data from: TidyTuesday Github Repository, Created by: Martha Miller, Date: 02/23/2025\")+\nscale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Martha Miller]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar-title\"}\n[COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar-title\"}\n[&lt;span class='chapter-number'&gt;3&lt;/span&gt;  &lt;span class='chapter-title'&gt;Trivariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-next\"}\n[&lt;span class='chapter-number'&gt;1&lt;/span&gt;  &lt;span class='chapter-title'&gt;Univariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-prev\"}\n[Welcome]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/index.htmlWelcome\"}\n[Best Work]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:quarto-sidebar-section-1\"}\n[&lt;span class='chapter-number'&gt;1&lt;/span&gt;  &lt;span class='chapter-title'&gt;Univariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-uni.html&lt;span-class='chapter-number'&gt;1&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Univariate-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;2&lt;/span&gt;  &lt;span class='chapter-title'&gt;Bivariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-bi.html&lt;span-class='chapter-number'&gt;2&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Bivariate-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;3&lt;/span&gt;  &lt;span class='chapter-title'&gt;Trivariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-tri.html&lt;span-class='chapter-number'&gt;3&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Trivariate-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;4&lt;/span&gt;  &lt;span class='chapter-title'&gt;Quadvariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-quad.html&lt;span-class='chapter-number'&gt;4&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Quadvariate-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;5&lt;/span&gt;  &lt;span class='chapter-title'&gt;Spatial Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/bw-spatial.html&lt;span-class='chapter-number'&gt;5&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Spatial-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;6&lt;/span&gt;  &lt;span class='chapter-title'&gt;Exam 1&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/exam1.html&lt;span-class='chapter-number'&gt;6&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Exam-1&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;7&lt;/span&gt;  &lt;span class='chapter-title'&gt;Solo Project&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/solo_project.html&lt;span-class='chapter-number'&gt;7&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Solo-Project&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;8&lt;/span&gt;  &lt;span class='chapter-title'&gt;Exam 2&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/bw/exam2.html&lt;span-class='chapter-number'&gt;8&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Exam-2&lt;/span&gt;\"}\n[Summary Sheets]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:quarto-sidebar-section-2\"}\n[&lt;span class='chapter-number'&gt;9&lt;/span&gt;  &lt;span class='chapter-title'&gt;Summary Sheet Exam 1&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/summary/ex1_summary.html&lt;span-class='chapter-number'&gt;9&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Summary-Sheet-Exam-1&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;10&lt;/span&gt;  &lt;span class='chapter-title'&gt;Summary Sheet Exam 2&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/summary/ex2_summary.html&lt;span-class='chapter-number'&gt;10&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Summary-Sheet-Exam-2&lt;/span&gt;\"}\n[In-class Activities]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:quarto-sidebar-section-3\"}\n[&lt;span class='chapter-number'&gt;11&lt;/span&gt;  &lt;span class='chapter-title'&gt;Univariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-uni.html&lt;span-class='chapter-number'&gt;11&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Univariate-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;12&lt;/span&gt;  &lt;span class='chapter-title'&gt;Bivariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-bi.html&lt;span-class='chapter-number'&gt;12&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Bivariate-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;13&lt;/span&gt;  &lt;span class='chapter-title'&gt;Mulivariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-multi.html&lt;span-class='chapter-number'&gt;13&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Mulivariate-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;14&lt;/span&gt;  &lt;span class='chapter-title'&gt;Spatial Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-spatial.html&lt;span-class='chapter-number'&gt;14&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Spatial-Viz&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;15&lt;/span&gt;  &lt;span class='chapter-title'&gt;Wrangling&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-wrangling.html&lt;span-class='chapter-number'&gt;15&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Wrangling&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;16&lt;/span&gt;  &lt;span class='chapter-title'&gt;Dates&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-dates.html&lt;span-class='chapter-number'&gt;16&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Dates&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;17&lt;/span&gt;  &lt;span class='chapter-title'&gt;Reshaping&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-reshaping.html&lt;span-class='chapter-number'&gt;17&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Reshaping&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;18&lt;/span&gt;  &lt;span class='chapter-title'&gt;Joining&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-joining.html&lt;span-class='chapter-number'&gt;18&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Joining&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;19&lt;/span&gt;  &lt;span class='chapter-title'&gt;Factors&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-factors.html&lt;span-class='chapter-number'&gt;19&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Factors&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;20&lt;/span&gt;  &lt;span class='chapter-title'&gt;Strings&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-strings.html&lt;span-class='chapter-number'&gt;20&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Strings&lt;/span&gt;\"}\n[&lt;span class='chapter-number'&gt;21&lt;/span&gt;  &lt;span class='chapter-title'&gt;Data Import&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar:/ica/ica-data_import.html&lt;span-class='chapter-number'&gt;21&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Data-Import&lt;/span&gt;\"}\n[Best Work]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-breadcrumbs-Best-Work\"}\n[&lt;span class='chapter-number'&gt;2&lt;/span&gt;  &lt;span class='chapter-title'&gt;Bivariate Viz&lt;/span&gt;]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-breadcrumbs-&lt;span-class='chapter-number'&gt;2&lt;/span&gt;--&lt;span-class='chapter-title'&gt;Bivariate-Viz&lt;/span&gt;\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[[2]{.chapter-number}  [Bivariate Viz]{.chapter-title} – COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-metatitle\"}\n[[2]{.chapter-number}  [Bivariate Viz]{.chapter-title} – COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercardtitle\"}\n[[2]{.chapter-number}  [Bivariate Viz]{.chapter-title} – COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardtitle\"}\n[COMP/STAT112 Notebook]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-metasitename\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercarddesc\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardddesc\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"Bivariate Viz\"\nexecute:\n  echo: true\n  warning: false\n  error: false\n\nformat: \n  html: \n    code-fold: true\n---\n\nquarto-executable-code-5450563D\n\n```r\nlibrary(tidyverse)\nlibrary (sf)\nlibrary (maps)\nlibrary (RColorBrewer)\nlibrary (gplots)\nlibrary(socviz)\nlibrary (leaflet)\nlibrary (devtools)\nlibrary(ggthemes)\nlibrary(viridis)\nquarto-executable-code-5450563D\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\nquarto-executable-code-5450563D\n#|fig_alt: \"This visualization is of a scatter plot of republican vote percentage in 2016 and 2020. The biggest takeaway is that the two variables are have strong positive correlation. Data from elections-https://mac-stat.github.io/data/election_2020_county.csv\"\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()+\n  labs(x=\"Republican % Vote (2016)\", y=\"Republican % Vote (2020)\", title= \"Relationship between Republican Vote %\", caption= \"Data from: TidyTuesday Github Repository, Created by: Martha Miller, Date: 02/23/2025\")+\nscale_color_viridis_d()\n\n:::",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-tri.html",
    "href": "bw/bw-tri.html",
    "title": "\n3  Trivariate Viz\n",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary (sf)\nlibrary (maps)\nlibrary (RColorBrewer)\nlibrary (gplots)\nlibrary(socviz)\nlibrary (leaflet)\nlibrary (devtools)\nlibrary(ggthemes)\nlibrary(viridis)\n\n\n\nCodeweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n\n\nCode#|fig-cap: \"This figure shows the relationship between the temperature at 9:00 AM and 3:00 PM in the 3 locations; Hobart, Uluru, and Wollogong. The x-axis shows the temperature at 3:00 pm, y-axis shows the temperature at 9:00 am, and the colors differentiate the location.\" \n\n#|fig_alt: \" This visualization is a scatter plot with 3 different colors that represent each location and their correlating temperatures. The biggest takeaways are that the there is a medium/strong and positive relationship in all of the locations, and that Uluru has typically the greatest temperatures. The data is from weather-https://mac-stat.github.io/data/weather_3_locations.csv\"\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point(alpha=0.5)+\n  labs(x=\"Temperature at 3:00 PM\", y=\"Temperature at 9:00 AM\", color=\"Location\",title=\"Comparing Temperatures of Different Times of the Day in Locations\", caption= \"Data from: TidyTuesday Github Repository, Created by: Martha Miller, Date: 02/23/2025\")+\n  scale_color_viridis_d()",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Trivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-quad.html",
    "href": "bw/bw-quad.html",
    "title": "\n4  Quadvariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking quadvariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nCodelibrary(tidyverse)\nlibrary (sf)\nlibrary (maps)\nlibrary (RColorBrewer)\nlibrary (gplots)\nlibrary(socviz)\nlibrary (leaflet)\nlibrary (devtools)\nlibrary(ggthemes)\nlibrary(viridis)\n\n\n\nCodeweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n\n\nCode#| fig-alt: \"A scatter plot displaying temperature readings at 3:00 AM and 9:00 AM for multiple locations, with additional data indicating whether it rained. The x-axis temperature at 9:00 AM, while the y-axis shows temperature values at 3:00 PM. Different shapes distinguish rainy and non-rainy conditions, illustrating potential trends in temperature shifts due to precipitation. The biggest takeaways are that the there is a medium/strong and positive relationship in all of the locations, and that Uluru has typically the greatest temperatures. Additionally, there seems to be more rain in hobart than the other two locations. The data is from weather-https://mac-stat.github.io/data/weather_3_locations.csv\"\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point(alpha=0.5)+\nlabs(y=\"Temperature at 3:00 PM\", x=\"Temperature at 9:00 AM\", color=\"Location\", shape=\"Rain\", title=\"Temperature Variations and Rainfall by Location at 3:00 AM and 9:00 AM\", caption=\"\")+\n  scale_color_viridis_d()\n\n\n\nTemperature readings at 3:00 AM and 9:00 AM across multiple locations, with indications of whether rainfall occurred. The graph highlights temperature variations and potential correlations with rainfall.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quadvariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-spatial.html",
    "href": "bw/bw-spatial.html",
    "title": "\n5  Spatial Viz\n",
    "section": "",
    "text": "Codelibrary (sf)\nlibrary (maps)\nlibrary (RColorBrewer)\nlibrary (gplots)\nlibrary(socviz)\nlibrary (leaflet)\nlibrary (devtools)\nlibrary(ggthemes)\n\n\n\nCodestarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\n\n\nCodelibrary(dplyr)\n\n# Check and rename if necessary\nstarbucks &lt;- starbucks %&gt;%\n  rename_all(tolower)  # Converts all column names to lowercase\n\n# Use the correct column name\nstarbucks %&gt;%\n  filter(country == \"US\", state.province == \"MN\")  # Adjust based on actual column name\n\n        brand store.number                          store.name ownership.type\n1   Starbucks 16960-177066             Target Alexandria T-821       Licensed\n2   Starbucks  76316-95687               Target Andover T-2025       Licensed\n3   Starbucks 25535-241231       Round Lake Blvd and Northdale  Company Owned\n4   Starbucks  72682-76602    Super Target Apple Valley ST-643       Licensed\n5   Starbucks 76896-121294 Super Target Apple Vly South ST-239       Licensed\n6   Starbucks 21376-209526 Lexington and Red Fox - Arden Hills  Company Owned\n7   Starbucks 15516-156753                 Target Baxter T-659       Licensed\n8   Starbucks 11880-106241       Hwy 371 and Woida Road-Baxter  Company Owned\n9   Starbucks 17378-177052               Target Bemidji T-657.       Licensed\n10  Starbucks  76115-88842         Super Target Blaine ST-1832       Licensed\n11  Starbucks 20107-189627           109th and Hwy 65 - Blaine  Company Owned\n12  Starbucks 26122-243619                  MOA 3rd Floor C310  Company Owned\n13  Starbucks   2578-36339        98th and Lyndale-Bloomington  Company Owned\n14  Starbucks  10077-98295                  Mall of America II  Company Owned\n15  Starbucks   2705-84660     164 E. Broadway - 1st level MOA  Company Owned\n16  Starbucks     2220-665          Southtown - I-494 and Penn  Company Owned\n17    Teavana 28432-249726           Teavana - Mall of America  Company Owned\n18  Starbucks  76187-88840              Target Bloomington T-5       Licensed\n19  Starbucks   9605-97352  Brooklyn Ctr-Brooklyn Blvd. & 55th  Company Owned\n20  Starbucks  76133-88561          Target Brooklyn Park T-693       Licensed\n21  Starbucks 21555-204764                 Target North Campus       Licensed\n22  Starbucks 47126-254733        Hy Vee @ Brooklyn Park Kiosk       Licensed\n23  Starbucks 48065-254734               Hyvee @ Brooklyn Park       Licensed\n24  Starbucks  76224-90369                Target Buffalo T-861       Licensed\n25  Starbucks 76809-110952     Super Target Burnsville ST-2340       Licensed\n26    Teavana 28371-249964         Teavana - Burnsville Center  Company Owned\n27  Starbucks 23397-220084              CR 42 and Burnhaven Dr  Company Owned\n28  Starbucks 48164-261724       Cambridge - Hwy 95 and Hwy 65  Company Owned\n29  Starbucks 23160-226966    Hwy 169 and Elm Creek - Champlin  Company Owned\n30  Starbucks  76112-88843       Super Target Champlin ST-1831       Licensed\n31  Starbucks  2450-198665          Chanhassen - Dakota Ave II  Company Owned\n32  Starbucks 20793-208085             Target Chanhassen T-862       Licensed\n33  Starbucks  72649-48359         Super Target Chaska ST-1352       Licensed\n34  Starbucks 23669-232351           Target Coon Rapids T-1144       Licensed\n35  Starbucks  10743-99776          Coon Rapids-Foley & Hwy 10  Company Owned\n36  Starbucks  76206-88844              Target Northtown T-820       Licensed\n37  Starbucks   9479-96696      Cottage Grove-E. Point Douglas  Company Owned\n38  Starbucks 20345-204770          Target Cottage Grove T-662       Licensed\n39  Starbucks 22725-223764     Bass Lake Rd & Broadway-Crystal  Company Owned\n40  Starbucks 15514-141825                   Target Duluth T-4       Licensed\n41    Teavana 28464-250041          Teavana - Miller Hill Mall  Company Owned\n42  Starbucks 21634-213712          Woodland Ave & College St.  Company Owned\n43  Starbucks   2733-88041           1405 Miller Trunk Highway  Company Owned\n44  Starbucks   9711-98049             Duluth - Superior & 4th  Company Owned\n45  Starbucks   2619-53139             2040 Cliff Road - Eagan  Company Owned\n46  Starbucks   2469-14451                       Eagan Station  Company Owned\n47  Starbucks 47509-257827                Hy Vee - Eagan #1165       Licensed\n48  Starbucks 70341-140691                  Kowalski's - Eagan       Licensed\n49  Starbucks 20688-204064   Cedar & Hwy 13-TC Premium Outlets  Company Owned\n50  Starbucks    2366-7808  Hwy 169 and Anderson Lakes Parkway  Company Owned\n51  Starbucks    2325-2482            582 Prairie Center Drive  Company Owned\n52  Starbucks 26250-243426                    62 and Shady Oak  Company Owned\n53    Teavana 28331-250065       Teavana - Eden Prairie Center  Company Owned\n54  Starbucks   2599-47139 930 Prairie Center Dr- Eden Prairie  Company Owned\n55    Teavana 28473-250151          Teavana - Southdale Center  Company Owned\n56  Starbucks 21755-192244                       Jerry's Edina       Licensed\n57  Starbucks     271-1488       3215 Galleria - Galleria Mall  Company Owned\n58  Starbucks     274-1489      3939 W. 50th - 50th and France  Company Owned\n59  Starbucks 16011-154128               Macy's-Southdale #231       Licensed\n60  Starbucks 24961-238825 Edina - Hwy 100 and Industrial Blvd  Company Owned\n61  Starbucks   2592-54119            Edina - Hwy 100 & Vernon  Company Owned\n62  Starbucks 76641-110944          Super Target Edina ST-2313       Licensed\n63  Starbucks 18565-178125        France Avenue & 76th - Edina  Company Owned\n64  Starbucks 20689-206848  Highway 169 and Freeport-Elk River  Company Owned\n65  Starbucks 27578-238022              Kowalski's - Excelsior       Licensed\n66  Starbucks 20343-204765           Target Forest Lake T-1244       Licensed\n67  Starbucks   2621-61481                         Forest Lake  Company Owned\n68  Starbucks 24962-238826          Central and 53rd - Fridley  Company Owned\n69  Starbucks     2228-659       Hwy 55 and Golden Valley Road  Company Owned\n70  Starbucks  76645-97519           Target Grand Rapids T-904       Licensed\n71  Starbucks 20342-204766              Target Hastings T-1245       Licensed\n72  Starbucks   9359-96468          Hwy 7 & Hopkins Crossroads  Company Owned\n73  Starbucks 23667-232356          Target - Hutchinson T-1210       Licensed\n74  Starbucks 16842-138228   Target Inver Grove Heights T-2519       Licensed\n75  Starbucks 26713-243085            Hy-Vee - Lakeville #1356       Licensed\n76  Starbucks   8959-95199 16227 Kenrick Ave - I35 & Cty Rd 46  Company Owned\n77  Starbucks   9762-97954    Lakeville - I-35 & Kenwood Trail  Company Owned\n78  Starbucks  72684-76620      Super Target Lakeville ST-1484       Licensed\n79  Starbucks  72677-64263     Super Target Lino Lakes ST-1448       Licensed\n80  Starbucks 21372-208604         Madison and Sioux - Mankato  Company Owned\n81  Starbucks  72951-93896                 Hy-Vee - Mankato #2       Licensed\n82  Starbucks 17564-178944        Maple Grove - Dunkirk Square  Company Owned\n83  Starbucks  76808-99120     Super Target Maple Grove T-2193       Licensed\n84  Starbucks    2318-2001                 7979 Wedgewood Lane  Company Owned\n85    Teavana 28570-249741               Teavana - Maple Grove  Company Owned\n86  Starbucks  76709-99117                Target Medina T-2223       Licensed\n87  Starbucks  72669-48339           Target Minneapolis T-1375       Licensed\n88  Starbucks  76188-88825                  Target Crystal T-3       Licensed\n89  Starbucks     295-1494                  25th and Riverside  Company Owned\n90  Starbucks  76974-96449             Target Minneapolis T-52       Licensed\n91  Starbucks 23000-226785            14th and 5th - Dinkytown  Company Owned\n92  Starbucks 17122-175084   University of Minnesota-Lind Hall       Licensed\n93  Starbucks 16152-165904 Target Minneapolis Downtown HQ T-13       Licensed\n94  Starbucks  2452-229484  22nd & Hennepin - Kenwood Crossing  Company Owned\n95  Starbucks  75137-68162        U of MN - Coffman Union Bldg       Licensed\n96  Starbucks    2341-2483      33 S. 6th Street - City Center  Company Owned\n97  Starbucks   8611-92908                 Nicollet & Franklin  Company Owned\n98  Starbucks   2488-21281     U of MN - 615 Washington Ave SE  Company Owned\n99  Starbucks  78063-99510      U of MN/Minneapolis - W Campus       Licensed\n100 Starbucks  76625-95375   Super Target Mpls/Fridley ST-2200       Licensed\n101 Starbucks  72621-28147                Macys -  Minneapolis       Licensed\n102 Starbucks     263-1496           IDS Center - Skyway level  Company Owned\n103 Starbucks   2648-54101 600 Hennepin Ave S - Mayo Clinic Sq  Company Owned\n104 Starbucks     288-1493           5351 Lyndale Avenue South  Company Owned\n105 Starbucks  76653-94395           Target Minneapolis T-1095       Licensed\n106 Starbucks  75788-85011       Double Tree Hotel Minneapolis       Licensed\n107 Starbucks     262-1495    120 S. 6th Street - Skyway level  Company Owned\n108 Starbucks 76736-104469    Target Minnetonka/Hopkins T-0100       Licensed\n109   Teavana 28366-249908          Teavana - Ridgedale Center  Company Owned\n110 Starbucks  72650-46879     Super Target Minnetonka ST-1356       Licensed\n111 Starbucks 76730-100251      Super Target Monticello T-2180       Licensed\n112 Starbucks  8960-103132                 Moorhead-I-94 & 8th  Company Owned\n113 Starbucks 16032-156750              Target Moorhead T-658.       Licensed\n114 Starbucks 26711-243086                Hy-Vee - New Hope #1       Licensed\n115   Teavana 28551-250039            Teavana - Maplewood Mall  Company Owned\n116 Starbucks 17380-177067           Target Northfield T-1211.       Licensed\n117 Starbucks 13347-106328        Hwy 36 and Hwy 5 - 5980 Neal  Company Owned\n118 Starbucks 27918-247538      Hy-Vee - Oakdale Main Entrance       Licensed\n119 Starbucks 26824-243087                      Hy-Vee Oakdale       Licensed\n120 Starbucks 76975-136638         Super Target Otsego ST-2456       Licensed\n121 Starbucks   8922-95305        641 W. Bridge St. - Owatonna  Company Owned\n122 Starbucks 20790-208086              Target Owatonna T-1068       Licensed\n123 Starbucks  2330-245290   Hwy 55 & Vicksburg Ave - Plymouth  Company Owned\n124 Starbucks  72685-76622        Super Target Plymouth ST-664       Licensed\n125 Starbucks   9712-97797        Plymouth - Hwy 55 & NW Blvd.  Company Owned\n126 Starbucks 29841-254985            Target - Red Wing T-1522       Licensed\n127 Starbucks 76815-110950      Super Target Richfield ST-2300       Licensed\n128 Starbucks   3484-92905                    Lyndale and 77th  Company Owned\n129 Starbucks 70009-126633 Doubletree Hotel Rochester Downtown       Licensed\n130 Starbucks 28120-248664                   HyVee Rochester 4       Licensed\n131 Starbucks  72639-32282      Super Target Rochester ST-1351       Licensed\n132   Teavana 28666-250117               Teavana - Apache Mall  Company Owned\n133 Starbucks 76812-110951 Super Target Rochester South ST-232       Licensed\n134 Starbucks  75560-62499                  Kahler Grand Hotel       Licensed\n135 Starbucks 24088-232347                Hy-Vee @ Rochester 2       Licensed\n136 Starbucks  72678-64266         Super Target Rogers ST-1456       Licensed\n137 Starbucks 18163-182985      Hwy 101 & Diamond Lk Rd-Rogers  Company Owned\n138 Starbucks   9652-97788           Rosemount - CR 42 & Hwy 3  Company Owned\n139 Starbucks  76441-90372      Super Target Roseville ST-2101       Licensed\n140 Starbucks     2209-658             NWC Hwy 36 and Fairview  Company Owned\n141 Starbucks 23753-154129                Macy's-Rosedale #233       Licensed\n142 Starbucks 13624-106909          Hwy 36 and Fairview Avenue  Company Owned\n143 Starbucks  76440-97594     Super Target Saint Paul ST-2046       Licensed\n144 Starbucks 47124-248644               Kowalski's - St. Paul       Licensed\n145 Starbucks 10602-101637               Savage-CR 42 & Hwy 13  Company Owned\n146 Starbucks  76116-87623         Super Target Savage ST-1833       Licensed\n147 Starbucks 21371-208605   CR 21 and Old Carriage - Shakopee  Company Owned\n148 Starbucks  72645-32261       Super Target Shoreview ST-619       Licensed\n149 Starbucks 49710-266984              Kowalski's @ Shoreview       Licensed\n150 Starbucks  76223-90370          Target St Cloud East T-930       Licensed\n151 Starbucks  76408-96764      Super Target Knollwood ST-2189       Licensed\n152 Starbucks  75587-80343      MSP Minn D6 Baggage Claim Area       Licensed\n153 Starbucks 75884-101521                  MSP North Ternimal       Licensed\n154 Starbucks   72336-5745   MSP Minn Green Concourse, Gate 69       Licensed\n155 Starbucks 19892-188669           St. Cloud, Division Place  Company Owned\n156 Starbucks  10203-99310   St. Louis Park-Minnetonka & Hwy 7  Company Owned\n157 Starbucks   2696-63425    Excelsior & Grand-St. Louis Park  Company Owned\n158 Starbucks  10156-99311      St. Paul-Grand Ave & Oxford St  Company Owned\n159 Starbucks 20425-204771                Target St. Paul T-68       Licensed\n160 Starbucks     276-1490                   2078 Ford Parkway  Company Owned\n161 Starbucks     2211-638 Snelling & Selby-171 Snelling Ave N  Company Owned\n162 Starbucks   2462-18999     380 St. Peter St-Lawson Commons  Company Owned\n163 Starbucks 76592-100250       Target St Paul/Oakdale T-2135       Licensed\n164 Starbucks 17498-147376                  Hamline University       Licensed\n165 Starbucks 76771-108255       SuperTarget - St. Paul T-2229       Licensed\n166 Starbucks  76130-88841             Target Stillwater T-931       Licensed\n167 Starbucks  79211-97807        Kowalski-Stillwater/Oak Park       Licensed\n168 Starbucks 20341-204767        Target Vadnais Heights T-751       Licensed\n169 Starbucks 25515-241175         Hwy 96 and Centerville Road  Company Owned\n170 Starbucks  76666-97597               Target Virginia T-847       Licensed\n171 Starbucks 76979-136639               Target Waconia T-2449       Licensed\n172 Starbucks 10511-101337  Waite Park - 2nd St S & Waite Park  Company Owned\n173 Starbucks   2440-20979        740 E. Lake Street - Wayzata  Company Owned\n174 Starbucks   9526-96609  S. Robert & Thompson-West St. Paul  Company Owned\n175 Starbucks 27985-249186              White Bear Ave & I-694  Company Owned\n176 Starbucks  72719-36379      Kowalski-White Bear Lake #4727       Licensed\n177 Starbucks  76622-99116                Target Winona T-1096       Licensed\n178 Starbucks   2529-35559               8362 Tamarack Village  Company Owned\n179 Starbucks  2241-213605              7230 Valley Creek Road  Company Owned\n180 Starbucks 15230-138227         Target Woodbury East T-2406       Licensed\n181 Starbucks 21296-210564           Woodbury Dr. and Commerce  Company Owned\n182 Starbucks  72701-27209                 Kowalski - Woodbury       Licensed\n183 Starbucks 28021-242084              Target Woodbury T-0694       Licensed\n184 Starbucks 25759-235385            Jerry's Foods - Woodbury       Licensed\n                                              street.address\n1                                              4404 Hwy 29 S\n2                                   2000 Bonker Lake Blvd NW\n3                                      13131 Riverdale Dr NW\n4                                            15150 Cedar Ave\n5                                        15560 Pilot Knob Rd\n6                                       3833 Lexington Ave N\n7                                          14546 Dellwood Dr\n8                                          15091 Edgewood Dr\n9                                     2100 Paul Bunyan Dr NW\n10                                         1500 109th Ave NE\n11                                    1384 109th Avenue, 100\n12              60 E. Broadway, C310, Clover Shopping Center\n13  704-708 West 98th Street, Mall of America - 2nd location\n14                   276 West Market Street, Mall of America\n15             164 E. Broadway, Suite E164, Southtown Center\n16                    7805 Southtown Center, Mall of America\n17                                        128 WEST MARKET ST\n18                                            2555 W 79th St\n19                                        5512 Brooklyn Blvd\n20                                       7535 W Broadway Ave\n21                                     7000 Target Parkway N\n22                                       9409 Zane Ave North\n23                                       9409 Zane Ave North\n24                                      1300 State Hwy 55 NE\n25                      810 County Rd 42W, Burnsville Center\n26                             1178 Burnsville Center, #2010\n27                                     1310 County Road 42 W\n28                                                Highway 95\n29                                        11223 Aquila Drive\n30                                11960 Business Park Blvd N\n31                                  190 Lake Drive East, 140\n32                                             851 W 78th St\n33                                         111 Pioneer Trail\n34                        3300 124th Ave NW, Pine Cone Plaza\n35                                     9920 Foley Blvd, 102A\n36                                    8600 Springbrook Dr NW\n37                            7190 East Point Douglas Rd. S.\n38                                 8655 E Point Douglas Rd S\n39                                     5596 W. Broadway Ave.\n40                   1902 Miller Trunk Hwy, Miller Hill Mall\n41                              1600 Miller Trunk Hwy., #E08\n42                                         1002 Woodland Ave\n43                                 1405 Miller Trunk Highway\n44                                       331 W. Superior St.\n45                  2040 Cliff Rd., Suite 101, Eagan Station\n46                                      3450 Pilot Knob Road\n47                      Yankee Doodle Road / Pilot Knob Road\n48            1646 Diffley Road, Paragon Outlets Twin Cities\n49                           3965 Eagan Outlets Parkway, 455\n50                               9639 Anderson Lakes Parkway\n51                              582 PRAIRIE CENTER DR., #260\n52             TBD Hwy 62 and Shady Oak, Eden Prairie Center\n53         8251 Flying Clould Dr., #1226, Prairieview Center\n54                930 Prairie Center Drive, Southdale Center\n55                                     1075 Southdale Center\n56                                  5101 Vernon Avenue South\n57                                             3215 Galleria\n58                                            3939 W 50th St\n59                                         100 Southdale Ctr\n60                  5122 Edina Industrial Blvd, Vernon Shops\n61                                       5121 Gus Young Lane\n62                                           7000 York Ave S\n63                                           7503 France Ave\n64                                     19179 Freeport Ave, A\n65                                              440 Water St\n66                   356 12th St SW, Forest Lake Marketplace\n67                                2009 W. Broadway Ave., 900\n68          5300 Central Ave NE, Golden Valley Commons, #140\n69                               7802 Olson Memorial Highway\n70                                       2140 S Pokegama Ave\n71                                     875 General Sieben Dr\n72                                          1505 State Hwy 7\n73                                         1370 Highway 15 S\n74                                            7841 Amana Trl\n75                                     16150 Pilot Knob Road\n76                       16227 Kenrick Ave., Argonne Village\n77                                       17740 Kenwood Trail\n78                                         18275 Kenrick Ave\n79                                             749 Apollo Dr\n80                                          1872 Madison Ave\n81                                             2010 Adams St\n82                                   9404 Dunkirk Lane North\n83                                      15300 Grove Circle N\n84                          7979 Wedgewood Lane, Maple Grove\n85                                      12113 ELM CREEK BLVD\n86                                         300 Clydesdale Rd\n87                                         900 Nicollet Mall\n88                                       5537 W Broadway Ave\n89                                             815 25th Ave.\n90                                            2500 E Lake St\n91                                        425 14th Avenue SE\n92                                          207 Church St SE\n93                                        1000 Nicollet Mall\n94                                  2212 Hennepin Ave, Bay 1\n95                                        218 Como Avenue SE\n96                                       33 South 6th Street\n97              2000 Nicollet Ave., Radisson Hotel Metrodome\n98                                     615 Washington Ave SE\n99                                            321 19th Ave S\n100                           755 53rd Ave NE, Nicollet Mall\n101                                          700 On the Mall\n102        80 South 8th Street, Minneapolis Lifestyle Centre\n103                                  600 Hennepin Ave. South\n104                                   5351 Lyndale Ave South\n105                                   1650 New Brighton Blvd\n106                                         1101 LaSalle Ave\n107                                        120 S. 6th Street\n108                     13201 Ridgedale Dr, Ridgedale Center\n109                           12401 Wayzata Blvd, Spc. #2138\n110                                     4802 County Road 101\n111                                         1447 East 7th St\n112                                        906 Holiday Drive\n113                                             3301 US-10 E\n114                          8200 42nd Ave N, Maplewood Mall\n115                               3001 Whitebear Ave., #2025\n116                                         2323 Highway 3 S\n117                                   5980 Neal Ave. N., 800\n118                                           7180 10th St N\n119                                       7120 10th St North\n120                                         15800 87th St NE\n121                           641 West Bridge St., Suite 100\n122                                              301 Park Dr\n123                                  15850 32nd Avenue North\n124                                     4175 Vinewood Lane N\n125                                          2661 Campus Dr.\n126                                           151 Tyler Rd N\n127                                      6445 Richfield Pkwy\n128                       7610 Lyndale Ave. South, Suite 800\n129                                           150 S Broadway\n130                                4200 West Circle Drive NW\n131                      3827 Marketplace Dr NW, Apache Mall\n132                                   1200 12th St. SW, #250\n133                                        4611 Maine Ave SE\n134                                         20 SW Second Ave\n135                                            500 37th St W\n136                                  21615 S Diamond Lake Rd\n137                          13590 Northdale Blvd, Suite 100\n138                                 14903 South Robert Trail\n139                                       1515 W County Rd B\n140                                     2305 Fairview Avenue\n141                      900 Rosedale CTR, Hwy 36 & Fairview\n142                                              1820 Hwy 36\n143                                         1750 S Robert St\n144                                        1261 GRAND AVENUE\n145                                13945 Hwy 13 Frontage Rd.\n146                                             14333 Hwy 13\n147                          8090 Old Carriage Ct N, Suite D\n148                                     3800 N Lexington Ave\n149                                             441 Hwy 96 W\n150                                       120 Lincoln Ave SE\n151                                                8900 SR-7\n152                                       4300 Gulmack Drive\n153                                       4300 Glumack Drive\n154                       4300 Gulmack Drive, Division Place\n155                                  2860 W. Division Street\n156            4201 Minnetonka Blvd., Excelsior Park Commons\n157                                           3850 Grand Way\n158                                          1062 Grand Ave.\n159                                        1744 Suburban Ave\n160                                        2078 FORD PARKWAY\n161                                    171 Snelling Avenue N\n162                                        380 St. Peter St.\n163                                           7900 32nd St N\n164                                       1536 Hewitt Avenue\n165                                      1300 University Ave\n166                                           2021 Market Dr\n167                                          5801 Neal Ave N\n168                                          975 County Rd E\n169                                            1016 E Hwy 96\n170                                           1001 13th St S\n171                 875 E Main St, Marketplace of Waite Park\n172                                     124 2nd Street South\n173                                     740 East Lake Street\n174                                   1470 Roberts St. South\n175                                          1950 Buerkle Rd\n176                                        4391 S. Lake Ave.\n177                        860 Mankato Ave, Tamarack Village\n178                                    8362 Tamarack Village\n179                                     7230 Valley Creek Rd\n180                                          449 Commerce Dr\n181                                    610 Woodbury Dr., 100\n182                                    8505 Valley Creek Rd.\n183                                  7200 Valley Creek Plaza\n184                                      7760 Hargis Parkway\n                   city state.province country  postcode   phone.number\n1            Alexandria             MN      US 563082915   320-763-6661\n2               Andover             MN      US 553044014   763-852-0113\n3                 Anoka             MN      US     55448     6127902009\n4          Apple Valley             MN      US 551247038   952-891-5500\n5          Apple Valley             MN      US 551247286   952-236-3165\n6           Arden Hills             MN      US     55126   651-486-3805\n7                Baxter             MN      US 564259744   218-828-3535\n8                Baxter             MN      US     56425   218-828-2978\n9               Bemidji             MN      US 566015645               \n10               Blaine             MN      US 554494670   763-354-1000\n11               Blaine             MN      US     55434     7637676833\n12          Bloomington             MN      US     55425   612-257-5599\n13          Bloomington             MN      US 554204718   952-881-9877\n14          Bloomington             MN      US 554255522   952-853-1274\n15          Bloomington             MN      US 554255511   952-854-1922\n16          Bloomington             MN      US 554311324   952-885-9985\n17          Bloomington             MN      US     55425   952-853-9880\n18          Bloomington             MN      US 554311277   952-888-7701\n19      Brooklyn Center             MN      US 554293054   763-560-1095\n20        Brooklyn Park             MN      US 554281287   612-425-5400\n21        Brooklyn Park             MN      US     55444               \n22        Brooklyn Park             MN      US     55443   763-488-4500\n23        Brooklyn Park             MN      US     55443   763-488-4500\n24              Buffalo             MN      US 553134321   763-682-4996\n25           Burnsville             MN      US 553374492   952-236-3003\n26           Burnsville             MN      US     55306   952-898-5103\n27           Burnsville             MN      US     55337     6125003915\n28            Cambridge             MN      US     55008   612-257-4407\n29             Champlin             MN      US     55316     6125003927\n30             Champlin             MN      US 553162005   763-354-1006\n31           Chanhassen             MN      US     55317     9522948040\n32           Chanhassen             MN      US     55317   952-470-0206\n33               Chaska             MN      US 553181123   952-361-3043\n34          Coon Rapids             MN      US     55433   763-323-2931\n35          Coon Rapids             MN      US     55448   763 783 3412\n36          Coon Rapids             MN      US 554336033   763-785-0322\n37        Cottage Grove             MN      US 550163024   651-769-0719\n38        Cottage Grove             MN      US     55016   651-458-8179\n39              Crystal             MN      US     55428     7635334869\n40               Duluth             MN      US 558111810   218-727-8851\n41               Duluth             MN      US     55811   218-722-2386\n42               Duluth             MN      US     55811     2187243034\n43               Duluth             MN      US 558115613   218-722-6132\n44               Duluth             MN      US 558024532   218-722-1025\n45                Eagan             MN      US 551222491   651/405-0800\n46                Eagan             MN      US 551223825   651-405-0534\n47                Eagan             MN      US     55122   515-267-2805\n48                Eagan             MN      US 551222213   651-328-8300\n49                Eagan             MN      US     55122     6514545153\n50         Eden Prairie             MN      US 553444155     9529448646\n51         Eden Prairie             MN      US 553447942     9528332170\n52         Eden Prairie             MN      US     55343     6129108947\n53         Eden Prairie             MN      US     55344   305-442-9602\n54         Eden Prairie             MN      US 553447304     9529752951\n55                Edina             MN      US     55435   952-929-1102\n56                Edina             MN      US     55436   952-929-2685\n57                Edina             MN      US 554352509     9529240344\n58                Edina             MN      US 554241244     9529277055\n59                Edina             MN      US 554352403   952-924-6725\n60                Edina             MN      US     55439     6126557209\n61                Edina             MN      US 554361530   952-920-0193\n62                Edina             MN      US 554354285   952-925-4610\n63                Edina             MN      US     55435     9527377820\n64            Elk River             MN      US     55330     7634413991\n65            Excelsior             MN      US     55331   952-229-8300\n66          Forest Lake             MN      US     55025   651-464-4700\n67          Forest Lake             MN      US 550259374   651-464-0688\n68              Fridley             MN      US     55421     6129004914\n69        Golden Valley             MN      US 554274715     7635466775\n70         Grand Rapids             MN      US 557442507   218-326-3031\n71             Hastings             MN      US     55033   651-480-2080\n72              Hopkins             MN      US     55305   952-938-3604\n73           Hutchinson             MN      US     55350   320-587-7113\n74  Inver Grove Heights             MN      US     55077   651-234-2949\n75            Lakeville             MN      US     55044   952-423-9340\n76            Lakeville             MN      US 550448498   952-892-0778\n77            Lakeville             MN      US 550449454   952-898-1373\n78            Lakeville             MN      US 550447306   952-892-5400\n79           Lino Lakes             MN      US 550143035   651-784-7601\n80              Mankato             MN      US     56001    507 6258900\n81              Mankato             MN      US 560016817   507-625-9070\n82          Maple Grove             MN      US     55311     7634161877\n83          Maple Grove             MN      US 553694469   763-447-2506\n84          Maple Grove             MN      US 553699411   763-420-6311\n85          Maple Grove             MN      US     55369   763-493-0040\n86               Medina             MN      US 553404538   763-852-0006\n87          Minneapolis             MN      US 554032530   763-440-9390\n88          Minneapolis             MN      US 554283592   763-533-2231\n89          Minneapolis             MN      US 554541418   612-305-0235\n90          Minneapolis             MN      US 554061976 (612) 721-5701\n91          Minneapolis             MN      US     55414     6123310167\n92          Minneapolis             MN      US     55455               \n93          Minneapolis             MN      US 554032542   612-761-7291\n94          Minneapolis             MN      US     55405     6123745168\n95          Minneapolis             MN      US 554141023   612-625-2037\n96          Minneapolis             MN      US 554023601     6123397084\n97          Minneapolis             MN      US 554042526   612-871-1338\n98          Minneapolis             MN      US 554142931   612-378-7765\n99          Minneapolis             MN      US 554550438   612-625-8585\n100         Minneapolis             MN      US 554211298   763-571-9361\n101         Minneapolis             MN      US     55402     6123752077\n102         Minneapolis             MN      US     55402   612-371-0383\n103         Minneapolis             MN      US 554031816   612/333-9944\n104         Minneapolis             MN      US 554191229   612-821-0686\n105         Minneapolis             MN      US 554131643   612-781-7033\n106         Minneapolis             MN      US 554032046   612-492-2230\n107         Minneapolis             MN      US 554021803   612-371-0555\n108          Minnetonka             MN      US 553051808   952-542-8250\n109          Minnetonka             MN      US     55305   952-544-0018\n110          Minnetonka             MN      US 553452635   952-401-3830\n111          Monticello             MN      US 553624666   763-586-3327\n112            Moorhead             MN      US 565604445   218-233-0392\n113            Moorhead             MN      US     56560   218-233-2953\n114            New Hope             MN      US     55427               \n115     North Maplewood             MN      US     55109   651-777-7397\n116          Northfield             MN      US     55057     5076454777\n117    Oak Park Heights             MN      US 550822190   651-275-8926\n118             Oakdale             MN      US     55128   651-714-3160\n119             Oakdale             MN      US     55128   651-714-3186\n120              Otsego             MN      US 553306546   763-252-1316\n121            Owatonna             MN      US 550606400   507-455-3624\n122            Owatonna             MN      US     55060   507-451-1882\n123            Plymouth             MN      US     55447               \n124            Plymouth             MN      US 554422624   763-553-0302\n125            Plymouth             MN      US 554412692   763-553-3087\n126            Red Wing             MN      US     55066     6513887704\n127           Richfield             MN      US 554236400   612-252-0473\n128           Richfield             MN      US 554234167   612-869-9839\n129           Rochester             MN      US 559046507     5075292433\n130           Rochester             MN      US     55901   507-292-2000\n131           Rochester             MN      US 559013192   507-536-2555\n132           Rochester             MN      US     55902   507-289-3711\n133           Rochester             MN      US 559046929   507-206-5020\n134           Rochester             MN      US 559023027   507-280-6200\n135           Rochester             MN      US     55901   507-289-1815\n136              Rogers             MN      US 553748893   763-428-1394\n137              Rogers             MN      US     55374     7634282083\n138           Rosemount             MN      US 550683106   651-423-7112\n139           Roseville             MN      US 551136005   651-482-0198\n140           Roseville             MN      US 551132724 (651) 697-0215\n141           Roseville             MN      US     55113   651-639-6600\n142           Roseville             MN      US 551134030   651-631-3089\n143          Saint Paul             MN      US 551185112   651-455-6671\n144          Saint Paul             MN      US     55105     6516983366\n145              Savage             MN      US 553782281   952-440-2314\n146              Savage             MN      US 553782153   652-226-1444\n147            Shakopee             MN      US     55379     9524454241\n148           Shoreview             MN      US 551262916   651-486-0048\n149           Shoreview             MN      US     55126               \n150            St Cloud             MN      US 563040822   320-654-0712\n151       St Louis Park             MN      US     55426   952-935-9641\n152             St Paul             MN      US 551113002   612-726-5360\n153             St Paul             MN      US 551113002     6123966531\n154             St Paul             MN      US 551113002     6123966531\n155           St. Cloud             MN      US     56301     3202497890\n156      St. Louis Park             MN      US 554164129   952-922-2084\n157      St. Louis Park             MN      US 554164788    952920-8531\n158            St. Paul             MN      US 551053805   651-221-4465\n159            St. Paul             MN      US     55106   651-778-1188\n160            St. Paul             MN      US 551161813   651-699-9409\n161            St. Paul             MN      US 551046747   651-659-9033\n162            St. Paul             MN      US 551021313   651-222-7118\n163            St. Paul             MN      US 551284054   651-855-0990\n164            St. Paul             MN      US 551041205               \n165            St. Paul             MN      US     55401   651-642-1146\n166          Stillwater             MN      US 550827546   651-439-0292\n167          Stillwater             MN      US 550822177   651-439-9161\n168     Vadnais Heights             MN      US     55127   651-483-9637\n169     Vadnais Heights             MN      US     55127               \n170            Virginia             MN      US 557923254   218-741-6603\n171             Waconia             MN      US 553871081   952-442-9333\n172          Waite Park             MN      US 563871363   320-251-6255\n173             Wayzata             MN      US 553911713   952-249-1555\n174       West St. Paul             MN      US 551183140   651-450-4494\n175     White Bear Lake             MN      US     55110     6124990682\n176     White Bear Lake             MN      US 551103455   651-429-5913\n177              Winona             MN      US 559874867   507-452-7006\n178            Woodbury             MN      US 551253392   651-735-7600\n179            Woodbury             MN      US     55125     6517140357\n180            Woodbury             MN      US     55125   651-239-1874\n181            Woodbury             MN      US     55125     6517399350\n182            Woodbury             MN      US 551252342   651-578-8800\n183            Woodbury             MN      US     55125   651-735-7083\n184            Woodbury             MN      US     55129   952-237-7161\n                     timezone longitude latitude\n1   GMT-06:00 America/Chicago    -95.39    45.85\n2   GMT-06:00 America/Chicago    -93.32    45.22\n3   GMT-06:00 America/Chicago    -93.36    45.21\n4   GMT-06:00 America/Chicago    -93.21    44.73\n5   GMT-06:00 America/Chicago    -93.18    44.72\n6   GMT-06:00 America/Chicago    -93.15    45.06\n7   GMT-06:00 America/Chicago    -94.24    46.37\n8   GMT-06:00 America/Chicago    -94.25    46.37\n9   GMT-06:00 America/Chicago    -94.91    47.49\n10  GMT-06:00 America/Chicago    -93.23    45.17\n11  GMT-06:00 America/Chicago    -93.24    45.17\n12  GMT-06:00 America/Chicago    -93.24    44.86\n13  GMT-06:00 America/Chicago    -93.29    44.83\n14  GMT-06:00 America/Chicago    -93.24    44.86\n15  GMT-06:00 America/Chicago    -93.24    44.85\n16  GMT-06:00 America/Chicago    -93.31    44.86\n17  GMT-06:00 America/Chicago    -93.24    44.85\n18  GMT-06:00 America/Chicago    -93.31    44.86\n19  GMT-06:00 America/Chicago    -93.32    45.06\n20  GMT-06:00 America/Chicago    -93.38    45.09\n21  GMT-06:00 America/Chicago    -93.37    45.14\n22  GMT-06:00 America/Chicago    -93.36    45.13\n23  GMT-06:00 America/Chicago    -93.36    45.12\n24  GMT-06:00 America/Chicago    -93.86    45.17\n25  GMT-06:00 America/Chicago    -93.29    44.75\n26  GMT-06:00 America/Chicago    -93.29    44.74\n27  GMT-06:00 America/Chicago    -93.30    44.75\n28  GMT-06:00 America/Chicago    -93.21    45.57\n29  GMT-06:00 America/Chicago    -93.39    45.16\n30  GMT-06:00 America/Chicago    -93.39    45.17\n31  GMT-06:00 America/Chicago    -93.52    44.86\n32  GMT-06:00 America/Chicago    -93.54    44.86\n33  GMT-06:00 America/Chicago    -93.60    44.83\n34  GMT-06:00 America/Chicago    -93.35    45.20\n35  GMT-06:00 America/Chicago    -93.28    45.15\n36  GMT-06:00 America/Chicago    -93.27    45.13\n37  GMT-06:00 America/Chicago    -92.96    44.83\n38  GMT-06:00 America/Chicago    -92.93    44.82\n39  GMT-06:00 America/Chicago    -93.36    45.05\n40  GMT-06:00 America/Chicago    -92.17    46.81\n41  GMT-06:00 America/Chicago    -92.16    46.80\n42  GMT-06:00 America/Chicago    -92.08    46.82\n43  GMT-06:00 America/Chicago    -92.16    46.81\n44  GMT-06:00 America/Chicago    -92.10    46.78\n45  GMT-06:00 America/Chicago    -93.21    44.79\n46  GMT-06:00 America/Chicago    -93.17    44.83\n47  GMT-06:00 America/Chicago    -93.17    44.84\n48  GMT-06:00 America/Chicago    -93.19    44.80\n49  GMT-06:00 America/Chicago    -93.21    44.81\n50  GMT-06:00 America/Chicago    -93.40    44.84\n51  GMT-06:00 America/Chicago    -93.43    44.85\n52  GMT-06:00 America/Chicago    -93.42    44.89\n53  GMT-06:00 America/Chicago    -93.43    44.85\n54  GMT-06:00 America/Chicago    -93.44    44.86\n55  GMT-06:00 America/Chicago    -93.33    44.88\n56  GMT-06:00 America/Chicago    -93.36    44.91\n57  GMT-06:00 America/Chicago    -93.32    44.88\n58  GMT-06:00 America/Chicago    -93.33    44.91\n59  GMT-06:00 America/Chicago    -93.33    44.88\n60  GMT-06:00 America/Chicago    -93.35    44.86\n61  GMT-06:00 America/Chicago    -93.35    44.91\n62  GMT-06:00 America/Chicago    -93.32    44.88\n63  GMT-06:00 America/Chicago    -93.33    44.87\n64  GMT-06:00 America/Chicago    -93.56    45.32\n65  GMT-06:00 America/Chicago    -93.57    44.90\n66  GMT-06:00 America/Chicago    -93.00    45.27\n67  GMT-06:00 America/Chicago    -93.01    45.28\n68  GMT-06:00 America/Chicago    -93.25    45.06\n69  GMT-06:00 America/Chicago    -93.38    44.98\n70  GMT-06:00 America/Chicago    -93.53    47.21\n71  GMT-06:00 America/Chicago    -92.89    44.74\n72  GMT-06:00 America/Chicago    -93.42    44.93\n73  GMT-06:00 America/Chicago    -94.38    44.87\n74  GMT-06:00 America/Chicago    -93.08    44.82\n75  GMT-06:00 America/Chicago    -93.18    44.72\n76  GMT-06:00 America/Chicago    -93.28    44.71\n77  GMT-06:00 America/Chicago    -93.28    44.69\n78  GMT-06:00 America/Chicago    -93.29    44.68\n79  GMT-06:00 America/Chicago    -93.11    45.18\n80  GMT-06:00 America/Chicago    -93.95    44.17\n81  GMT-06:00 America/Chicago    -93.95    44.17\n82  GMT-06:00 America/Chicago    -93.49    45.13\n83  GMT-06:00 America/Chicago    -93.48    45.13\n84  GMT-06:00 America/Chicago    -93.45    45.10\n85  GMT-06:00 America/Chicago    -93.44    45.09\n86  GMT-06:00 America/Chicago    -93.53    45.05\n87  GMT-06:00 America/Chicago    -93.27    44.97\n88  GMT-06:00 America/Chicago    -93.36    45.05\n89  GMT-06:00 America/Chicago    -93.24    44.96\n90  GMT-06:00 America/Chicago    -93.24    44.95\n91  GMT-06:00 America/Chicago    -93.24    44.98\n92  GMT-06:00 America/Chicago    -93.23    44.97\n93  GMT-06:00 America/Chicago    -93.27    44.97\n94  GMT-06:00 America/Chicago    -93.29    44.96\n95  GMT-06:00 America/Chicago    -93.24    44.97\n96  GMT-06:00 America/Chicago    -93.27    44.98\n97  GMT-06:00 America/Chicago    -93.28    44.96\n98  GMT-06:00 America/Chicago    -93.23    44.97\n99  GMT-06:00 America/Chicago    -93.25    44.97\n100 GMT-06:00 America/Chicago    -93.25    45.07\n101 GMT-06:00 America/Chicago    -93.27    44.98\n102 GMT-06:00 America/Chicago    -93.27    44.98\n103 GMT-06:00 America/Chicago    -93.28    44.98\n104 GMT-06:00 America/Chicago    -93.29    44.91\n105 GMT-06:00 America/Chicago    -93.23    45.00\n106 GMT-06:00 America/Chicago    -93.28    44.97\n107 GMT-06:00 America/Chicago    -93.27    44.98\n108 GMT-06:00 America/Chicago    -93.45    44.97\n109 GMT-06:00 America/Chicago    -93.44    44.97\n110 GMT-06:00 America/Chicago    -93.50    44.92\n111 GMT-06:00 America/Chicago    -93.77    45.29\n112 GMT-06:00 America/Chicago    -96.77    46.85\n113 GMT-06:00 America/Chicago    -96.75    46.86\n114 GMT-06:00 America/Chicago    -93.39    45.03\n115 GMT-06:00 America/Chicago    -93.02    45.03\n116 GMT-06:00 America/Chicago    -93.19    44.43\n117 GMT-06:00 America/Chicago    -92.85    45.03\n118 GMT-06:00 America/Chicago    -92.96    44.96\n119 GMT-06:00 America/Chicago    -92.96    44.96\n120 GMT-06:00 America/Chicago    -93.56    45.28\n121 GMT-06:00 America/Chicago    -93.24    44.08\n122 GMT-06:00 America/Chicago    -93.25    44.09\n123 GMT-06:00 America/Chicago    -93.48    45.02\n124 GMT-06:00 America/Chicago    -93.45    45.03\n125 GMT-06:00 America/Chicago    -93.45    45.01\n126 GMT-06:00 America/Chicago    -92.58    44.57\n127 GMT-06:00 America/Chicago    -93.25    44.88\n128 GMT-06:00 America/Chicago    -93.29    44.86\n129 GMT-06:00 America/Chicago    -92.46    44.02\n130 GMT-06:00 America/Chicago    -92.53    44.07\n131 GMT-06:00 America/Chicago    -92.50    44.06\n132 GMT-06:00 America/Chicago    -92.48    44.00\n133 GMT-06:00 America/Chicago    -92.47    43.95\n134 GMT-06:00 America/Chicago    -92.47    44.02\n135 GMT-06:00 America/Chicago    -92.47    44.06\n136 GMT-06:00 America/Chicago    -93.55    45.20\n137 GMT-06:00 America/Chicago    -93.55    45.20\n138 GMT-06:00 America/Chicago    -93.13    44.73\n139 GMT-06:00 America/Chicago    -93.16    45.01\n140 GMT-06:00 America/Chicago    -93.18    45.01\n141 GMT-06:00 America/Chicago    -93.17    45.01\n142 GMT-06:00 America/Chicago    -93.18    45.01\n143 GMT-06:00 America/Chicago    -93.08    44.89\n144 GMT-06:00 America/Chicago    -93.15    44.94\n145 GMT-06:00 America/Chicago    -93.38    44.75\n146 GMT-06:00 America/Chicago    -93.38    44.74\n147 GMT-06:00 America/Chicago    -93.41    44.78\n148 GMT-06:00 America/Chicago    -93.14    45.06\n149 GMT-06:00 America/Chicago    -93.12    45.08\n150 GMT-06:00 America/Chicago    -94.15    45.57\n151 GMT-06:00 America/Chicago    -93.39    44.94\n152 GMT-06:00 America/Chicago    -93.21    44.88\n153 GMT-06:00 America/Chicago    -93.21    44.88\n154 GMT-06:00 America/Chicago    -93.21    44.88\n155 GMT-06:00 America/Chicago    -94.19    45.55\n156 GMT-06:00 America/Chicago    -93.33    44.95\n157 GMT-06:00 America/Chicago    -93.34    44.93\n158 GMT-06:00 America/Chicago    -93.14    44.94\n159 GMT-06:00 America/Chicago    -93.03    44.95\n160 GMT-06:00 America/Chicago    -93.19    44.92\n161 GMT-06:00 America/Chicago    -93.17    44.95\n162 GMT-06:00 America/Chicago    -93.10    44.95\n163 GMT-06:00 America/Chicago    -92.95    45.00\n164 GMT-06:00 America/Chicago    -93.17    44.97\n165 GMT-06:00 America/Chicago    -93.16    44.95\n166 GMT-06:00 America/Chicago    -92.84    45.04\n167 GMT-06:00 America/Chicago    -92.84    45.03\n168 GMT-06:00 America/Chicago    -93.06    45.05\n169 GMT-06:00 America/Chicago    -93.06    45.08\n170 GMT-06:00 America/Chicago    -92.55    47.51\n171 GMT-06:00 America/Chicago    -93.77    44.85\n172 GMT-06:00 America/Chicago    -94.22    45.55\n173 GMT-06:00 America/Chicago    -93.51    44.97\n174 GMT-06:00 America/Chicago    -93.08    44.90\n175 GMT-06:00 America/Chicago    -93.02    45.04\n176 GMT-06:00 America/Chicago    -93.02    45.07\n177 GMT-06:00 America/Chicago    -91.62    44.03\n178 GMT-06:00 America/Chicago    -92.94    44.94\n179 GMT-06:00 America/Chicago    -92.96    44.93\n180 GMT-06:00 America/Chicago    -92.91    44.94\n181 GMT-06:00 America/Chicago    -92.90    44.94\n182 GMT-06:00 America/Chicago    -92.93    44.92\n183 GMT-06:00 America/Chicago    -92.96    44.93\n184 GMT-06:00 America/Chicago    -92.94    44.89\n\n\n\nCodestarbucks_mn &lt;- starbucks |&gt;   \n  filter(state.province== \"MN\")\n\n\n\nCode#|fig_cap: \"A spatial point map displaying the locations of Starbucks stores within Minnesota. The distribution highlights store density in the Twin Cities which is the major metropolitan area.\"\n\n#|fig_alt: \"A point map (interactive) of Minnesota showing the locations of Starbucks stores. Densely clustered points appear in major cities, with sparser distribution in rural areas. Another key takeaway is that there are more concentrated in the Twin Cities than the rest of Minnesota. The map provides a visual representation of Starbucks' geographic presence across the state. Data is from starbucks https://mac-stat.github.io/data/starbucks.csv\".\n\nleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"darkgreen\"))",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "bw/exam1.html",
    "href": "bw/exam1.html",
    "title": "\n6  Exam 1\n",
    "section": "",
    "text": "Codelibrary(tidytuesdayR)\nlibrary(tidyverse)\n\n\n\nCode# Get the Data\n\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-02-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 8)\n\n\nfood_consumption &lt;- tuesdata$food_consumption\n\n\n\nCodedim(food_consumption)\n\n[1] 1430    4\n\nCode#nrow(food_consumption)\n#ncol(food_consumption)\nhead(food_consumption)\n\n# A tibble: 6 × 4\n  country   food_category consumption co2_emmission\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Argentina Pork                10.5          37.2 \n2 Argentina Poultry             38.7          41.5 \n3 Argentina Beef                55.5        1712   \n4 Argentina Lamb & Goat          1.56         54.6 \n5 Argentina Fish                 4.36          6.96\n6 Argentina Eggs                11.4          10.5 \n\nCodenames(food_consumption)\n\n[1] \"country\"       \"food_category\" \"consumption\"   \"co2_emmission\"\n\nCodestructure(food_consumption)\n\n# A tibble: 1,430 × 4\n   country   food_category            consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                           10.5          37.2 \n 2 Argentina Poultry                        38.7          41.5 \n 3 Argentina Beef                           55.5        1712   \n 4 Argentina Lamb & Goat                     1.56         54.6 \n 5 Argentina Fish                            4.36          6.96\n 6 Argentina Eggs                           11.4          10.5 \n 7 Argentina Milk - inc. cheese            195.          278.  \n 8 Argentina Wheat and Wheat Products      103.           19.7 \n 9 Argentina Rice                            8.77         11.2 \n10 Argentina Soybeans                        0             0   \n# ℹ 1,420 more rows\n\nCodetail(food_consumption)\n\n# A tibble: 6 × 4\n  country    food_category            consumption co2_emmission\n  &lt;chr&gt;      &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n1 Bangladesh Eggs                            2.08          1.91\n2 Bangladesh Milk - inc. cheese             21.9          31.2 \n3 Bangladesh Wheat and Wheat Products       17.5           3.33\n4 Bangladesh Rice                          172.          220.  \n5 Bangladesh Soybeans                        0.61          0.27\n6 Bangladesh Nuts inc. Peanut Butter         0.72          1.27\n\n\n\nCode#Shows first 22 rows\nhead(food_consumption, 22)\n\n# A tibble: 22 × 4\n   country   food_category            consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                           10.5          37.2 \n 2 Argentina Poultry                        38.7          41.5 \n 3 Argentina Beef                           55.5        1712   \n 4 Argentina Lamb & Goat                     1.56         54.6 \n 5 Argentina Fish                            4.36          6.96\n 6 Argentina Eggs                           11.4          10.5 \n 7 Argentina Milk - inc. cheese            195.          278.  \n 8 Argentina Wheat and Wheat Products      103.           19.7 \n 9 Argentina Rice                            8.77         11.2 \n10 Argentina Soybeans                        0             0   \n# ℹ 12 more rows\n\n\n\nCode#Shows last 22 rows\ntail(food_consumption, 22)\n\n# A tibble: 22 × 4\n   country food_category            consumption co2_emmission\n   &lt;chr&gt;   &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Liberia Pork                            4.01         14.2 \n 2 Liberia Poultry                         8.91          9.57\n 3 Liberia Beef                            0.78         24.1 \n 4 Liberia Lamb & Goat                     0.48         16.8 \n 5 Liberia Fish                            4.13          6.59\n 6 Liberia Eggs                            2.05          1.88\n 7 Liberia Milk - inc. cheese              3.04          4.33\n 8 Liberia Wheat and Wheat Products       11.0           2.09\n 9 Liberia Rice                           94.8         121.  \n10 Liberia Soybeans                        0.63          0.28\n# ℹ 12 more rows\n\n\nThere are 4 variables “country” “food_category” “consumption” “co2_emmission”\nWhat does consumption of each food category in each country look like?\nA spacial plot wiht a map showing the size if points with the amount of condumption and the color on the points representing the food category.\ngeom_tile = table with colors\nConcepts to make final viz more effective\n1.) labels 2.) alternate text (fig-alt) 3.) color blind friendliness (viridis_d()) 4.) text captions (fig-cap) 5.) controlling height of plot (fig-height) 6.) controlling width (fig-width)\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\") \n\nworld_coords &lt;- world %&gt;% \n  select(name, geometry) %&gt;% \n  mutate(centroid = st_centroid(geometry),\n         lon = st_coordinates(centroid)[,1],\n         lat = st_coordinates(centroid)[,2]) %&gt;%\n  select(name, lon, lat)\n\nfood_map_data &lt;- food_consumption %&gt;%\n  left_join(world_coords, by = c(\"country\" = \"name\")) %&gt;%\n  filter(!is.na(lon) & !is.na(lat))  \n\nggplot() +\n  geom_sf(data = world, fill = \"gray90\", color = \"white\") +  # Base map\n  geom_point(data = food_map_data, \n             aes(x = lon, y = lat, size = consumption, color = food_category),\n             alpha = 0.7) +  \n  scale_size(range = c(1, 10)) +  \n  labs(title = \"Global Food Consumption\", subtitle = \"(Size = Consumption, Color = Food Category)\", x = \"Longitude\", y = \"Latitude\", caption= \"Data from: TidyTuesday Github Repository, Created by: Martha Miller, Date: 02/23/2025\") +\n  theme_minimal()",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exam 1</span>"
    ]
  },
  {
    "objectID": "bw/solo_project.html",
    "href": "bw/solo_project.html",
    "title": "\n7  Solo Project\n",
    "section": "",
    "text": "Codepolling_places &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-16/polling_places.csv')\n\n\n\nCodelibrary(tidyverse)\n\n\n\nCodewi_polling_places &lt;- polling_places |&gt;\n  filter(state %in% c(\"WI\"))\n\n\n\nCodelibrary(sf)\nlibrary(ggplot2)\nlibrary(tigris)\n\n\n\nCodewi_counties &lt;- counties(state = \"WI\", cb = TRUE, class = \"sf\")\n\n\nDownloading: 5.5 kB     \nDownloading: 5.5 kB     \nDownloading: 11 kB     \nDownloading: 11 kB     \nDownloading: 27 kB     \nDownloading: 27 kB     \nDownloading: 40 kB     \nDownloading: 40 kB     \nDownloading: 57 kB     \nDownloading: 57 kB     \nDownloading: 66 kB     \nDownloading: 66 kB     \nDownloading: 82 kB     \nDownloading: 82 kB     \nDownloading: 98 kB     \nDownloading: 98 kB     \nDownloading: 110 kB     \nDownloading: 110 kB     \nDownloading: 110 kB     \nDownloading: 110 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 250 kB     \nDownloading: 250 kB     \nDownloading: 250 kB     \nDownloading: 250 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 510 kB     \nDownloading: 510 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 600 kB     \nDownloading: 600 kB     \nDownloading: 600 kB     \nDownloading: 600 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 750 kB     \nDownloading: 750 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 780 kB     \nDownloading: 780 kB     \nDownloading: 800 kB     \nDownloading: 800 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 830 kB     \nDownloading: 830 kB     \nDownloading: 850 kB     \nDownloading: 850 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 880 kB     \nDownloading: 880 kB     \nDownloading: 900 kB     \nDownloading: 900 kB     \nDownloading: 920 kB     \nDownloading: 920 kB     \nDownloading: 930 kB     \nDownloading: 930 kB     \nDownloading: 950 kB     \nDownloading: 950 kB     \nDownloading: 960 kB     \nDownloading: 960 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 1,000 kB     \nDownloading: 1,000 kB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.8 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.2 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.3 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.4 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 3.9 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.1 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.3 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.5 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.8 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.3 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.5 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.2 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.4 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.6 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.8 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.3 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.6 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.8 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.3 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.6 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.8 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.2 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.4 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.7 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 9.9 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \n\n\n\nCodepolling_counts &lt;- polling_places |&gt;\n  count(county = county_name)\n\n\n\nCodewi_map &lt;- left_join(wi_counties, polling_counts, by = c(\"NAME\" = \"county\"))\n\n\n\nCode#|fig_alt: \"A spacial map of Wisconsin broken into county boundaries. The color of the county represents the amount of polling places within the county. The lighter color designates a county that has more polling places where a darker color is lesser amounts.\"\n#|\nggplot(wi_map) +\n  geom_sf(aes(fill=n), color=\"white\")+\n  scale_fill_viridis_c(option=\"magma\", na.value = \"grey\")+\n  labs(title= \"Polling Places in Wisconsin by County (2020-2021)\", fill= \"Polling Places\", caption= \"Data from: TidyTuesday Github Repository, Created by: Martha Miller, Date: 04/07/2025\", )+\n  theme_minimal()\n\n\n\n\n\n\n\nThis plot of the polling_places data set shows that the counties in the southern part of Wisconsin have the most polling places. This is because those counties have the larger cities of Milwaukee and Madison in them with more population. Northern Wisconsin has much fewer people and needs less options for polling places.",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Solo Project</span>"
    ]
  },
  {
    "objectID": "bw/exam2.html",
    "href": "bw/exam2.html",
    "title": "\n8  Exam 2\n",
    "section": "",
    "text": "Codelibrary(tidyverse)\nfc &lt;- read_csv(\"../data/food_consumption.csv\")\n\n\n\nCodelibrary(tidytuesdayR)\nlibrary(rnaturalearth)\nlibrary(sf)\n\n\n\nCodestr(fc)\n\nspc_tbl_ [1,430 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country      : chr [1:1430] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ food_category: chr [1:1430] \"Pork\" \"Poultry\" \"Beef\" \"Lamb & Goat\" ...\n $ consumption  : num [1:1430] 10.51 38.66 55.48 1.56 4.36 ...\n $ co2_emmission: num [1:1430] 37.2 41.53 1712 54.63 6.96 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   food_category = col_character(),\n  ..   consumption = col_double(),\n  ..   co2_emmission = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nCodehead(fc)\n\n# A tibble: 6 × 4\n  country   food_category consumption co2_emmission\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Argentina Pork                10.5          37.2 \n2 Argentina Poultry             38.7          41.5 \n3 Argentina Beef                55.5        1712   \n4 Argentina Lamb & Goat          1.56         54.6 \n5 Argentina Fish                 4.36          6.96\n6 Argentina Eggs                11.4          10.5 \n\n\n\nCodefc |&gt;\n  select(food_category)\n\n# A tibble: 1,430 × 1\n   food_category           \n   &lt;chr&gt;                   \n 1 Pork                    \n 2 Poultry                 \n 3 Beef                    \n 4 Lamb & Goat             \n 5 Fish                    \n 6 Eggs                    \n 7 Milk - inc. cheese      \n 8 Wheat and Wheat Products\n 9 Rice                    \n10 Soybeans                \n# ℹ 1,420 more rows\n\n\n\nCodefcc &lt;- fc |&gt;\n  mutate(food_category = case_when (\n    food_category == \"Lamb & Goat\" ~ \"Lamb\",\n     food_category == \"Milk - inc. cheese\" ~ \"Dairy\",\n     food_category == \"Wheat and Wheat Products\" ~ \"Wheat\",\n     food_category == \"Nuts inc. Peanut Butter\" ~ \"Nuts\",\n    TRUE ~ food_category\n  ))\n\nhead(fcc,11)\n\n# A tibble: 11 × 4\n   country   food_category consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                10.5          37.2 \n 2 Argentina Poultry             38.7          41.5 \n 3 Argentina Beef                55.5        1712   \n 4 Argentina Lamb                 1.56         54.6 \n 5 Argentina Fish                 4.36          6.96\n 6 Argentina Eggs                11.4          10.5 \n 7 Argentina Dairy              195.          278.  \n 8 Argentina Wheat              103.           19.7 \n 9 Argentina Rice                 8.77         11.2 \n10 Argentina Soybeans             0             0   \n11 Argentina Nuts                 0.49          0.87\n\n\n\nCode#cc &lt;- fc |&gt;\n # group_by(country) |&gt;\n  #summarise(total_consumption = sum(consumption, na.rm = TRUE)) |&gt;\n  #arrange(desc(total_consumption)) |&gt;\n\n#print(cc)\n\n\n\nCode#|fig_cap: \"Faceted maps showing global food consumption by each category of food. There are 11 categories of food. It shows that the milk and cheese category has byfar the most consumed. \"\n\n#|fig_alt: \"A facet wrapped map showing global food consumption. Maps are faceted by each food category on a map showing the world. Lighter areas on the map represent a greater kg/person/year for each food category. Data is \"food_consumption.csv\".\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nfood_by_country_category &lt;- fc|&gt;\n  group_by(country, food_category) |&gt;\n  summarise(total_consumption = sum(consumption, na.rm = TRUE)) |&gt;\n  ungroup()\n\nmap_data &lt;- world |&gt;\n  left_join(food_by_country_category, by = c(\"name\" = \"country\"))\n\nggplot(map_data) +\n  geom_sf(aes(fill = total_consumption), color = \"gray70\", size = 0.1) +\n  scale_fill_viridis_c(option = \"C\", na.value = \"lightgray\") +\n  facet_wrap(~ food_category, ncol= 2) +\n  labs(\n    title = \"Global Food Consumption by Category\",\n    fill = \"kg/person/year\", caption= \"Data from: TidyTuesday Github Repository, Created by: Martha Miller, Date: 02/23/2025\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(size = 10),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Exam 2</span>"
    ]
  },
  {
    "objectID": "ica/ica-uni.html",
    "href": "ica/ica-uni.html",
    "title": "Univariate Viz",
    "section": "",
    "text": "11.1 Exercises\n# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-uni.html#exercises",
    "href": "ica/ica-uni.html#exercises",
    "title": "Univariate Viz",
    "section": "",
    "text": "11.1.1 Exercise 1: Research Questions\nLet’s dig into the hikes data, starting with the elevation and difficulty ratings of the hikes:\n\nhead(hikes)\n\n             peak elevation difficulty ascent length time    rating\n1     Mt. Marcy        5344          5   3166   14.8 10.0  moderate\n2 Algonquin Peak       5114          5   2936    9.6  9.0  moderate\n3   Mt. Haystack       4960          7   3570   17.8 12.0 difficult\n4   Mt. Skylight       4926          7   4265   17.9 15.0 difficult\n5 Whiteface Mtn.       4867          4   2535   10.4  8.5      easy\n6       Dix Mtn.       4857          5   2800   13.2 10.0  moderate\n\n\n\nWhat features would we like a visualization of the categorical difficulty rating variable to capture? We would like to create a layer that demonstrates the changes in scale between each peak.\nWhat about a visualization of the quantitative elevation variable? We would like to create a layer that shows how the elevation changes at each peak (relationship between data points). ### Exercise 2: Load tidyverse {.unnumbered}\n\nIn order to use ggplot tools, we have to first load the tidyverse package in which they live. We’ve installed the package but we need to tell R when we want to use it. Run the chunk below to load the library. You’ll need to do this within any .qmd file that uses ggplot().\n\n# Load the package\nlibrary(tidyverse)\n\nExercise 3: Bar Chart of Ratings - Part 1\nConsider some specific research questions about the difficulty rating of the hikes:\n\nHow many hikes fall into each category?\nAre the hikes evenly distributed among these categories, or are some more common than others?\n\nAll of these questions can be answered with: (1) a bar chart; of (2) the categorical data recorded in the rating column. First, set up the plotting frame:\n\nggplot(hikes, aes(x = rating))\n\n\n\n\n\n\n\nThink about:\n\nWhat did this do? What do you observe? Blank axis with the three different ratings.\nWhat, in general, is the first argument of the ggplot() function? To look in the data file titled hikes.\nWhat is the purpose of writing x = rating? Tells R to make the x-axis out of the different possible data points in rating.\nWhat do you think aes stands for?!? Aesthetics\n\n11.1.2 Exercise 4: Bar Chart of Ratings - Part 2\nNow let’s add a geometric layer to the frame / canvas, and start customizing the plot’s theme. To this end, try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\nNOTE:\n\nPay attention to the general code properties and structure, not memorization.\nNot all of these are “good” plots. We’re just exploring ggplot.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar() +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  labs(x = \"Rating\", y = \"Number of hikes\") +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise 5: Bar Chart Follow-up\nPart a\nReflect on the ggplot() code.\n\nWhat’s the purpose of the +? When do we use it? Adding a component to the plot.\nWe added the bars using geom_bar()? Why “geom”? Geometric layer to the frame/canvas\nWhat does labs() stand for? labels\nWhat’s the difference between color and fill? Color is border color and fill is the color within the individual bars.\nPart b\nIn general, bar charts allow us to examine the following properties of a categorical variable:\n\n\nobserved categories: What categories did we observe?\n\nRating (easy, moderate, difficult)\n\n\nvariability between categories: Are observations evenly spread out among the categories, or are some categories more common than others?\n\nSome categories are more common than others.\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Summarize below what you learned from the bar chart, in context.\nThe bar chart shows us that moderate hikes were the most common type of hike, with difficult hikes being the least common. There are roughly 45 total hikes.\nPart c\nIs there anything you don’t like about this barplot? For example: check out the x-axis again.\nI wish that the x-axis was in the order of easy to moderate to difficult (I don’t need them to be in order of ascending frequency). I also would like a few more numbers on the y-axis.\nExercise 6: Sad Bar Chart\nLet’s now consider some research questions related to the quantitative elevation variable:\n\nAmong the hikes, what’s the range of elevation and how are the hikes distributed within this range (e.g. evenly, in clumps, “normally”)?\nWhat’s a typical elevation?\nAre there any outliers, i.e. hikes that have unusually high or low elevations?\n\nHere:\n\nConstruct a bar chart of the quantitative elevation variable.\nExplain why this might not be an effective visualization for this and other quantitative variables. (What questions does / doesn’t it help answer?)\n\nThis bar chart is an effective visualization to see whether there are peaks with the same elevation, but it does not show individual peaks, nor does it emphasize individual data points–the line is distracting… could have done better with a point. Hard to see where they clump. There is not an average line to compare the data with, which would demonstrate typical elevation.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_bar(fill = \"purple\")  +\n  labs(x = \"Elevation\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise 7: A Histogram of Elevation\nQuantitative variables require different viz than categorical variables. Especially when there are many possible outcomes of the quantitative variable. It’s typically insufficient to simply count up the number of times we’ve observed a particular outcome as the bar graph did above. It gives us a sense of ranges and typical outcomes, but not a good sense of how the observations are distributed across this range. We’ll explore two methods for graphing quantitative variables: histograms and density plots.\nHistograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Check out the example below:\n\nPart a\nLet’s dig into some details.\n\nHow many hikes have an elevation between 4500 and 4700 feet? ~6 hikes\nHow many total hikes have an elevation of at least 5100 feet? ~2\nPart b\nNow the bigger picture. In general, histograms allow us to examine the following properties of a quantitative variable:\n\n\ntypical outcome: Where’s the center of the data points? What’s typical?\n\nvariability & range: How spread out are the outcomes? What are the max and min outcomes?\n\nshape: How are values distributed along the observed range? Is the distribution symmetric, right-skewed, left-skewed, bi-modal, or uniform (flat)?\n\noutliers: Are there any outliers, i.e. outcomes that are unusually large/small?\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Addressing each of the features in the above list, summarize below what you learned from the histogram, in context.\nThe typical elevation of the hikes in the Adirondacks is roughly 4300 but the most common elevation is 4000 ft. The minimum outcome is ~3750 ft and the maximum elevation is 5500 ft.The graph seems to be skewed right. Outliers may exist in hikes above 5100 ft.\nExercise 8: Building Histograms - Part 1\n2-MINUTE CHALLENGE: Thinking of the bar chart code, try to intuit what line you can tack on to the below frame of elevation to add a histogram layer. Don’t forget a +. If it doesn’t come to you within 2 minutes, no problem – all will be revealed in the next exercise.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"pink\", fill = \"purple\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nExercise 9: Building Histograms - Part 2\nLet’s build some histograms. Try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\nExercise 10: Histogram Follow-up\n\nWhat function added the histogram layer / geometry?\n\ngeom_histogram\n\nWhat’s the difference between color and fill?\n\ncolor is thr outline, and fill is the color within the bars\n\nWhy does adding color = \"white\" improve the visualization?\n\nBecause we are able to distinguish between the different bars (otherwise it becomes a blob)\n\nWhat did binwidth do?\n\nBinwidth changes the size of the bars (the range of elevation that we are counting the hikes in)\n\nWhy does the histogram become ineffective if the binwidth is too big (e.g. 1000 feet)?\n\nEncapsulates too much of the data set (no patterns can be seen)\n\nWhy does the histogram become ineffective if the binwidth is too small (e.g. 5 feet)?\n\nIt becomes a bar chart again! We cannot see any significant patterns because the bars are too skinny and the array is too limited.\nExercise 11: Density Plots\nDensity plots are essentially smooth versions of the histogram. Instead of sorting observations into discrete bins, the “density” of observations is calculated across the entire range of outcomes. The greater the number of observations, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range.\nCheck out a density plot of elevation. Notice that the y-axis (density) has no contextual interpretation – it’s a relative measure. The higher the density, the more common are elevations in that range.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density()\n\n\n\n\n\n\n\nQuestions\n\n\nINTUITION CHECK: Before tweaking the code and thinking back to geom_bar() and geom_histogram(), how do you anticipate the following code will change the plot?\n\ngeom_density(color = \"blue\")\ngeom_density(fill = \"orange\")\n\n\nTRY IT! Test out those lines in the chunk below. Was your intuition correct?\n\n\nggplot(hikes, aes(x = elevation)) +\ngeom_density(color = \"blue\", fill = \"orange\")\n\n\n\n\n\n\n\n\nExamine the density plot. How does it compare to the histogram? What does it tell you about the typical elevation, variability / range in elevations, and shape of the distribution of elevations within this range?\n\nWe get a more complete breakdown of the density of elevations. I.e. how many hikes at each elevation. More clearly shows the skew of the plot and potential outliers.\nExercise 12: Density Plots vs Histograms\nThe histogram and density plot both allow us to visualize the behavior of a quantitative variable: typical outcome, variability / range, shape, and outliers. What are the pros/cons of each? What do you like/not like about each?\nI like the smooth look of the density plot… it lets us see the skew more clearly, but I think that it could be more clear to talk about frequency of hikes.\nExercise 13: Code = communication\nWe obviously won’t be done until we talk about communication. All code above has a similar general structure (where the details can change):\n\nggplot(___, aes(x = ___)) + \n  geom___(color = \"___\", fill = \"___\") + \n  labs(x = \"___\", y = \"___\")\n\n\nThough not necessary to the code working, it’s common, good practice to indent or tab the lines of code after the first line (counterexample below). Why?\n\n\n# YUCK\nggplot(hikes, aes(x = elevation)) +\ngeom_histogram(color = \"white\", binwidth = 200) +\nlabs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nThough not necessary to the code working, it’s common, good practice to put a line break after each + (counterexample below). Why?\n\n\n# YUCK \nggplot(hikes, aes(x = elevation)) + geom_histogram(color = \"white\", binwidth = 200) + labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\nExercise 14: Practice\nPart a\nPractice your viz skills to learn about some of the variables in one of the following datasets from the previous class:\n\n# Data on students in this class\nsurvey &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/survey.csv\")\n\n# World Cup data\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\n\n# Load the package\nlibrary(tidyverse)\n\nggplot(survey, aes(x = hangout)) + \n  geom_bar(color = \"blue\", fill = \"pink\") + \n  labs(x = \"spot\", y = \"frequency\")\n\n\n\n\n\n\n\nPart b\nCheck out the RStudio Data Visualization cheat sheet to learn more features of ggplot.\nWhen done, don’t forgot to click Render Book and check the resulting HTML files. If happy, jump to GitHub Desktop and commit the changes with the message Finish activity 3 and push to GitHub. Wait few seconds, then visit your portfolio website and make sure the changes are there.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html",
    "href": "ica/ica-bi.html",
    "title": "\n12  Bivariate Viz\n",
    "section": "",
    "text": "Use this file for practice with the bivariate viz in-class activity. Refer to the class website for details.\n\n13 Import data\n\nsurvey &lt;- read.csv(\"https://ajohns24.github.io/data/112/about_us_2024.csv\") \n\n\n14 How many students have now filled out the survey?\n\n15 hmmm… nope!\nhead(survey) #did that work? I think so. nrow(survey)\n\n16 What type of variables do we have?\n\nclass (survey)\n\n[1] \"data.frame\"\n\nmode(survey) \n\n[1] \"list\"\n\ntypeof(survey)\n\n[1] \"list\"\n\nstr(survey)\n\n'data.frame':   28 obs. of  4 variables:\n $ cafe_mac         : chr  \"Cheesecake\" \"Cheese pizza\" \"udon noodles\" \"egg rolls\" ...\n $ minutes_to_campus: int  15 10 4 7 5 35 5 15 7 20 ...\n $ fave_temp        : num  18 24 18 10 18 7 75 24 13 16 ...\n $ hangout          : chr  \"the mountains\" \"a beach\" \"the mountains\" \"a beach\" ...\n\n\n\n17 Attach a package needed to use the ggplot function\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n18 Make a ggplot for hangouts\n\n#Making a ggplot for hangout\nggplot (survey, aes (x=hangout))+\ngeom_bar(color = \"blue\", fill = \"dark blue\")+\nlabs(x= \"Location\", y = \"# of Responses\")\n\n\n\n\n\n\n\n\n19 Make a ggplot for temperatures\n\nggplot (survey, aes (x=fave_temp))+\ngeom_histogram(color = \"blue\", fill = \"dark blue\")+\nlabs(x= \"Location\", y = \"# of Responses\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nggplot (survey, aes (x=fave_temp))+\ngeom_density(color = \"blue\", fill = \"dark blue\")+\nlabs(x= \"Location\", y = \"# of Responses\")\n\n\n\n\n\n\n\n\ndata.frame(temp_3pm = c(24, 26, 20, 15, 15, 15), temp_9am = c(14, 18, 15, 13, 11, 11))\n\n  temp_3pm temp_9am\n1       24       14\n2       26       18\n3       20       15\n4       15       13\n5       15       11\n6       15       11\n\n\n\nweather &lt;- data.frame(temp_3pm = c(24, 26, 20, 15, 15, 0, 40, 60, 57, 44, 51, 75),\n                      location = rep(c(\"A\", \"B\"), each = 6))\nweather\n\n   temp_3pm location\n1        24        A\n2        26        A\n3        20        A\n4        15        A\n5        15        A\n6         0        A\n7        40        B\n8        60        B\n9        57        B\n10       44        B\n11       51        B\n12       75        B\n\n\n\nggplot(weather, aes(x = temp_3pm)) +\n      geom_density()\n\n\n\n\n\n\n\n\nweather &lt;- data.frame(rain_today = c(\"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\"),\n                        location = c(rep(\"A\", 7), rep(\"B\", 5)))\n    weather\n\n   rain_today location\n1          no        A\n2          no        A\n3          no        A\n4          no        A\n5         yes        A\n6          no        A\n7         yes        A\n8          no        B\n9         yes        B\n10        yes        B\n11         no        B\n12        yes        B\n\n\n\nggplot(weather, aes(x = location)) +\n      geom_bar()\n\n\n\n\n\n\n\n\n# Load data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n# Check it out\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\n\n20 Exercise 0\n\nggplot(elections, aes(x = winner_20)) + \n  geom_bar(color = \"blue\", fill = \"pink\") + \n  labs(x = \"Party\", y = \"Vote Count\")\n\n\n\n\n\n\n\n\nggplot(elections, aes(x = repub_pct_20)) + \n  geom_histogram(color = \"blue\", fill = \"pink\") + \n  labs(x = \"Party\", y = \"Vote Count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Exercise 1\n\n# Set up the plotting frame\n# How does this differ than the frame for our histogram of repub_pct_20 alone?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16))\n\n\n\n\n\n\n#For scatter plots, there is a y and x term defined.\n\n\n# Add a layer of points for each county\n# Take note of the geom!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# Change the shape of the points\n# What happens if you change the shape to another number?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(shape = 2)\n\n\n\n\n\n\n# Changing the number of the shape term will change the shape of the points (two is triangle, ten is some weird bulls eye thing)\n\n\n# YOU TRY: Modify the code to make the points \"orange\"\n# NOTE: Try to anticipate if \"color\" or \"fill\" will be useful here. Then try both.\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color = \"orange\")\n\n\n\n\n\n\n#color is helpful, fill is not.\n\n\n# Add a layer that represents each county by the state it's in\n# Take note of the geom and the info it needs to run!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color = \"orange\")+\n  geom_text(aes(label = state_abbr))\n\n\n\n\n\n\n\n#Exercise 3: Reflect\nStrong positive linear correlation indicating that there is a strong relationship between the way a county votes from one election year to the next.\nThere seems to be a few outliers in Texas… although it is hard to see on this very crowded graph.\n#Exercise 4: Visualizing trend\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#Exercise 5: Your Turn\nTo examine how the 2020 results are related to some county demographics, construct scatterplots of repub_pct_20 vs median_rent, and repub_pct_20 vs median_age. Summarize the relationship between these two variables and comment on which is the better predictor of repub_pct_20, median_rent or median_age.\n\n# Scatterplot of repub_pct_20 vs median_rent\nggplot(elections, aes(y = repub_pct_20, x = median_rent)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThere is a fair negative linear relationship between median rent and percent of republican vote. This could indicate that with higher rents, there is less of a tendency to vote Republican.\n\n# Scatterplot of repub_pct_20 vs median_age\nggplot(elections, aes(y = repub_pct_20, x = median_age)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThis visualization shows a poor positive linear relationship between age and Republican vote, indicating that age may not be a good predictor for political voting.\n#Exercise 6: A Sad Scatterplot\nNext, let’s explore the relationship between a county’s 2020 Republican support repub_pct_20 and the historical political trends in its state. In this case repub_pct_20 is quantitative, but historical is categorical. Explain why a scatterplot might not be an effective visualization for exploring this relationship. (What questions does / doesn’t it help answer?)\n\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_point()\n\n\n\n\n\n\n\nThis graph shows that there is a wide spread of percentage of republican voting in counties that are historically blue, purple, and red, with a very slight dominance in red counties.\n#Exercise 7: Quantitative vs Categorical – Violins & Boxes\nThough the above scatterplot did group the counties by historical category, it’s nearly impossible to pick out meaningful patterns in 2020 Republican support in each category. Let’s try adding 2 different geom layers to the frame:\n\n# Side-by-side violin plots\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n# Side-by-side boxplots (defined below)\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFrom the above plots, I can surmise that there is a higher median of percent Republican votes in red counties, and that that pattern steadily decreases from red to purple to blue. There is a sharp buldge in red counties for ~75% of votes to be Republican.\n#Exercise 8: Quantitative vs Categorical – Intuition Check\nWe can also visualize the relationship between repub_pct_20 and historical using our familiar density plots. In the plot below, notice that we simply created a separate density plot for each historical category. (The plot itself is “bad” but we’ll fix it below.) Try to adjust the code chunk below, which starts with a density plot of repub_pct_20 alone, to re-create this image.\n\nggplot(elections, aes(x = repub_pct_20)) +\n  geom_density()\n\n\n\n\n\n\n\nHm… I have no idea how to adjust the code. Three minutes are up! HMMM IT WAS JUST ADDING FILL? WUT.\n#Exercise 9: Quantitative vs Categorical – Density Plots\n\n# Name two \"bad\" things about this plot\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n\n\n\n\n\nThe overlap is very confusing and I am not sure where the counties come into play…\n\n# What does scale_fill_manual do?\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n\n\n**scale_fill_manual seems to change the colors of the scale based on categorical values.\n\n# What does alpha = 0.5 do?\n# Play around with different values of alpha, between 0 and 1\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density(alpha = 0.2) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n\n\n“Alpha = some number” seems to adjust transparency. With 0 being translucent and 1 being opaque.\n\n# What does facet_wrap do?!\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ historical)\n\n\n\n\n\n\n\n“facet_wrap” separates each of the categorical values and places them side by side.\n\n# Let's try a similar grouping strategy with a histogram instead of density plot.\n# Why is this terrible?\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_histogram(color = \"white\") +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nThis is terrible because the colors suuuuck but also because I still don’t understand the county breakdown and it’s hard to see how blue compares with purple, etc except to say that red is the predominant predictor (sometimes… it’s confusing).\n#Exercise 10\nWe’ve now learned 3 (of many) ways to visualize the relationship between a quantitative and categorical variable: side-by-side violins, boxplots, and density plots.\nWhich do you like best? I like boxplots, but maybe that’s just because I have had the most exposure to them.\nWhat is one pro of density plots relative to boxplots? Pros include better side by side comparison, clearer outline of where density is at it’s max.\nWhat is one con of density plots relative to boxplots? Hard to see the spread, ie maximums and minimums are a bit convoluted in the violin plots\n#Exercise 11: Categorical vs Categorical – Intuition Check\nFinally, let’s simply explore who won each county in 2020 (winner_20) and how this breaks down by historical voting trends in the state. That is, let’s explore the relationship between 2 categorical variables! Following the same themes as above, we can utilize grouping features such as fill/color or facets to distinguish between different categories of winner_20 and historical.\n\n# Plot 1: adjust this to recreate the top plot\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()+\n  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\n# Plot 2: adjust this to recreate the bottom plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar()+\n  facet_wrap(~historical)\n\n\n\n\n\n\n\n#Exercise 12: Categorical vs Categorical Construct the following 4 bar plot visualizations.\n\n# A stacked bar plot\n# How are the \"historical\" and \"winner_20\" variables mapped to the plot, i.e. what roles do they play?\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n# A faceted bar plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() +\n  facet_wrap(~ historical)\n\n\n\n\n\n\n\n\n# A side-by-side bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n# A proportional bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n#Part a Name one pro and one con of using the “proportional bar plot” instead of one of the other three options.\n#Part b What’s your favorite bar plot from part and why?\n#Exercise 13: Practice (now or later) Import some daily weather data from a few locations in Australia:\n\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\")\n\nConstruct plots that address the research questions in each chunk. You might make multiple plots–there are many ways to do things!. However, don’t just throw spaghetti at the wall.\nReflect before doing anything. What types of variables are these? How might you plot just 1 of the variables, and then tweak the plot to incorporate the other?\n\n# How do 3pm temperatures (temp3pm) differ by location?\n# In answering this question we have two variables, one numeric and one categorical. Therefore it is probably best to have either boxplots, violin plots, or a stacked density plot. \n\n#density plot\nggplot(weather, aes(x = temp3pm, fill = location)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"pink\", \"purple\",\"green\"))+\n    facet_wrap(~ location)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_violin(fill = \"pink\")\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\n# How might we predict the 3pm temperature (temp3pm) by the 9am temperature (temp9am)?\n#Similar to the early exercises, we could make a scatter plot and fit it with a line of best fit. \nggplot(weather, aes(y = temp3pm, x = temp9am)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 27 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n# How do the number of rainy days (raintoday) differ by location?\nggplot(weather, aes(x = location, fill = raintoday)) + \n  geom_bar()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html",
    "href": "ica/ica-multi.html",
    "title": "\n13  Mulivariate Viz\n",
    "section": "",
    "text": "13.1 Review\nLet’s review some univariate and bivariate plotting concepts using some daily weather data from Australia. This is a subset of the data from the weatherAUS data in the rattle package.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\n# What are the units of observation?\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# How many data points do we have? \nnrow(weather)\n\n[1] 2367\n\n# What type of variables do we have?\nstr(weather)\n\n'data.frame':   2367 obs. of  24 variables:\n $ date         : Date, format: \"2020-01-01\" \"2020-01-02\" ...\n $ location     : chr  \"Wollongong\" \"Wollongong\" \"Wollongong\" \"Wollongong\" ...\n $ mintemp      : num  17.1 17.7 19.7 20.4 19.8 18.3 19.9 20.1 19.8 20.5 ...\n $ maxtemp      : num  23.1 24.2 26.8 35.5 21.4 22.9 25.6 23.2 23.1 25.4 ...\n $ rainfall     : num  0 0 0 0 0 0 0.8 1.6 0 0 ...\n $ evaporation  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ sunshine     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ windgustdir  : chr  \"SSW\" \"SSW\" \"NE\" \"SSW\" ...\n $ windgustspeed: int  39 37 41 78 57 35 44 41 39 56 ...\n $ winddir9am   : chr  \"SSW\" \"S\" \"NNW\" \"NE\" ...\n $ winddir3pm   : chr  \"SSE\" \"ENE\" \"NNE\" \"NNE\" ...\n $ windspeed9am : int  20 13 7 15 31 17 30 31 24 19 ...\n $ windspeed3pm : int  15 15 17 17 35 20 7 33 26 39 ...\n $ humidity9am  : int  69 72 72 77 70 71 76 77 76 79 ...\n $ humidity3pm  : int  64 54 71 69 75 71 72 76 79 76 ...\n $ pressure9am  : num  1015 1020 1018 1009 1019 ...\n $ pressure3pm  : num  1014 1018 1013 1004 1020 ...\n $ cloud9am     : int  8 7 6 NA NA NA NA 8 NA NA ...\n $ cloud3pm     : int  1 1 NA NA 7 NA NA NA NA NA ...\n $ temp9am      : num  19.1 19.8 23.4 24.5 20.7 20.9 22.9 21.3 21.2 23 ...\n $ temp3pm      : num  22.9 23.6 25.7 26.7 20 22.6 24.9 22.2 22.2 25.1 ...\n $ raintoday    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ risk_mm      : num  0 0 0 0 0 0.8 1.6 0 0 1 ...\n $ raintomorrow : chr  \"No\" \"No\" \"No\" \"No\" ...",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#review",
    "href": "ica/ica-multi.html#review",
    "title": "\n13  Mulivariate Viz\n",
    "section": "",
    "text": "Example 1\nConstruct a plot that allows us to examine how temp3pm varies.\n\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_violin()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\nggplot(weather, aes(x = temp3pm)) +\n  geom_density() \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nExample 2\nConstruct 3 plots that address the following research question:\nHow do afternoon temperatures (temp3pm) differ by location?\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_violin()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\n# Plot 3 (facets)\nggplot(weather, aes(x = temp3pm, fill = location)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ location)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nReflection\n\nTemperatures tend to be highest, and most variable, in Uluru. There, they range from ~10 to ~45 with a typical temp around ~30 degrees.\nTemperatures tend to be lowest in Hobart. There, they range from ~5 to ~45 with a typical temp around ~15 degrees.\nWollongong temps are in between and are the least variable from day to day.\n\nSUBTLETIES: Defining fill or color by a variable\nHow we define the fill or color depends upon whether we’re defining it by a named color or by some variable in our dataset. For example:\n\ngeom___(fill = \"blue\")named colors are defined outside the aesthetics and put in quotes\ngeom___(aes(fill = variable)) or ggplot(___, aes(fill = variable))\ncolors/fills defined by a variable are defined inside the aesthetics\nExample 3\nLet’s consider Wollongong alone:\n\n# Don't worry about the syntax (we'll learn it soon)\nwoll &lt;- weather |&gt;\n  filter(location == \"Wollongong\") |&gt; \n  mutate(date = as.Date(date))  \n\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday))+\n  geom_bar(fill = \"blue\")\n\n\n\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday))+\ngeom_bar(aes(fill = raintomorrow))\n\n\n\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nReflection\nThere’s often not one “best plot”, but a combination of plots that provide a complete picture:\n\nThe stacked and side-by-side bars reflect that on most days, it does not rain.\nThe proportional / filled bars lose that information, but make it easier to compare proportions: it’s more likely to rain tomorrow if it also rains today.\nExample 4\nConstruct a plot that illustrates how 3pm temperatures (temp3pm) vary by date in Wollongong. Represent each day on the plot and use a curve/line to help highlight the trends.\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\n\nggplot(woll, aes(y = temp3pm, x = date)) +\n  geom_point() +\n  geom_smooth(span = 0.5)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\n\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\n\n\n\nNOTE: A line plot isn’t always appropriate! It can be useful in situations like this, when our data are chronological.\nReflection\nThere’s a seasonal / cyclic behavior in temperatures – they’re highest in January (around 23 degrees) and lowest in July (around 16 degrees). There are also some outliers – some abnormally hot and cold days.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#new-stuff",
    "href": "ica/ica-multi.html#new-stuff",
    "title": "\n13  Mulivariate Viz\n",
    "section": "\n13.2 New Stuff",
    "text": "13.2 New Stuff\nNext, let’s consider the entire weather data for all 3 locations. The addition of location adds a 3rd variable into our research questions:\n\nHow does the relationship between raintoday and raintomorrow vary by location?\nHow does the behavior of temp3pm over date vary by location?\nAnd so on.\n\nThus far, we’ve focused on the following components of a plot:\n\nsetting up a frame\n\nadding layers / geometric elements\nsplitting the plot into facets for different groups / categories\nchange the theme, e.g. axis labels, color, fill\n\nWe’ll have to think about all of this, along with scales. Scales change the color, fill, size, shape, or other properties according to the levels of a new variable. This is different than just assigning scale by, for example, color = \"blue\".\nWork on the examples below in your groups. Check in with your intuition! We’ll then discuss as a group as relevant.\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am)) + \n  geom_point()+\n  geom_text(aes(label = location))\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() +\n  facet_wrap(~ raintoday)\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point()\n\nWarning: Removed 69 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\") +\n  facet_wrap(~ location)\n\n\n\n\n\n\n\nThere’s no end to the number and type of visualizations you could make. And it’s important to not just throw spaghetti at the wall until something sticks. FlowingData shows that one dataset can be visualized many ways, and makes good recommendations for data viz workflow, which we modify and build upon here:\n\nIdentify simple research questions.\nWhat do you want to understand about the variables or the relationships among them?\n\nStart with the basics and work incrementally.\n\nIdentify what variables you want to include in your plot and what structure these have (eg: categorical, quantitative, dates)\nStart simply. Build a plot of just 1 of these variables, or the relationship between 2 of these variables.\nSet up a plotting frame and add just one geometric layer at a time.\nStart tweaking: add whatever new variables you want to examine,\n\n\n\nAsk your plot questions.\n\nWhat questions does your plot answer? What questions are left unanswered by your plot?\nWhat new questions does your plot spark / inspire?\nDo you have the viz tools to answer these questions, or might you learn more?\n\n\nFocus.\nReporting a large number of visualizations can overwhelm the audience and obscure your conclusions. Instead, pick out a focused yet comprehensive set of visualizations.\n\n:::",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#exercises-required",
    "href": "ica/ica-multi.html#exercises-required",
    "title": "\n13  Mulivariate Viz\n",
    "section": "\n13.3 Exercises (required)",
    "text": "13.3 Exercises (required)\nThe story\nThough far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education dataset contains various education variables for each state:\n\n# Import and check out data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\nhead(education)\n\n       State expend ratio salary frac verbal math  sat  fracCat\n1    Alabama  4.405  17.2 31.144    8    491  538 1029   (0,15]\n2     Alaska  8.963  17.6 47.951   47    445  489  934 (45,100]\n3    Arizona  4.778  19.3 32.175   27    448  496  944  (15,45]\n4   Arkansas  4.459  17.1 28.934    6    482  523 1005   (0,15]\n5 California  4.992  24.0 41.078   45    417  485  902  (15,45]\n6   Colorado  5.443  18.4 34.571   29    462  518  980  (15,45]\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\nExercise 1: SAT scores\nPart a\nConstruct a plot of how the average sat scores vary from state to state. (Just use 1 variable – sat not State!)\n\nggplot(education, aes(x = sat)) + \n  geom_density()\n\n\n\n\n\n\n\nPart b\nSummarize your observations from the plot. Comment on the basics: range, typical outcomes, shape. (Any theories about what might explain this non-normal shape?)\nThere is a broad range of scores from 850 to a little past 1100. Most scores are around 900, with another lesser maximum density at 1050. And the shape is non-normal. \nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nThe first question we’d like to answer is: Can the variability in sat scores from state to state be partially explained by how much a state spends on education, specifically its per pupil spending (expend) and typical teacher salary?\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\n\nggplot(education, aes(y = sat, x = expend)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\n\nggplot(education, aes(y = sat, x = salary)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nPart b\nWhat are the relationship trends between SAT scores and spending? Is there anything that surprises you?\nIn both of the above plots we see a very loose decrease SAT scores in spending and salary. While this was initially a surprising result, I realized that the relationship is not strong, making me question whether this is a significant relationship.\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\nConstruct one visualization of the relationship of sat with salary and expend. HINT: Start with just 2 variables and tweak that code to add the third variable. Try out a few things!\n\nggplot(education, aes(y = sat, x = salary, color = expend)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\nExercise 4: Another way to Incorporate Scale\nIt can be tough to distinguish color scales and size scales for quantitative variables. Another option is to discretize a quantitative variable, or basically cut it up into categories.\nConstruct the plot below. Check out the code and think about what’s happening here. What happens if you change “2” to “3”?\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\nDescribe the trivariate relationship between sat, salary, and expend.\nIt seems like there is limited relationship between a higher salary and sat scores. However, in schools that have low salaries, we see a sharp decrease in sat scores as expenditures increase. In schools with high salaries, there is a slight upward trend with increasing expenditures leading to increased sat scores.\nExercise 5: Finally an Explanation\nIt’s strange that SAT scores seem to decrease with spending. But we’re leaving out an important variable from our analysis: the fraction of a state’s students that actually take the SAT. The fracCat variable indicates this fraction: low (under 15% take the SAT), medium (15-45% take the SAT), and high (at least 45% take the SAT).\nPart a\nBuild a univariate viz of fracCat to better understand how many states fall into each category.\n\nggplot(education, aes(x = fracCat)) +\n  geom_bar() \n\n\n\n\n\n\n\nPart b\nBuild 2 bivariate visualizations that demonstrate the relationship between sat and fracCat. What story does your graphic tell and why does this make contextual sense?\n\nggplot(education, aes(y = sat)) +\n  geom_boxplot() +\n  facet_wrap (~fracCat)\n\n\n\n\n\n\n\n\nggplot(education, aes(x = sat, fill = fracCat)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\nPart c\nMake a trivariate visualization that demonstrates the relationship of sat with expend AND fracCat. Highlight the differences in fracCat groups through color AND unique trend lines. What story does your graphic tell?\nDoes it still seem that SAT scores decrease as spending increases?\n\nggplot(education, aes(y = sat, x = expend, color = fracCat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nPart d\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why did it appear that SAT scores decrease as spending increases even though the opposite is true?",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#exercises-optional",
    "href": "ica/ica-multi.html#exercises-optional",
    "title": "\n13  Mulivariate Viz\n",
    "section": "\n13.4 Exercises (optional)",
    "text": "13.4 Exercises (optional)\nExercise 6: Heat Maps\nAs usual, we’ve only just scratched the surface! There are lots of other data viz techniques for exploring multivariate relationships. Let’s start with a heat map.\nPart a\nRun the chunks below. Check out the code, but don’t worry about every little detail! NOTES:\n\nThis is not part of the ggplot() grammar, making it a bit complicated.\nIf you’re curious about what a line in the plot does, comment it out (#) and check out what happens!\nIn the plot, for each state (row), each variable (column) is scaled to indicate whether the state has a relative high value (yellow), a relatively low value (purple), or something in between (blues/greens).\nYou can also play with the color scheme. Type ?cm.colors in the console to learn about various options.\nWe’ll improve the plot later, so don’t spend too much time trying to learn something from this plot.\n\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\nPart b\nIn the final two plots, the states (rows) are rearranged by similarity with respect to these education metrics. The final plot includes a dendrogram which further indicates clusters of similar states. In short, states that have a shorter path to connection are more similar than others.\nPutting this all together, what insight do you gain about the education trends across U.S. states? Which states are similar? In what ways are they similar? Are there any outliers with respect to 1 or more of the education metrics?\nExercise 7: Star plots\nLike heat maps, star plots indicate the relative scale of each variable for each state. Thus, we can use star maps to identify similar groups of states, and unusual states!\nPart a\nConstruct and check out the star plot below. Note that each state has a “pie”, with each segment corresponding to a different variable. The larger a segment, the larger that variable’s value is in that state. For example:\n\nCheck out Minnesota. How does Minnesota’s education metrics compare to those in other states? What metrics are relatively high? Relatively low?\nWhat states appear to be similar? Do these observations agree with those that you gained from the heat map?\n\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\nPart b\nFinally, let’s plot the state stars by geographic location! What new insight do you gain here?!\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html#solutions",
    "href": "ica/ica-multi.html#solutions",
    "title": "\n13  Mulivariate Viz\n",
    "section": "\n13.5 Solutions",
    "text": "13.5 Solutions\n\nClick for Solutions\n\nlibrary(tidyverse)\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\n# What are the units of observation?\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# How many data points do we have? \nnrow(weather)\n\n[1] 2367\n\n# What type of variables do we have?\nstr(weather)\n\n'data.frame':   2367 obs. of  24 variables:\n $ date         : Date, format: \"2020-01-01\" \"2020-01-02\" ...\n $ location     : chr  \"Wollongong\" \"Wollongong\" \"Wollongong\" \"Wollongong\" ...\n $ mintemp      : num  17.1 17.7 19.7 20.4 19.8 18.3 19.9 20.1 19.8 20.5 ...\n $ maxtemp      : num  23.1 24.2 26.8 35.5 21.4 22.9 25.6 23.2 23.1 25.4 ...\n $ rainfall     : num  0 0 0 0 0 0 0.8 1.6 0 0 ...\n $ evaporation  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ sunshine     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ windgustdir  : chr  \"SSW\" \"SSW\" \"NE\" \"SSW\" ...\n $ windgustspeed: int  39 37 41 78 57 35 44 41 39 56 ...\n $ winddir9am   : chr  \"SSW\" \"S\" \"NNW\" \"NE\" ...\n $ winddir3pm   : chr  \"SSE\" \"ENE\" \"NNE\" \"NNE\" ...\n $ windspeed9am : int  20 13 7 15 31 17 30 31 24 19 ...\n $ windspeed3pm : int  15 15 17 17 35 20 7 33 26 39 ...\n $ humidity9am  : int  69 72 72 77 70 71 76 77 76 79 ...\n $ humidity3pm  : int  64 54 71 69 75 71 72 76 79 76 ...\n $ pressure9am  : num  1015 1020 1018 1009 1019 ...\n $ pressure3pm  : num  1014 1018 1013 1004 1020 ...\n $ cloud9am     : int  8 7 6 NA NA NA NA 8 NA NA ...\n $ cloud3pm     : int  1 1 NA NA 7 NA NA NA NA NA ...\n $ temp9am      : num  19.1 19.8 23.4 24.5 20.7 20.9 22.9 21.3 21.2 23 ...\n $ temp3pm      : num  22.9 23.6 25.7 26.7 20 22.6 24.9 22.2 22.2 25.1 ...\n $ raintoday    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ risk_mm      : num  0 0 0 0 0 0.8 1.6 0 0 1 ...\n $ raintomorrow : chr  \"No\" \"No\" \"No\" \"No\" ...\n\n\nExample 1\n\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nExample 2\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\nggplot(weather, aes(y = temp3pm, x = location)) + \n  geom_boxplot()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n# Plot 3 (facets)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ location)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nExample 3\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(fill = \"blue\")\n\n\n\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(aes(fill = raintomorrow))\n\n\n\n\n\n\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nExample 4\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_point() + \n  geom_smooth(span = 0.5)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() +\n  facet_wrap(~ raintoday)\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point()\n\nWarning: Removed 69 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\") + \n  facet_wrap(~ location)\n\n\n\n\n\n\n\nExercise 1: SAT scores\nPart a\n\n# A histogram would work too!\nggplot(education, aes(x = sat)) + \n  geom_density()\n\n\n\n\n\n\n\nPart b\naverage SAT scores range from roughly 800 to 1100. They appear bi-modal.\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = salary)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nPart b\nThe higher the student expenditures and teacher salaries, the worse the SAT performance.\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\n\nggplot(education, aes(y = sat, x = salary, color = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\nExercise 4: Another Way to Incorporate Scale\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 2))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nStates with lower salaries and expenditures tend to have higher SAT scores.\nExercise 5: Finally an Explanation\nPart a\n\nggplot(education, aes(x = fracCat)) + \n  geom_bar()\n\n\n\n\n\n\n\nPart b\nThe more students in a state that take the SAT, the lower the average scores tend to be. This is probably related to self-selection.\n\nggplot(education, aes(x = sat, fill = fracCat)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\nPart c\nWhen we control for the fraction of students that take the SAT, SAT scores increase with expenditure.\n\nggplot(education, aes(y = sat, x = expend, color = fracCat)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nPart d\nStudent participation tends to be lower among states with lower expenditures (which are likely also the states with higher ed institutions that haven’t historically required the SAT). Those same states tend to have higher SAT scores because of the self-selection of who participates.\nExercise 6: Heat Maps\nPart a\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\nPart b\n\nSimilar values in verbal, math, and sat.\nHigh contrast (an inverse relationship) verbal/math/sat scores and the fraction of students that take the SAT.\nOutliers of Utah and California in ratio (more students per teacher).\nWhile grouped, fraction and salary are not as similar to each other as the sat scores; it is also interesting to notice states that have high ratios have generally low expenditures per student.\nExercise 7: Star Plots\nPart a\nMN is high on the SAT performance related metrics and low on everything else. MN is similar to Iowa, Kansas, Mississippi, Missouri, the Dakotas…\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\n\n\n\n\n\n\nPart b\nWhen the states are in geographical ordering, we’d notice more easily that states in similar regions of the U.S. have similar patterns of these variables.\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html",
    "href": "ica/ica-spatial.html",
    "title": "\n14  Spatial Viz\n",
    "section": "",
    "text": "14.1 Review\nIn the previous activity, we explored a Simpson’s Paradox–it seemed that - states with higher spending… - tend to have lower average SAT scores.\nBUT this was explained by a confounding (aka omitted and lurking) variable which is the % of students in a state that take the SAT. Hence,\nThus, when controlling for the % of students that take the SAT, more spending is correlated with higher scores.\nLet’s explore a Simpson’s paradox related to Mac!\nBack in the 2000s, Macalester invested in insulating a few campus-owned houses, with the hopes of leading to energy savings.  Former Mac Prof Danny Kaplan accessed monthly data on energy use and other info for these addresses, before and after renovations:\n# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::map()    masks maps::map()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import the data and only keep 2 addresses\nenergy &lt;- read.csv(\"https://mac-stat.github.io/data/MacNaturalGas.csv\") |&gt; \n  mutate(date = as.Date(paste0(month, \"/1/\", year), \"%m/%d/%Y\")) |&gt; \n  filter(address != \"c\")\n\n# Check it out\nhead(energy)\n\n  month year  price therms hdd address renovated       date\n1     6 2005  35.21     21   0       a        no 2005-06-01\n2     7 2005  37.37     21   0       a        no 2005-07-01\n3     8 2005  36.93     21   3       a        no 2005-08-01\n4     9 2005  62.36     39  61       a        no 2005-09-01\n5    10 2005 184.15    120 416       a        no 2005-10-01\n6    11 2005 433.35    286 845       a        no 2005-11-01\nThe part of dataset codebook is below:",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#review",
    "href": "ica/ica-spatial.html#review",
    "title": "\n14  Spatial Viz\n",
    "section": "",
    "text": "States with higher spending…\ntend to have a higher % of students of students that take the SAT…\nwhich then “leads to” lower average SAT scores.\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmeaning\n\n\n\ntherms\na measure of energy use–the more energy used, the larger the therms\n\n\naddress\na or b\n\n\nrenovated\nwhether the location had been renovated, yes or no\n\n\nmonth\nfrom 1 (January) to 12 (December)\n\n\nhdd\nmonthly heating degree days. A proxy measure of outside temperatures–the higher the hdd, the COLDER it was outside",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#examples",
    "href": "ica/ica-spatial.html#examples",
    "title": "\n14  Spatial Viz\n",
    "section": "\n14.2 Examples",
    "text": "14.2 Examples\n\nConstruct a plot that addresses each research question\nInclude a 1-sentence summary of the plot.\n\nExample 1\nWhat was range in, and typical, energy used each month, as measured by therms? How does this differ by address?\nThere is a wide variety in each plot. In month 3 (March), for example, there is a range from about 200 to 400. However, in month 7 (July) there is a significantly smaller range from 0-50. Typical energy use varies greatly by month. \n\nggplot(energy, aes(x = therms, fill = address)) + \n  geom_density(alpha = 0.5)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nExample 2\nHow did energy use (therms) change over time (date) at the two addresses?\nThere is a slight increase in energy usage at both addresses over time.\n\nggplot(energy, aes(y = therms, x = date, color = address)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nExample 3\nHow did the typical energy use (therms) at the two addresses change before and after they were renovated?\n\nggplot(energy, aes(y = therms, x = renovated, color = renovated))+\n  geom_boxplot()+\n  facet_wrap(~address)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nExample 4\nThat seems unfortunate that energy usage went up after renovations. But also fishy.\nTake 5 minutes in your groups to try and explain what’s going on here. Think: What confounding, lurking, or omitted variable related to energy usage are we ignoring here? Try to make some plots to prove your point.\n\nggplot(energy, aes(y = therms, x = hdd, color = renovated))+\n  geom_boxplot()\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nExample 5\nLet’s summarize the punchlines by filling in the ???. It seemed that:\n\nAfter renovation…\nenergy use increased.\n\nBUT this was explained by a confounding or omitted or lurking variable: ???\n\nAfter renovation…\ntherms increased at both addresses\nwhich then leads to higher energy use.\n\nThus, when controlling for hdd, renovations led to decreased energy use.\n\n# When controlling for outside temps (via hdd), energy use decreased post-renovation\nggplot(energy, aes(y = therms, x = hdd, color = renovated)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ address)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#new-stuff",
    "href": "ica/ica-spatial.html#new-stuff",
    "title": "\n14  Spatial Viz\n",
    "section": "\n14.3 New stuff",
    "text": "14.3 New stuff\nTypes of spatial viz:\n\nPoint Maps: plotting locations of individual observations\nexample: bigfoot sightings\nContour Maps: plotting the density or distribution of observations (not the individual observations themselves)\n\nChoropleth Maps: plotting outcomes in different regions\n\nNYT article on effects of redlining\nMinnesota Reformer article on how Mpls / St Paul voted on 2021 ballot measures related to mayoral, policing, and rent policies\n\n\n\nThese spatial maps can be static or dynamic/interactive.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#exercises",
    "href": "ica/ica-spatial.html#exercises",
    "title": "\n14  Spatial Viz\n",
    "section": "\n14.4 Exercises",
    "text": "14.4 Exercises\n\n14.4.1 Preview\nYou’ll explore some R spatial viz tools below. In general, there are two important pieces to every map:\nPiece 1: A dataset\nThis dataset must include either:\n\nlocation coordinates for your points of interest (for point maps); or\nvariable outcomes for your regions of interest (for choropleth maps)\n\nPiece 2: A background map\nWe need latitude and longitude coordinates to specify the boundaries for your regions of interest (eg: countries, states). This is where it gets really sticky!\n\nCounty-level, state-level, country-level, continent-level info live in multiple places.\nWhere we grab this info can depend upon whether we want to make a point map or a choropleth map. (The background maps can be used somewhat interchangeably, but it requires extra code :/)\nWhere we grab this info can also depend upon the structure of our data and how much data wrangling / cleaning we’re up for. For choropleth maps, the labels of regions in our data must match those in the background map. For example, if our data labels states with their abbreviations (eg: MN) and the background map refers to them as full names in lower case (eg: minnesota), we have to wrangle our data so that it matches the background map.\n\nIn short, the code for spatial viz gets very specialized. The goal of these exercises is to:\n\nplay around and experience the wide variety of spatial viz tools out there\nunderstand the difference between point maps and choropleth maps\nhave fun\n\nYou can skip around as you wish and it’s totally fine if you don’t finish everything. Just come back at some point to play around.\nPart 1: Interactive points on a map with leaflet\n\nLeaflet is an open-source JavaScript library for creating maps. We can use it inside R through the leaflet package.\nThis uses a different plotting framework than ggplot2, but still has a tidyverse feel (which will become more clear as we learn other tidyverse tools!).\nThe general steps are as follows:\n\nCreate a map widget by calling leaflet() and telling it the data to use.\nAdd a base map using addTiles() (the default) or addProviderTiles().\nAdd layers to the map using layer functions (e.g. addMarkers(), addPolygons()).\nPrint the map widget to display it.\nExercise 1: A leaflet with markers / points\nEarlier this semester, I asked for the latitude and longitude of one of your favorite places. I rounded these to the nearest whole number, so that they’re near to but not exactly at those places. Let’s load the data and map it!\n\nfave_places &lt;- read.csv(\"https://ajohns24.github.io/data/112/our_fave_places.csv\")\n\n# Check it out\nhead(fave_places)\n\n  latitude longitude\n1       46      -123\n2       33        52\n3       48       -90\n4       36      -112\n5       59        25\n6       39      -106\n\n\nPart a\nYou can use a “two-finger scroll” to zoom in and out.\n\n# Load the leaflet package\nlibrary(leaflet)\n\n# Just a plotting frame\nleaflet(data = fave_places)\n\n\n\n\n\n\n# Now what do we have?\nleaflet(data = fave_places) |&gt; \n  addTiles()\n\n\n\n\n\n\n# Now what do we have?\n# longitude and latitude refer to the variables in our data\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers(lng = ~longitude, lat = ~latitude)\n\n\n\n\n\n\n# Since we named them \"longitude\" and \"latitude\", the function\n# automatically recognizes these variables. No need to write them!\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\n\n\n\n\nPart b\nPLAY AROUND! This map is interactive. Zoom in on one location. Keep zooming – what level of detail can you get into? How does that detail depend upon where you try to zoom in (thus what are the limitations of this tool)?\nExercise 2: Details\nWe can change all sorts of details in leaflet maps.\n\n# Load package needed to change color\nlibrary(gplots)\n\n# We can add colored circles instead of markers at each location\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"red\"))\n\n\n\n\n\n\n# We can change the background\n# Mark locations with yellow dots\n# And connect the dots, in their order in the dataset, with green lines\n# (These green lines don't mean anything here, but would if this were somebody's travel path!)\nleaflet(data = fave_places) |&gt;\n  addProviderTiles(\"USGS\") |&gt;\n  addCircles(weight = 10, opacity = 1, color = col2hex(\"yellow\")) |&gt;\n  addPolylines(\n    lng = ~longitude,\n    lat = ~latitude,\n    color = col2hex(\"green\")\n  )\n\n\n\n\n\nIn general:\n\naddProviderTiles() changes the base map.\nTo explore all available provider base maps, type providers in the console. (Though some don’t work :/)\n\nUse addMarkers() or addCircles() to mark locations. Type ?addControl into the console to pull up a help file which summarizes the aesthetics of these markers and how you can change them. For example:\n\n\nweight = how thick to make the lines, points, pixels\n\nopacity = transparency (like alpha in ggplot2)\ncolors need to be in “hex” form. We used the col2hex() function from the gplots library to do that\n\n\nExercise 3: Your turn\nThe starbucks data, compiled by Danny Kaplan, contains information about every Starbucks in the world at the time the data were collected, including Latitude and Longitude:\n\n# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\nLet’s focus on only those in Minnesota for now:\n\n# Don't worry about the syntax\nstarbucks_mn &lt;- starbucks |&gt;   \n  filter(Country == \"US\", State.Province == \"MN\")\n\nCreate a leaflet map of the Starbucks locations in Minnesota. Keep it simple – go back to Exercise 1 for an example.\n\nleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"darkgreen\"))\n\nAssuming \"Longitude\" and \"Latitude\" are longitude and latitude, respectively\n\n\n\n\n\n\nPart 2: Static points on a map\nLeaflet is very powerful and fun. But:\n\nIt’s not great when we have lots of points to map – it takes lots of time.\nIt makes good interactive maps, but we often need a static map (eg: we can not print interactive maps!).\n\nLet’s explore how to make point maps with ggplot(), not leaflet().\nExercise 3: A simple scatterplot\nLet’s start with the ggplot() tools we already know. Construct a scatterplot of all starbucks locations, not just those in Minnesota, with:\n\nLatitude and Longitude coordinates (which goes on the y-axis?!)\nMake the points transparent (alpha = 0.2) and smaller (size = 0.2)\n\nIt’s pretty cool that the plots we already know can provide some spatial context. But what don’t you like about this plot?\nThe plot doesn’t give me relative context, and the dots are too small for me to really see any overlap.\n\nggplot(starbucks, aes(x = Longitude, y = Latitude)) +\n  geom_point(alpha = 0.2, size = 0.2) \n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nExercise 4: Adding a country-level background\nLet’s add a background map of country-level boundaries.\nPart a\nFirst, we can grab country-level boundaries from the rnaturalearth package.\n\n# Load the package\nlibrary(rnaturalearth)\n\n# Get info about country boundaries across the world\n# in a \"sf\" or simple feature format\nworld_boundaries &lt;- ne_countries(returnclass = \"sf\")\n\nIn your console, type world_boundaries to check out what’s stored there. Don’t print it our in your Rmd – printing it would be really messy there (even just the head()).\nPart b\nRun the chunks below to build up a new map.\n\n# What does this code produce?\n# What geom are we using for the point map?\nggplot(world_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\n# Load package needed to change map theme\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following object is masked from 'package:ggthemes':\n\n    theme_map\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n# Add a point for each Starbucks\n# NOTE: The Starbucks info is in our starbucks data, not world_boundaries\n# How does this change how we use geom_point?!\nggplot(world_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3, size = 0.2, color = \"darkgreen\"\n  ) +\n  theme_map()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart c\nSummarize what you learned about Starbucks from this map.\nThere is a Starbucks in every continent except for Antarctica, with an obvious concentration in the United States and Western Europe.\nExercise 5: Zooming in on some countries\nInstead of world_boundaries &lt;- ne_countries(returnclass = 'sf') we could zoom in on…\n\nthe continent of Africa: ne_countries(continent = 'Africa', returnclass = 'sf')\n\na set of countries: ne_countries(country = c('france', 'united kingdom', 'germany'), returnclass = 'sf')\n\nboundaries within a country: ne_states(country = 'united states of america', returnclass = 'sf')\n\n\nOur goal here will be to map the Starbucks locations in Canada, Mexico, and the US.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only Canada, Mexico, and the US, labeled as “CA”, “MX”, “US” in the starbucks data.\n\n\n# We'll learn this syntax soon! Don't worry about it now.\nstarbucks_cma &lt;- starbucks |&gt; \n  filter(Country %in% c('CA', 'MX', 'US'))\n\n\nA background map of state- and national-level boundaries in Canada, Mexico, and the US. This requires ne_states() in the rnaturalearth package where the countries are labeled ‘canada’, ‘mexico’, ‘united states of america’.\n\n\ncma_boundaries &lt;- ne_states(\n  country = c(\"canada\", \"mexico\", \"united states of america\"),\n  returnclass = \"sf\")\n\nPart b\nMake the map!\n\n# Just the boundaries\nggplot(cma_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\n# Add the points\n# And zoom in\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50)) +\n  theme_map()\n\n\n\n\n\n\n\nExercise 6: A state and county-level map\nLet’s get an even higher resolution map of Starbucks locations within the states of Minnesota, Wisconsin, North Dakota, and South Dakota, with a background map at the county-level.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only the states of interest.\n\n\nstarbucks_midwest &lt;- starbucks |&gt; \n  filter(State.Province %in% c(\"MN\", \"ND\", \"SD\", \"WI\"))\n\n\nA background map of state- and county-level boundaries in these states. This requires st_as_sf() in the sf package, and map() in the maps package, where the countries are labeled ‘minnesota’, ‘north dakota’, etc.\n\n\n# Load packages\nlibrary(sf)\nlibrary(maps)\n\n# Get the boundaries\nmidwest_boundaries &lt;- st_as_sf(\n  maps::map(\"county\",\n            region = c(\"minnesota\", \"wisconsin\", \"north dakota\", \"south dakota\"), \n            fill = TRUE, plot = FALSE))\n\n# Check it out\nhead(midwest_boundaries)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.81268 ymin: 45.05167 xmax: -93.01397 ymax: 48.53526\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\n                                     ID                           geom\nminnesota,aitkin       minnesota,aitkin MULTIPOLYGON (((-93.03689 4...\nminnesota,anoka         minnesota,anoka MULTIPOLYGON (((-93.51817 4...\nminnesota,becker       minnesota,becker MULTIPOLYGON (((-95.14537 4...\nminnesota,beltrami   minnesota,beltrami MULTIPOLYGON (((-95.58655 4...\nminnesota,benton       minnesota,benton MULTIPOLYGON (((-93.77027 4...\nminnesota,big stone minnesota,big stone MULTIPOLYGON (((-96.10794 4...\n\n\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(midwest_boundaries) + \n   geom_sf() + \n   geom_point(\n     data = starbucks_midwest,\n     aes(x = Longitude, y = Latitude),\n     alpha = 0.7,\n     size = 0.2, \n     color = 'darkgreen'\n ) + \n   theme_map()\n\n\n\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nNow check out the contour map.\n\n# What changed in the plot? Instead of points, there are now radiating rings. \n# What changed in our code?! Instead of geom_point we are using geom_density_2d\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_density_2d(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nPart 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first. We’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties_Republican &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\nExercise 8: State-level choropleth maps\nLet’s map the 2020 Republican support in each state, repub_pct_20.\nPart a\nWe again need two pieces of information.\n\nData on elections in each state, which we already have: elections_by_state.\nA background map of state boundaries in the US. The boundaries we used for point maps don’t work here. (Optional detail: they’re sf objects and we now need a data.frame object.) Instead, we can use the map_data() function from the ggplot2 package:\n\n\n# Get the latitude and longitude coordinates of state boundaries\nstates_map &lt;- map_data(\"state\")\n\n# Check it out\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\n14.4.1.1 Pause\nImportant detail: Note that the region variable in states_map, and the state_name variable in elections_by_state both label states by the full name in lower case letters. This is critical to the background map and our data being able to communicate.\n\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nhead(elections_by_state) \n\n   state_name state_abbr repub_pct_20 repub_20_categories\n1     alabama         AL        62.03               60-64\n2    arkansas         AR        62.40               60-64\n3     arizona         AZ        49.06               45-49\n4  california         CA        34.33               30-34\n5    colorado         CO        41.90               40-44\n6 connecticut         CT        39.21               35-39\n\n\n\n14.4.1.2 Part b\nNow map repub_pct_20 by state.\n\n# Note where the dataset, elections_by_state, is used. It is being used in the first unit of the ggplot().\n# Note where the background map, states_map, is used. Background map is being used in geom_map.\n\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() \n\n\n\n\n\n\n\n\n# Make it nicer! We are adding a gradient here. \nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_gradientn(name = \"% Republican\", colors = c(\"blue\", \"purple\", \"red\"), values = scales::rescale(seq(0, 100, by = 5)))\n\n\n\n\n\n\n\nIt’s not easy to get fine control over the color scale for the quantitative repub_pct_20 variable. Instead, let’s plot the discretized version, repub_20_categories:\n\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map()\n\n\n\n\n\n\n\n\n# Load package needed for refining color palette\nlibrary(RColorBrewer)\n\n# Now fix the colors\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\n\n14.4.1.3 Part c\nWe can add other layers, like points, on top of a choropleth map. Add a Starbucks layer! Do you notice any relationship between Starbucks and elections? Or are we just doing things at this point? ;)\n\n# Get only the starbucks data from the US\nstarbucks_us &lt;- starbucks |&gt; \n  filter(Country == \"US\")\n\n# Map it\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  geom_point(\n    data = starbucks_us,\n    aes(x = Longitude, y = Latitude),\n    size = 0.05,\n    alpha = 0.2,\n    inherit.aes = FALSE\n  ) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\nDetails (if you’re curious)\n\n\nmap_id is a required aesthetic for geom_map().\n\nIt specifies which variable in our dataset indicates the region (here state_name).\nIt connects this variable (state_name) to the region variable in our mapping background (states_map). These variables must have the same possible outcomes in order to be matched up (alabama, alaska, arizona,…).\n\n\n\nexpand_limits() assures that the map covers the entire area it’s supposed to, by pulling longitudes and latitudes from the states_map.\nPart d\nWe used geom_sf() for point maps. What geom do we use for choropleth maps?\nExercise 9: County-level choropleth maps\nLet’s map the 2020 Republican support in each county.\nPart a\nWe again need two pieces of information.\n\nData on elections in each county, which we already have: elections_by_county.\nA background map of county boundaries in the US, stored in the county_map dataset in the socviz package:\n\n\n# Get the latitude and longitude coordinates of county boundaries\nlibrary(socviz)\ndata(county_map) \n\n# Check it out\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\n\n\n14.4.1.4 Pause\nImportant detail: We officially have a headache. Our county_map refers to each county by a 5-number id. Our elections_by_counties data refers to each county by a county_fips code, which is mostly the same as id, BUT drops any 0’s at the beginning of the code.\n\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\nhead(elections_by_counties)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\nThis just means that we have to wrangle the data so that it can communicate with the background map.\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\n\n14.4.1.5 Part b\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\nNow map Republican support by county. Let’s go straight to the discretized repub_20_categories variable, and a good color scale.\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = repub_20_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\n\n\n\n\n14.4.2 Exercise 10: Play around!\nConstruct county-level maps of median_rent and median_age.\n\nggplot(elections_by_counties, aes(median_age))+\ngeom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n# Map it\n# Note where the dataset, elections_by_state, is used. It is being used in the first unit of the ggplot().\n# Note where the background map, states_map, is used. Background map is being used in geom_map.\n\nelections_by_counties_median_age &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(median_age_categories = \n           cut(median_age, \n               breaks = seq(20, 80, by = 10),\n               include.lowest = TRUE))\n\nggplot(elections_by_counties_median_age, aes(map_id = county_fips, fill = median_age_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"Median Age\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\n\n\n\nExercise 11: Choropleth maps with leaflet\nThough ggplot() is often better for this purpose, we can also make choropleth maps with leaflet(). If you’re curious, check out the leaflet documentation:\nhttps://rstudio.github.io/leaflet/choropleths.html",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html#solutions",
    "href": "ica/ica-spatial.html#solutions",
    "title": "\n14  Spatial Viz\n",
    "section": "\n14.5 Solutions",
    "text": "14.5 Solutions\n\nClick for Solutions\nExample 1\nBoth addresses used between 0 and 450 therms per month. There seem to be two types of months – those with lower use around 50 therms and those with higher use around 300/400 therms.\n\nggplot(energy, aes(x = therms, fill = address)) + \n  geom_density(alpha = 0.5)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nExample 2\nEnergy use is seasonal, with higher usage in winter months. It seems that address a uses slightly more energy.\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_point()\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_line()\n\n\n\n\n\n\n\nExample 3\nAt both addresses, typical energy use increased after renovations.\n\nggplot(energy, aes(y = therms, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n# A density plot isn't very helpful for comparing typical therms in this example!\nggplot(energy, aes(x = therms, fill = renovated)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ address)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nExample 4\nlurking variable = outdoor temperature (as reflected by hdd)\n\n# It happened to be colder outside after renovations (higher hdd)\nggplot(energy, aes(y = hdd, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\n\n\n\n\n\n# When controlling for outside temps (via hdd), energy use decreased post-renovation\nggplot(energy, aes(y = therms, x = hdd, color = renovated)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ address)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nExample 5\nBUT this was explained by a confounding or omitted or lurking variable: hdd (outdoor temperature)\n\nAfter renovation…\n\nit happened to be colder…\nwhich then leads to higher energy use.\n\nThus, when controlling for outdoor temps, renovations led to decreased energy use.\nExercise 3: Your turn\n\nleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\nAssuming \"Longitude\" and \"Latitude\" are longitude and latitude, respectively\n\n\n\n\n\n\nExercise 3: A simple scatterplot\nIt would be nice to also have some actual reference maps of countries in the background.\n\nggplot(starbucks, aes(y = Latitude, x = Longitude)) + \n  geom_point(size = 0.5)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nExercise 6: A state and county-level map\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(midwest_boundaries) +\n  geom_sf() +\n  geom_point(\n    data = starbucks_midwest,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.7,\n    size = 0.2,\n    color = 'darkgreen'\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nNow check out the contour map.\n\n# What changed in the plot?\n# What changed in our code?!\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_density_2d(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\n\n\n\nExercises Part 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first.\nWe’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\nExercise 8: State-level choropleth maps\nPart d\ngeom_map()\nExercise 10: Play around!\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_rent)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median rent\", colors = c(\"white\", \"lightgreen\", \"darkgreen\"))\n\n\n\n\n\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_age)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median age\", colors = terrain.colors(10))",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html",
    "href": "ica/ica-wrangling.html",
    "title": "\n15  Wrangling\n",
    "section": "",
    "text": "15.1 Exercises",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html#exercises",
    "href": "ica/ica-wrangling.html#exercises",
    "title": "\n15  Wrangling\n",
    "section": "",
    "text": "Exercise 1: select Practice\nUse select() to create a simplified dataset that we’ll use throughout the exercises below.\n\nStore this dataset as elections_small.\nOnly keep the following variables: state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16\n\n\n\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n\nelections_small &lt;- elections |&gt;\n  select(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16)\n\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections_small)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n\nExercise 2: filter Demo\nWhereas select() selects certain variables or columns, filter() keeps certain units of observation or rows relative to their outcome on certain variables. To this end, we must:\n\nIdentify the variable(s) that are relevant to the filter.\n\nUse a “logical comparison operator” to define which values of the variable to keep:\n\n\nsymbol\nmeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(???, ???)\na list of multiple values\n\n\n\n\nUse quotes \"\" when specifying outcomes of interest for a categorical variable.\n\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n# Keep only data on counties in Hawaii\n elections_small |&gt;\n  filter(state_name %in% c(\"Hawaii\"))\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\n# What does this do?\n elections_small |&gt; \n   filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\nData for Hawaii and Delaware.\n\n# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\n# THINK: What variable is relevant here?\nelections_small |&gt;\n  filter(repub_pct_20 &gt; 93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\n# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row (observation) than your answer above\nelections_small |&gt;\n  filter(repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\nWe can also filter with respect to 2 rules! Here, think what variables are relevant.\n\n# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\nelections_small |&gt; \n  filter(state_name %in% c(\"Texas\")) |&gt; \n  filter(dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Method 2: 1 filter with 2 conditions\nelections_small |&gt; \n  filter(state_name %in% c(\"Texas\"),dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n\nExercise 3: arrange Demo\narrange() arranges or sorts the rows in a dataset according to a given column or variable, in ascending or descending order:\narrange(variable), arrange(desc(variable))\n\n# Arrange the counties in elections_small from lowest to highest percentage of 2020 Republican support\n# Print out just the first 6 rows\n elections_small |&gt; \n   arrange(repub_pct_20) |&gt;\n  head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n\n\n# Arrange the counties in elections_small from highest to lowest percentage of 2020 Republican support\n# Print out just the first 6 rows\n elections_small |&gt; \n   arrange(desc(repub_pct_20))|&gt;\n  head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n\nExercise 4: mutate Demo\nmutate() can either transform / mutate an existing variable (column), or define a new variable based on existing ones.\nPart a\n\n# What did this code do?\n#It created a new column called diff_20 that was the repub_pct_20 (-) the dem_pct_20. It shows the difference between the columns.\n elections_small |&gt; \n   mutate(diff_20 = repub_pct_20 - dem_pct_20) |&gt; \n   head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n\n\n# What did this code do?\n# It created a nuew column called repub_votes_20 that rounded the number of total votes in 2020 multiplied by the republican percent.\n elections_small |&gt; \n   mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20/100)) |&gt; \n   head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n\n\n# What did this code do?\n# Created a new column called repub_win_20 that showed when teh republican percent was greater than democrat percent meaning that rebuplicans won. \n elections_small |&gt; \n   mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt; \n   head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\nPart b\n\n# You try\n# Define a variable that calculates the change in Dem support in 2020 vs 2016\nelections_small |&gt; \n  mutate(change_dem_pct = dem_pct_20 - dem_pct_16) |&gt; \n head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 change_dem_pct\n1          24661      23.96           3.06\n2          94090      19.57           2.84\n3          10390      46.66          -0.87\n4           8748      21.42          -0.72\n5          25384       8.47           1.10\n6           4701      75.09          -0.39\n\n\n\n# You try\n# Define a variable that determines whether the Dem support was higher in 2020 than in 2016 (TRUE/FALSE)\nelections_small |&gt; \n  mutate(dem_pct_higher = dem_pct_20 &gt; dem_pct_16) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_pct_higher\n1          24661      23.96           TRUE\n2          94090      19.57           TRUE\n3          10390      46.66          FALSE\n4           8748      21.42          FALSE\n5          25384       8.47           TRUE\n6           4701      75.09          FALSE\n\n\nExercise 5: Pipe Series\nLet’s now combine these verbs into a pipe series!\nPart a\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce?\nI think it will show the results from Wisconsin in 2020 in what county the democratic votes are greater than the republican. The order will be in counties with the greatest number of total votes to the least.\n\n\n\n elections_small |&gt; \n  filter(state_name == \"Wisconsin\",\n          repub_pct_20 &lt; dem_pct_20) |&gt; \n   arrange(desc(total_votes_20)) |&gt; \n   head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart b\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of filter and arrange:\n\nthe results will be the same\n\n\n\n\n# Now try it. Change the order of filter and arrange below.\n  elections_small |&gt; \n  filter(repub_pct_20 &lt; dem_pct_20,state_name == \"Wisconsin\") |&gt; \n   arrange(desc(total_votes_20)) |&gt; \n   head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart c\nSo the order of filter() and arrange() did not matter – rerranging them produces the same results. BUT what is one advantage of filtering before arranging?’ When reading back the code we can see which variables were being looked at before they we arranged.\nPart d\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce?\nI think it was show the results from Delaware if the republican candidate won in 2020 along with the overall republican and democratic values.\n\n\n\n elections_small |&gt; \n   filter(state_name == \"Delaware\") |&gt; \n   mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt; \n   select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\nPart e\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of mutate and select:\n\nwe’ll get an error\n\n\n\n\n# Now try it. Change the order of mutate and select below.\n #elections_small |&gt; \n# filter(state_name == \"Delaware\") |&gt; \n #select(county_name, repub_pct_20, dem_pct_20, repub_win_20) |&gt;\n #mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20)\n\nExercise 6: DIY Pipe Series\nWe’ve now learned 4 of the 6 wrangling verbs: select, filter, mutate, arrange. Let’s practice combining these into pipe series. Here are some hot tips:\n\nBefore writing any code, translate the prompt: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time – don’t try writing a whole chunk at once.\n\nPart a\nShow just the counties in Minnesota and their Democratic 2020 vote percentage, from highest to lowest. Your answer should have just 2 columns.\n\nelections_small |&gt;\n  filter(state_name==\"Minnesota\") |&gt;\n  select(county_name, dem_pct_20) |&gt;\n  arrange(desc(dem_pct_20))\n\n                county_name dem_pct_20\n1             Ramsey County      71.50\n2           Hennepin County      70.46\n3               Cook County      65.58\n4          St. Louis County      56.64\n5             Dakota County      55.73\n6            Olmsted County      54.16\n7         Washington County      53.46\n8         Blue Earth County      50.84\n9               Clay County      50.74\n10              Lake County      50.64\n11          Nicollet County      50.31\n12           Carlton County      49.58\n13            Winona County      49.07\n14              Rice County      48.76\n15          Mahnomen County      48.26\n16             Anoka County      47.79\n17          Beltrami County      47.24\n18            Carver County      46.37\n19             Mower County      46.00\n20             Scott County      45.52\n21           Houston County      42.42\n22           Goodhue County      41.23\n23          Freeborn County      40.96\n24            Norman County      40.80\n25            Itasca County      40.61\n26       Koochiching County      38.41\n27          Watonwan County      38.20\n28           Kittson County      38.12\n29           Stevens County      37.80\n30           Stearns County      37.58\n31          Fillmore County      37.48\n32            Steele County      37.47\n33         Kandiyohi County      36.12\n34            Aitkin County      35.98\n35              Lyon County      35.94\n36     Lac qui Parle County      35.79\n37           Wabasha County      35.78\n38             Grant County      35.58\n39          Traverse County      35.46\n40         Big Stone County      35.41\n41        Pennington County      35.29\n42              Pope County      35.27\n43              Polk County      34.88\n44              Cass County      34.68\n45            Wright County      34.49\n46           Hubbard County      34.42\n47             Swift County      34.35\n48         Crow Wing County      34.17\n49           Chisago County      34.15\n50            Becker County      33.96\n51              Pine County      33.87\n52          Le Sueur County      33.73\n53          Chippewa County      33.67\n54            Nobles County      33.65\n55            Waseca County      33.65\n56             Dodge County      33.47\n57        Otter Tail County      32.85\n58            Benton County      32.70\n59           Douglas County      32.56\n60             Brown County      32.48\n61         Sherburne County      32.48\n62         Faribault County      31.98\n63          Red Lake County      31.47\n64          Renville County      30.71\n65            McLeod County      30.64\n66   Yellow Medicine County      30.54\n67           Lincoln County      30.08\n68        Cottonwood County      30.03\n69           Kanabec County      30.02\n70            Martin County      30.02\n71           Jackson County      29.99\n72        Mille Lacs County      29.98\n73            Wilkin County      29.91\n74              Rock County      29.69\n75            Murray County      29.60\n76            Isanti County      29.45\n77            Sibley County      28.60\n78            Meeker County      28.58\n79           Redwood County      28.43\n80 Lake of the Woods County      27.87\n81        Clearwater County      26.76\n82         Pipestone County      26.44\n83            Wadena County      26.35\n84            Roseau County      25.98\n85          Marshall County      25.33\n86              Todd County      24.79\n87          Morrison County      22.33\n\n\nPart b\nCreate a new dataset named mn_wi that sorts the counties in Minnesota and Wisconsin from lowest to highest in terms of the change in Democratic vote percentage in 2020 vs 2016. This dataset should include the following variables (and only these variables): state_name, county_name, dem_pct_20, dem_pct_20, and a variable measuring the change in Democratic vote percentage in 2020 vs 2016.\n\n# Define the dataset\n# Only store the results once you're confident that they're correct\n mn_wi &lt;- elections |&gt;\n  filter(state_name %in% c(\"Wisconsin\", \"Minnesota\")) |&gt;\n  mutate(diff_dem = dem_pct_20 - dem_pct_16) |&gt;\n  select(state_name, county_name, diff_dem, dem_pct_20, dem_pct_16) |&gt;\n  arrange(diff_dem)\n\n# Check out the first 6 rows to confirm your results\n\nhead(mn_wi)\n\n  state_name        county_name diff_dem dem_pct_20 dem_pct_16\n1  Minnesota     Stevens County    -1.75      37.80      39.55\n2  Wisconsin      Forest County    -1.06      34.06      35.12\n3  Wisconsin    Kewaunee County    -0.86      32.87      33.73\n4  Wisconsin       Clark County    -0.82      30.37      31.19\n5  Wisconsin       Adams County    -0.77      36.63      37.40\n6  Wisconsin Trempealeau County    -0.71      40.86      41.57\n\n\nPart c\nConstruct and discuss a plot of the county-level change in Democratic vote percent in 2020 vs 2016, and how this differs between Minnesota and Wisconsin.\n\nmn_wi &lt;- elections |&gt;\n  filter(state_name %in% c(\"Wisconsin\", \"Minnesota\")) |&gt;\n  mutate(diff_dem = dem_pct_20 - dem_pct_16)\n\nExercise 7: summarize Demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nLet’s talk about the last 2 verbs. summarize() (or equivalently summarise()) takes an entire data frame as input and outputs a single row with one or more summary statistics. For each chunk below, indicate what the code does.\n\n# What does this do?\n# Shows the summary of the repub_pct_2 column.\n elections_small |&gt; \n   summarize(median(repub_pct_20))\n\n  median(repub_pct_20)\n1                68.29\n\n\n\n# What does this do?\n# Summarizes the median of the values in repub_pct_20.\n elections_small |&gt; \n   summarize(median_repub = median(repub_pct_20))\n\n  median_repub\n1        68.29\n\n\n\n# What does this do?\n elections_small |&gt; \n   summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n  median_repub total_votes\n1        68.29   157949293\n\n\nExercise 8: summarize + group_by demo\nFinally, group_by() groups the units of observation or rows of a data frame by a specified set of variables. Alone, this function doesn’t change the appearance of our dataset or seem to do anything at all:\n\n elections_small |&gt; \n   group_by(state_name)\n\n# A tibble: 3,109 × 7\n# Groups:   state_name [50]\n   state_name county_name  total_votes_20 repub_pct_20 dem_pct_20 total_votes_16\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;\n 1 Alabama    Autauga Cou…          27770         71.4      27.0           24661\n 2 Alabama    Baldwin Cou…         109679         76.2      22.4           94090\n 3 Alabama    Barbour Cou…          10518         53.4      45.8           10390\n 4 Alabama    Bibb County            9595         78.4      20.7            8748\n 5 Alabama    Blount Coun…          27588         89.6       9.57          25384\n 6 Alabama    Bullock Cou…           4613         24.8      74.7            4701\n 7 Alabama    Butler Coun…           9488         57.5      41.8            8685\n 8 Alabama    Calhoun Cou…          50983         68.8      29.8           47376\n 9 Alabama    Chambers Co…          15284         57.3      41.6           13778\n10 Alabama    Cherokee Co…          12301         86.0      13.2           10503\n# ℹ 3,099 more rows\n# ℹ 1 more variable: dem_pct_16 &lt;dbl&gt;\n\n\nThough it does change the underlying structure of the dataset:\n\n# Check out the structure before and after group_by\n elections_small |&gt; \n   class()\n\n[1] \"data.frame\"\n\n elections_small |&gt; \n   group_by(state_name) |&gt; \n   class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWhere it really shines is in partnership with summarize().\n\n# What does this do?\n# (What if we didn't use group_by?)\n# It summarizes the median of republican percent in 2020 but shows all the values per state. Without the group_by it groups everything together and there is not a separation of states. \n elections_small |&gt; \n    group_by(state_name) |&gt;\n   summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20)) \n\n# A tibble: 50 × 3\n   state_name           median_repub total_votes\n   &lt;chr&gt;                       &lt;dbl&gt;       &lt;int&gt;\n 1 Alabama                      70.6     2323304\n 2 Arizona                      57.9     3387326\n 3 Arkansas                     72.1     1219069\n 4 California                   44.8    17495906\n 5 Colorado                     56.2     3256953\n 6 Connecticut                  41.0     1824280\n 7 Delaware                     47.1      504010\n 8 District of Columbia          5.4      344356\n 9 Florida                      64.6    11067456\n10 Georgia                      68       4997716\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\nReflect\n\n\n\nNotice that group_by() with summarize() produces new data frame or tibble! But the units of observation are now states instead of counties within states.\n\n\nExercise 9: DIY\nLet’s practice (some of) our 6 verbs: select, filter, arrange, mutate, summarize, group_by Remember:\n\nBefore writing any code, translate the given prompts: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time.\n\nPart a\nNOTE: Part a is a challenge exercise. If you get really stuck, move on to Part b which is the same overall question, but with hints.\n\n# Sort the *states* from the most to least total votes cast in 2020\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarise(total_votes=sum(total_votes_20)) |&gt;\n  arrange(desc(total_votes))\n\n# A tibble: 50 × 2\n   state_name     total_votes\n   &lt;chr&gt;                &lt;int&gt;\n 1 California        17495906\n 2 Texas             11317911\n 3 Florida           11067456\n 4 New York           8616205\n 5 Pennsylvania       6925255\n 6 Illinois           6038850\n 7 Ohio               5922202\n 8 Michigan           5539302\n 9 North Carolina     5524801\n10 Georgia            4997716\n# ℹ 40 more rows\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each *state*?\nelections_small |&gt;\n  mutate(total_votes_rep= (repub_pct_20/100)*total_votes_20) |&gt;\n  mutate(total_votes_dem= (dem_pct_20/100)*total_votes_20) |&gt;\n  group_by(state_name) |&gt;\n  summarise(total_votes_dem=sum(total_votes_dem, na.rm = TRUE),   total_votes_rep=sum(total_votes_rep, na.rm = TRUE))\n\n# A tibble: 50 × 3\n   state_name           total_votes_dem total_votes_rep\n   &lt;chr&gt;                          &lt;dbl&gt;           &lt;dbl&gt;\n 1 Alabama                      849665.        1441153.\n 2 Arizona                     1672126.        1661672.\n 3 Arkansas                     423919.         760639.\n 4 California                 11109643.        6006034.\n 5 Colorado                    1804395.        1364625.\n 6 Connecticut                 1080677.         715315.\n 7 Delaware                     296274.         200601.\n 8 District of Columbia         317324.          18595.\n 9 Florida                     5297129.        5668599.\n10 Georgia                     2473656.        2461871.\n# ℹ 40 more rows\n\n\n\n# What states did the Democratic candidate win in 2020?\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarise(dem_win_20 = sum(dem_pct_20&gt;repub_pct_20)&gt;0) |&gt;\n  filter(dem_win_20) |&gt;\n  select(state_name)\n\n# A tibble: 48 × 1\n   state_name          \n   &lt;chr&gt;               \n 1 Alabama             \n 2 Arizona             \n 3 Arkansas            \n 4 California          \n 5 Colorado            \n 6 Connecticut         \n 7 Delaware            \n 8 District of Columbia\n 9 Florida             \n10 Georgia             \n# ℹ 38 more rows\n\n\nPart b\n\n# Sort the states from the most to least total votes cast in 2020\n# HINT: Calculate the total number of votes in each state, then sort\n\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarise(total_votes = sum(total_votes_20, na.rm = TRUE)) |&gt;\n  arrange(desc(total_votes))\n\n# A tibble: 50 × 2\n   state_name     total_votes\n   &lt;chr&gt;                &lt;int&gt;\n 1 California        17495906\n 2 Texas             11317911\n 3 Florida           11067456\n 4 New York           8616205\n 5 Pennsylvania       6925255\n 6 Illinois           6038850\n 7 Ohio               5922202\n 8 Michigan           5539302\n 9 North Carolina     5524801\n10 Georgia            4997716\n# ℹ 40 more rows\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each state?\n# HINT: First calculate the number of Dem and Repub votes in each *county*\n# Then group and summarize these by state\n\nelections_small |&gt;\n  mutate(\n    total_votes_rep = (repub_pct_20 / 100) * total_votes_20,\n    total_votes_dem = (dem_pct_20 / 100) * total_votes_20\n  ) |&gt;\n  group_by(state_name) |&gt;\n  summarise(\n    total_votes_dem = sum(total_votes_dem, na.rm = TRUE),\n    total_votes_rep = sum(total_votes_rep, na.rm = TRUE))\n\n# A tibble: 50 × 3\n   state_name           total_votes_dem total_votes_rep\n   &lt;chr&gt;                          &lt;dbl&gt;           &lt;dbl&gt;\n 1 Alabama                      849665.        1441153.\n 2 Arizona                     1672126.        1661672.\n 3 Arkansas                     423919.         760639.\n 4 California                 11109643.        6006034.\n 5 Colorado                    1804395.        1364625.\n 6 Connecticut                 1080677.         715315.\n 7 Delaware                     296274.         200601.\n 8 District of Columbia         317324.          18595.\n 9 Florida                     5297129.        5668599.\n10 Georgia                     2473656.        2461871.\n# ℹ 40 more rows\n\n\n\n# What states did the Democratic candidate win in 2020?\n# HINT: Start with the results from the previous chunk, and then keep only some rows\nelections_small |&gt;\n  mutate(\n    total_votes_rep = (repub_pct_20 / 100) * total_votes_20,\n    total_votes_dem = (dem_pct_20 / 100) * total_votes_20\n  ) |&gt;\n  group_by(state_name) |&gt;\n  summarise(\n    total_votes_dem = sum(total_votes_dem, na.rm = TRUE),\n    total_votes_rep = sum(total_votes_rep, na.rm = TRUE)\n  ) |&gt;\n  filter(total_votes_dem &gt; total_votes_rep) |&gt;\n  select(state_name)\n\n# A tibble: 26 × 1\n   state_name          \n   &lt;chr&gt;               \n 1 Arizona             \n 2 California          \n 3 Colorado            \n 4 Connecticut         \n 5 Delaware            \n 6 District of Columbia\n 7 Georgia             \n 8 Hawaii              \n 9 Illinois            \n10 Maine               \n# ℹ 16 more rows\n\n\nExercise 10: Practice on New Data\nRecall the World Cup football/soccer data from TidyTuesday:\n\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\nYou can find a codebook here. Use (some of) our 6 verbs (select, filter, arrange, mutate, summarize, group_by) and data viz to address the following prompts.\n\n# In what years did Brazil win the World Cup?\nworld_cup |&gt;\n  filter(winner == \"Brazil\") |&gt;\n  select(year)\n\n  year\n1 1958\n2 1962\n3 1970\n4 1994\n5 2002\n\n\n\n# What were the 6 World Cups with the highest attendance?\nworld_cup |&gt;\n  select(year, attendance) |&gt;\n  arrange(desc(attendance)) |&gt;\n  head(6)\n\n  year attendance\n1 1994    3568567\n2 2014    3441450\n3 2006    3367000\n4 2018    3031768\n5 1998    2859234\n6 2002    2724604\n\n\n\n# Construct a univariate plot of goals_scored (no wrangling necessary)\n# This provides a visual summary of how the number of goals_scored varies from World Cup to World Cup\n\nggplot(world_cup, aes(x=goals_scored))+\n  geom_bar(color=\"black\")+\n  labs(x=\"Goals Scored\")\n\n\n\n\n\n\n\n\n# Let's follow up the plot with some more precise numerical summaries\n# Calculate the min, median, and max number of goals_scored across all World Cups\n# NOTE: Visually compare these numerical summaries to what you observed in the plot\n\n\nworld_cup |&gt;\n  summarise(min_goals=min(goals_scored), medial_goals=median(goals_scored),max_goals=max(goals_scored))\n\n  min_goals medial_goals max_goals\n1        70          126       171\n\n\n\n# Construct a bivariate plot of how the number of goals_scored in the World Cup has changed over the years\n# No wrangling necessary\nggplot(world_cup, aes(x=year,y=goals_scored))+\n  geom_point(color=\"black\", fill=\"white\")+\n  labs(x=\"Year of World Cup\", y=\"Number of Goals Scored\")\n\n\n\n\n\n\n\n\n# Our above summaries might be a bit misleading.\n# The number of games played at the World Cup varies.\n# Construct a bivariate plot of how the typical number of goals per game has changed over the years\nworld_cup1 &lt;-world_cup |&gt;\n  mutate(ave_goals=goals_scored/games)\n\n\nggplot(world_cup1, aes(x=year, y=ave_goals))+\n  geom_point(color=\"black\", fill=\"white\")+\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n  labs(x=\"Year of World Cup\", y=\"Typical Number of Goals Scored (per game)\")\n\n$x\n[1] \"Year of World Cup\"\n\n$y\n[1] \"Typical Number of Goals Scored (per game)\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nExercise 11: Practice on Your Data\nReturn to the TidyTuesday data you’re using in Homework 3. Use your new wrangling skills to play around. What new insights can you gain?!",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html",
    "href": "ica/ica-dates.html",
    "title": "\n16  Dates\n",
    "section": "",
    "text": "16.1 Exercises Part 1: Same Verbs, New Tricks",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#exercises-part-1-same-verbs-new-tricks",
    "href": "ica/ica-dates.html#exercises-part-1-same-verbs-new-tricks",
    "title": "\n16  Dates\n",
    "section": "",
    "text": "Exercise 1: More Filtering\nRecall the “logical comparison operators” we can use to filter() our data:\n\n\nsymbol\nmeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(***, ***)\na list of multiple values\n\n\n\nPart a\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n# Create a dataset with just Adelie and Chinstrap using %in%\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt; \n   filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt; \n   count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n# Create a dataset with just Adelie and Chinstrap using !=\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt; \n   filter(species != \"Gentoo\" ) |&gt; \n   count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\nPart b\nNotice that some of our penguins have missing (NA) data on some values:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n\n\n\n\n\nHandeling NA Values\n\n\n\nThere are many ways to handle missing data. The right approach depends upon your research goals. A general rule is: Only get rid of observations with missing data if they’re missing data on variables you need for the specific task at hand!\n\n\nExample 1\nSuppose our research focus is just on body_mass_g. Two penguins are missing this info:\n\n# NOTE the use of is.na()\n#Shoes number that are missing body mass.\npenguins |&gt; \n  summarize(sum(is.na(body_mass_g)))\n\n# A tibble: 1 × 1\n  `sum(is.na(body_mass_g))`\n                      &lt;int&gt;\n1                         2\n\n\nLet’s define a new dataset that removes these penguins:\n\n# NOTE the use of is.na()\npenguins_w_body_mass &lt;- penguins |&gt; \n  filter(!is.na(body_mass_g))\n\n# Compare the number of penguins in this vs the original data\nnrow(penguins_w_body_mass)\n\n[1] 342\n\nnrow(penguins)\n\n[1] 344\n\n\nNote that some penguins in penguins_w_body_mass are missing info on sex, but we don’t care since that’s not related to our research question:\n\npenguins_w_body_mass |&gt; \n  summarize(sum(is.na(sex)))\n\n# A tibble: 1 × 1\n  `sum(is.na(sex))`\n              &lt;int&gt;\n1                 9\n\n\nExample 2\nIn the very rare case that we need complete information on every variable for the specific task at hand, we can use na.omit() to get rid of any penguin that’s missing info on any variable:\n\npenguins_complete &lt;- penguins |&gt; \n  na.omit()\n\nHow many penguins did this eliminate?\nIt eliminated 11 penguins.\n\nnrow(penguins_complete)\n\n[1] 333\n\nnrow(penguins)\n\n[1] 344\n\n\nPart c\nExplain why we should only use na.omit() in extreme circumstances.\nna.omit deletes all that are missing any data. They may have the data that we are focusing on but since they are missing something else they are omitted in the data set.\nExercise 2: More Selecting\nBeing able to select() only certain columns can help simplify our data. This is especially important when we’re working with lots of columns (which we haven’t done yet). It can also get tedious to type out every column of interest. Here are some shortcuts:\n\n\n- removes a given variable and keeps all others (e.g. select(-island))\n\nstarts_with(\"___\"), ends_with(\"___\"), or contains(\"___\") selects only the columns that either start with, end with, or simply contain the given string of characters\n\nUse these shortcuts to create the following datasets.\n\n# First: recall the variable names\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n# Use a shortcut to keep everything but the year and island variables\npenguins |&gt;\n  select(-year,-island)\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the penguin characteristics measured in mm\npenguins |&gt; \n  select(species, contains(\"mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and bill-related measurements\npenguins |&gt;\n  select(species, contains(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the length-related characteristics\npenguins |&gt;\n  select(species, contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n\n\nExercise 3: Arranging, Counting, & Grouping by Multiple Variables\nWe’ve done examples where we need to filter() by more than one variable, or select() more than one variable. Use your intuition for how we can arrange(), count(), and group_by() more than one variable.\n\n# Change this code to sort the penguins by species, and then island name\n# NOTE: The first row should be an Adelie penguin living on Biscoe island\npenguins |&gt; \n  arrange(species, island)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n# Change this code to count the number of male/female penguins observed for each species\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\n# Change this code to calculate the average body mass by species and sex\npenguins |&gt; \n  group_by(species,sex) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;   3540 \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;   4588.\n\n\n\nExercise 4: Dates\nBefore some wrangling practice, let’s explore another important concept: working with or mutating date variables. Dates are a whole special object type or class in R that automatically respect the order of time.\n\n# Get today's date\nas.Date(today())\n\n[1] \"2025-04-14\"\n\n# Let's store this as \"today\" so we can work with it below\ntoday &lt;- as.Date(today())\n\n# Check out the class of this object\nclass(today)\n\n[1] \"Date\"\n\n\nThe lubridate package inside tidyverse contains functions that can extract various information from dates. Let’s learn about some of the most common functions by applying them to today. For each, make a comment on what the function does\n\nyear(today)\n\n[1] 2025\n\n\n\n# What do these lines produce / what's their difference?\n#shows the quantitative number of the month versus the categorical \"feb\"\nmonth(today)\n\n[1] 4\n\nmonth(today, label = TRUE)\n\n[1] Apr\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\n\n# What does this number mean?\n# It is the 9th week of the year.\nweek(today)\n\n[1] 15\n\n\n\n# What do these lines produce / what's their difference?\n# mday shows the day of the month, and yday shows the number day in the year.\nmday(today)\n\n[1] 14\n\nyday(today)  # This is often called the \"Julian day\"\n\n[1] 104\n\n\n\n# What do these lines produce / what's their difference?\n#Shows the number of the day of the week versus the categorical word of todays day. \nwday(today)\n\n[1] 2\n\nwday(today, label = TRUE)\n\n[1] Mon\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n# What do the results of these 2 lines tell us?\n# If today is a past or equal to a date or if today is before a date.\ntoday &gt;= ymd(\"2024-02-14\")\n\n[1] TRUE\n\ntoday &lt; ymd(\"2024-02-14\")\n\n[1] FALSE",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#exercises-part-2-application",
    "href": "ica/ica-dates.html#exercises-part-2-application",
    "title": "\n16  Dates\n",
    "section": "\n16.2 Exercises Part 2: Application",
    "text": "16.2 Exercises Part 2: Application\nThe remaining exercises are similar to some of those on the homework. Hence, the solutions are not provided. Let’s apply these ideas to the daily Birthdays dataset in the mosaic package.\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\ndata(\"Birthdays\")\nhead(Birthdays)\n\n  state year month day       date wday births\n1    AK 1969     1   1 1969-01-01  Wed     14\n2    AL 1969     1   1 1969-01-01  Wed    174\n3    AR 1969     1   1 1969-01-01  Wed     78\n4    AZ 1969     1   1 1969-01-01  Wed     84\n5    CA 1969     1   1 1969-01-01  Wed    824\n6    CO 1969     1   1 1969-01-01  Wed    100\n\n\nBirthdays gives the number of births recorded on each day of the year in each state from 1969 to 19881. We can use our wrangling skills to understand some drivers of daily births. Putting these all together can be challenging! Remember the following ways to make tasks more manageable:\n\nTranslate the prompt into our 6 verbs (and count()). That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\n\nExercise 5: Warming up\n\n# How many days of data do we have for each state?\nBirthdays |&gt;\n  count(state, date) |&gt;\n  count(state)\n\n   state    n\n1     AK 7305\n2     AL 7305\n3     AR 7305\n4     AZ 7305\n5     CA 7305\n6     CO 7305\n7     CT 7305\n8     DC 7305\n9     DE 7305\n10    FL 7305\n11    GA 7305\n12    HI 7305\n13    IA 7305\n14    ID 7305\n15    IL 7305\n16    IN 7305\n17    KS 7305\n18    KY 7305\n19    LA 7305\n20    MA 7305\n21    MD 7305\n22    ME 7305\n23    MI 7305\n24    MN 7305\n25    MO 7305\n26    MS 7305\n27    MT 7305\n28    NC 7305\n29    ND 7305\n30    NE 7305\n31    NH 7305\n32    NJ 7305\n33    NM 7305\n34    NV 7305\n35    NY 7305\n36    OH 7305\n37    OK 7305\n38    OR 7305\n39    PA 7305\n40    RI 7305\n41    SC 7305\n42    SD 7305\n43    TN 7305\n44    TX 7305\n45    UT 7305\n46    VA 7305\n47    VT 7305\n48    WA 7305\n49    WI 7305\n50    WV 7305\n51    WY 7305\n\n# How many total births were there in this time period?\nBirthdays |&gt;\n  summarise(total_births=sum(births))\n\n  total_births\n1     70486538\n\n# How many total births were there per state in this time period, sorted from low to high?\n\nExercise 6: Homework Reprise\nCreate a new dataset named daily_births that includes the total number of births per day (across all states) and the corresponding day of the week, eg, Mon. NOTE: Name the column with total births so that it’s easier to wrangle and plot.\n\ndaily_births &lt;- Birthdays |&gt;\n  group_by(date) |&gt;\n  summarise(total_births=sum(births)) |&gt;\n  mutate(day_of_week = weekdays(date))\n\nUsing this data, construct a plot of births over time, indicating the day of week.\n\nggplot(daily_births, aes(x=date, y=total_births, color=day_of_week))+\n  geom_point()+\n  geom_line()+\n  labs(title=\"Total Births Over Time\", x=\"Date\", y=\"Total Births\", color=\"Day of week\")\n\n\n\n\n\n\n\nExercise 7: Wrangle & Plot\nFor each prompt below, you can decide whether you want to: (1) wrangle and store data, then plot; or (2) wrangle data and pipe directly into ggplot. For example:\n\npenguins |&gt; \n  filter(species != \"Gentoo\") |&gt; \n  ggplot(aes(y = bill_length_mm, x = bill_depth_mm, color = species)) + \n    geom_point()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart a\nCalculate the total number of births in each month and year, eg, Jan 1969, Feb 1969, …. Label month by names not numbers, eg, Jan not 1. Then, plot the births by month and comment on what you learn.\n\nmonthly_births &lt;- Birthdays |&gt;\n  mutate(year = year(date),\n         month = format(date, \"%b\")) |&gt;  # Extracts month as a three-letter abbreviation (e.g., \"Jan\", \"Feb\")\n  group_by(year, month) |&gt;\n  summarise(total_births = sum(births, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(year, match(month, month.abb))  # Ensures months are in correct order\n\nggplot(monthly_births, aes(x = factor(month, levels = month.abb), y = total_births, group = year, color = as.factor(year))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Total Births by Month and Year\",\n       x = \"Month\",\n       y = \"Total Births\",\n       color = \"Year\") \n\n\n\n\n\n\n\nPart b\nIn 1988, calculate the total number of births per week in each state. Get rid of week “53”, which isn’t a complete week! Then, make a line plot of births by week for each state and comment on what you learn. For example, do you notice any seasonal trends? Are these the same in every state? Any outliers?\n\nweekly_births_1988 &lt;- Birthdays |&gt; \n  filter(year(date) == 1988) |&gt;  # Keep only data from 1988\n  mutate(week = isoweek(date)) |&gt;  # Extract ISO week number\n  filter(week != 53) |&gt;  # Remove incomplete week 53\n  group_by(state, week) |&gt; \n  summarise(total_births = sum(births, na.rm = TRUE), .groups = \"drop\")\n\nggplot(weekly_births_1988, aes(x = week, y = total_births, color = state)) +\n  geom_line() +\n  labs(title = \"Total Births Per Week in 1988 by State\",\n       x = \"Week of the Year\",\n       y = \"Total Births\",\n       color = \"State\") \n\n\n\n\n\n\n\nPart c\nRepeat the above for just Minnesota (MN) and Louisiana (LA). MN has one of the coldest climates and LA has one of the warmest. How do their seasonal trends compare? Do you think these trends are similar in other colder and warmer states? Try it!\n\nweekly_births_mn_la &lt;- Birthdays |&gt; \n  filter(year(date) == 1988, state %in% c(\"MN\", \"LA\")) |&gt;  # Keep 1988 data for MN & LA\n  mutate(week = isoweek(date)) |&gt;  # Extract ISO week number\n  filter(week != 53) |&gt;  # Remove incomplete week 53\n  group_by(state, week) |&gt;  \n  summarise(total_births = sum(births, na.rm = TRUE), .groups = \"drop\")  # Summarize births per state per week\n\nggplot(weekly_births_mn_la, aes(x = week, y = total_births, color = state)) +\n  geom_line(size = 1) +\n labs(title = \"Total Births Per Week in 1988: Minnesota vs. Louisiana\",\n       x = \"Week of the Year\",\n       y = \"Total Births\",\n       color = \"State\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nExercise 8: More Practice\nPart a\nCreate a dataset with only births in Massachusetts (MA) in 1979 and sort the days from those with the most births to those with the fewest.\n\nma_births_1979 &lt;- Birthdays |&gt; \n  filter(year(date) == 1979, state == \"MA\") |&gt;  # Filter for MA in 1979\n  arrange(desc(births))  # Sort from most to fewest births\n\n# View the first few rows\nhead(ma_births_1979)\n\n  state year month day       date wday births\n1    MA 1979     9  28 1979-09-28  Fri    262\n2    MA 1979     9  11 1979-09-11 Tues    252\n3    MA 1979    12  28 1979-12-28  Fri    249\n4    MA 1979     9  26 1979-09-26  Wed    246\n5    MA 1979     7  24 1979-07-24 Tues    245\n6    MA 1979     4  27 1979-04-27  Fri    243\n\n\nPart b\nMake a table showing the five states with the most births between September 9, 1979 and September 12, 1979, including the 9th and 12th. Arrange the table in descending order of births.\n\ntop_5_states &lt;- Birthdays |&gt; \n  filter(date &gt;= as.Date(\"1979-09-09\") & date &lt;= as.Date(\"1979-09-12\")) |&gt;  # Filter date range\n  group_by(state) |&gt;  \n  summarise(total_births = sum(births, na.rm = TRUE), .groups = \"drop\") |&gt;  # Sum births per state\n  arrange(desc(total_births)) |&gt;  # Sort in descending order\n  slice_head(n = 5)  # Select the top 5 states\n\n# View the table\ntop_5_states\n\n# A tibble: 5 × 2\n  state total_births\n  &lt;chr&gt;        &lt;int&gt;\n1 CA            4422\n2 TX            3151\n3 NY            2621\n4 IL            2235\n5 OH            1938",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html#footnotes",
    "href": "ica/ica-dates.html#footnotes",
    "title": "\n16  Dates\n",
    "section": "",
    "text": "The fivethirtyeight package has more recent data.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html",
    "href": "ica/ica-reshaping.html",
    "title": "\n17  Reshaping\n",
    "section": "",
    "text": "17.1 Exercises",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#exercises",
    "href": "ica/ica-reshaping.html#exercises",
    "title": "\n17  Reshaping\n",
    "section": "",
    "text": "Exercise 1: What’s the problem?\nConsider data on a sleep study in which subjects received only 3 hours of sleep per night. Each day, their reaction time to a stimulus (in ms) was recorded.1\n\nsleep_wide &lt;- read.csv(\"https://mac-stat.github.io/data/sleep_wide.csv\")\n\nhead(sleep_wide)\n\n  Subject  day_0  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8  day_9\n1     308 249.56 258.70 250.80 321.44 356.85 414.69 382.20 290.15 430.59 466.35\n2     309 222.73 205.27 202.98 204.71 207.72 215.96 213.63 217.73 224.30 237.31\n3     310 199.05 194.33 234.32 232.84 229.31 220.46 235.42 255.75 261.01 247.52\n4     330 321.54 300.40 283.86 285.13 285.80 297.59 280.24 318.26 305.35 354.05\n5     331 287.61 285.00 301.82 320.12 316.28 293.32 290.08 334.82 293.75 371.58\n6     332 234.86 242.81 272.96 309.77 317.46 310.00 454.16 346.83 330.30 253.86\n\n\nPart a\nWhat are the units of observation in sleep_wide?\n\ndim(sleep_wide)\n\n[1] 18 11\n\nnames(sleep_wide)\n\n [1] \"Subject\" \"day_0\"   \"day_1\"   \"day_2\"   \"day_3\"   \"day_4\"   \"day_5\"  \n [8] \"day_6\"   \"day_7\"   \"day_8\"   \"day_9\"  \n\n\nPart b\nSuppose I ask you to plot each subject’s reaction time (y-axis) vs the number of days of sleep restriction (x-axis). “Sketch” out in words what the first few rows of the data need to look like in order to do this. It might help to think about what you’d need to complete the plotting frame:\nggplot(sleep_wide, aes(y = reaction_time, x = days_sleep_restrict, color = subject))\nPart c\nHow can you obtain the dataset you sketched in part b?\n\njust using sleep_wide\n\npivot_longer()\npivot_wider()\nExercise 2: Pivot longer\nTo plot reaction time by day for each subject, we need to reshape the data into a long format where each row represents a subject/day combination. Specifically, we want a dataset with 3 columns and a first few rows that look something like this:\n\n\nSubject\nday\nreaction_time\n\n\n\n308\n0\n249.56\n\n\n308\n1\n258.70\n\n\n308\n2\n250.80\n\n\n\nPart a\nUse pivot_longer() to create the long-format dataset above. Show the first 3 lines (head(3)), which should be similar to those above. Follow-up: Thinking forward to plotting reaction time vs day for each subject, what would you like to fix / change about this dataset?\n\n# For cols, try 2 appproaches: using - and starts_with\n sleep_wide |&gt;\n  pivot_longer(cols = -Subject, names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\nsleep_wide |&gt;\n  pivot_longer(cols = starts_with(\"day\"), names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\n\nPart b\nRun this chunk:\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\")\n\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject day   reaction_time\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     308 0              250.\n2     308 1              259.\n3     308 2              251.\n4     308 3              321.\n5     308 4              357.\n6     308 5              415.\n\n\nFollow-up:\n\nBesides putting each argument on a different line for readability and storing the results, what changed in the code?\nHow did this impact how the values are recorded in the day column?\nPart c\nUsing sleep_long, construct a line plot of reaction time vs day for each subject. This will look goofy no matter what you do. Why? HINT: look back at head(sleep_long). What class or type of variables are Subject and day? What do we want them to be?\n\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n\n\nExercise 3: Changing variable classes & plotting\nLet’s finalize sleep_long by mutating the Subject variable to be a factor (categorical) and the day variable to be numeric (quantitative). Take note of the mutate() code! You’ll use this type of code a lot.\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt; \n  mutate(Subject = as.factor(Subject), day = as.numeric(day))\n\n# Check it out\n# Same data, different class\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject   day reaction_time\n  &lt;fct&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 308         0          250.\n2 308         1          259.\n3 308         2          251.\n4 308         3          321.\n5 308         4          357.\n6 308         5          415.\n\n\nPart a\nNow make some plots.\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on the same frame\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n\n\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line() + \n  facet_wrap(~ Subject)\n\n\n\n\n\n\n\nPart b\nSummarize what you learned from the plots. For example:\n\nWhat’s the general relationship between reaction time and sleep?\nIs this the same for everybody? What differs?\n\nThe reaction time over the days varies widely between subjects. However, there is a common trend of overall increased reaction time as the days increase.\nExercise 4: Pivot wider\nMake the data wide again, with each day becoming its own column.\nPart a\nAdjust the code below. What don’t you like about the column labels?\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time) |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject   `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nPart b\nUsing your intuition, adjust your code from part a to name the reaction time columns “day_0”, “day_1”, etc.\n\n sleep_long |&gt;\n   pivot_wider(names_from = day, values_from = reaction_time, names_prefix = \"day\") |&gt; \n   head()\n\n# A tibble: 6 × 11\n  Subject  day0  day1  day2  day3  day4  day5  day6  day7  day8  day9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nExercise 5: Practice with Billboard charts\nLoad data on songs that hit the billboard charts around the year 2000. Included for each song is the artist name, track name, the date it hit the charts (date.enter), and wk-related variables that indicate rankings in each subsequent week on the charts:\n\n# Load data\nlibrary(tidyr)\ndata(\"billboard\")\n\n# Check it out\nhead(billboard)\n\n# A tibble: 6 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac       Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n2 2Ge+her     The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n3 3 Doors Do… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n4 3 Doors Do… Loser 2000-10-21      76    76    72    69    67    65    55    59\n5 504 Boyz    Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n6 98^0        Give… 2000-08-19      51    39    34    26    26    19     2     2\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nIn using this data, you’ll need to determine if and when the data needs to be reshaped for the task at hand.\nPart a\nConstruct and summarize a plot of how a song’s Billboard ranking its 2nd week on the chart (y-axis) is related to its ranking the 1st week on the charts (x-axis). Add a reference line geom_abline(intercept = 0, slope = 1). Songs above this line improved their rankings from the 1st to 2nd week.\n\nggplot(billboard, aes(y = wk2, x = wk1)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1)\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nPart b\nUse your wrangling tools to identify which songs are those above the line in Part a, i.e. with rankings that went up from week 1 to week 2.\n\nbillboard |&gt; \n  filter(wk2 &gt; wk1)\n\n# A tibble: 7 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Carey, Mar… Cryb… 2000-06-24      28    34    48    62    77    90    95    NA\n2 Clark, Ter… A Li… 2000-12-16      75    82    88    96    99    99    NA    NA\n3 Diffie, Joe The … 2000-01-01      98   100   100    90    93    94    NA    NA\n4 Hart, Beth  L.A.… 1999-11-27      99   100    98    99    99    99    98    90\n5 Jay-Z       Hey … 2000-08-12      98   100    98    94    83    83    80    78\n6 Lil' Zane   Call… 2000-07-29      83    89    57    40    34    21    33    46\n7 Pearl Jam   Noth… 2000-05-13      49    70    84    89    93    91    NA    NA\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nPart c\nDefine a new dataset, nov_1999, which:\n\nonly includes data on songs that entered the Billboard charts on November 6, 1999\nkeeps all variables except track and date.entered. HINT: How can you avoid writing out all the variable names you want to keep?\n\n\n# Define nov_1999\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(-track, -date.entered)\n\n# Confirm that nov_1999 has 2 rows (songs) and 77 columns\ndim(nov_1999)\n\n[1]  2 77\n\n\nPart d\nCreate and discuss a visualization of the rankings (y-axis) over time (x-axis) for the 2 songs in nov_1999. There are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\n\nnov_1999 |&gt; \n  pivot_longer(cols = -artist, names_to = \"week\", names_prefix = \"wk\", values_to = \"ranking\") |&gt; \n  mutate(week = as.numeric(week)) |&gt; \n  ggplot(aes(y = ranking, x = week, color = artist)) + \n    geom_line()\n\nWarning: Removed 79 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nHints:\n\nShould you first pivot wider or longer?\nOnce you pivot, the week number is turned into a character variable. How can you change it to a number?\nExercise 6: Practice with the Daily Show\nThe data associated with this article is available in the fivethirtyeight package, and is loaded into daily below. It includes a list of every guest to ever appear on Jon Stewart’s The Daily Show, a “late-night talk and satirical news” program (per Wikipedia). Check out the dataset and note that when multiple people appeared together, each person receives their own line:\n\ninstall.packages('fivethirtyeightdata', repos = 'https://fivethirtyeightdata.github.io/drat/', type =\n'source')\n\n\nlibrary(fivethirtyeight)\ndata(\"daily_show_guests\")\ndaily &lt;- daily_show_guests\n\nIn analyzing this data, you’ll need to determine if and when the data needs to be reshaped.\nPart a\nIdentify the 15 guests that appeared the most. (This isn’t a very diverse guest list!)\n\ndaily |&gt; \n  count(raw_guest_list) |&gt; \n  arrange(desc(n)) |&gt; \n  head(15)\n\n# A tibble: 15 × 2\n   raw_guest_list        n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Fareed Zakaria       19\n 2 Denis Leary          17\n 3 Brian Williams       16\n 4 Paul Rudd            13\n 5 Ricky Gervais        13\n 6 Tom Brokaw           12\n 7 Bill O'Reilly        10\n 8 Reza Aslan           10\n 9 Richard Lewis        10\n10 Will Ferrell         10\n11 Sarah Vowell          9\n12 Adam Sandler          8\n13 Ben Affleck           8\n14 Louis C.K.            8\n15 Maggie Gyllenhaal     8\n\n\nPart b\nCHALLENGE: Create the following data set containing 19 columns:\n\nThe first column should have the 15 guests with the highest number of total appearances on the show, listed in descending order of number of appearances.\n17 columns should show the number of appearances of the corresponding guest in each year from 1999 to 2015 (one per column).\nAnother column should show the total number of appearances for the corresponding guest over the entire duration of the show.\n\nThere are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\n\ndaily |&gt; \n  count(year, raw_guest_list) |&gt; \n  group_by(raw_guest_list) |&gt; \n  mutate(total = sum(n)) |&gt;\n  pivot_wider(names_from = year, \n              values_from = n,\n              values_fill = 0) |&gt; \n  arrange(desc(total)) |&gt; \n  head(15)\n\n# A tibble: 15 × 19\n# Groups:   raw_guest_list [15]\n   raw_guest_list  total `1999` `2000` `2001` `2002` `2003` `2004` `2005` `2006`\n   &lt;chr&gt;           &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 Fareed Zakaria     19      0      0      1      0      1      2      2      2\n 2 Denis Leary        17      1      0      1      2      1      0      0      1\n 3 Brian Williams     16      0      0      0      0      1      1      2      1\n 4 Paul Rudd          13      1      0      1      1      1      1      1      0\n 5 Ricky Gervais      13      0      0      0      0      0      0      1      2\n 6 Tom Brokaw         12      0      0      0      1      0      2      1      0\n 7 Richard Lewis      10      1      0      2      2      1      1      0      0\n 8 Will Ferrell       10      0      1      1      0      1      1      1      1\n 9 Bill O'Reilly      10      0      0      1      1      0      1      1      0\n10 Reza Aslan         10      0      0      0      0      0      0      1      2\n11 Sarah Vowell        9      0      0      0      1      0      1      1      1\n12 Adam Sandler        8      1      2      0      1      0      0      0      1\n13 Ben Affleck         8      0      0      0      0      2      0      0      1\n14 Maggie Gyllenh…     8      0      0      0      0      1      0      1      1\n15 Louis C.K.          8      0      0      0      0      0      0      0      1\n# ℹ 9 more variables: `2007` &lt;int&gt;, `2008` &lt;int&gt;, `2009` &lt;int&gt;, `2010` &lt;int&gt;,\n#   `2011` &lt;int&gt;, `2012` &lt;int&gt;, `2013` &lt;int&gt;, `2014` &lt;int&gt;, `2015` &lt;int&gt;\n\n\nHINTS: There are lots of ways to do this. You don’t necessarily need all of these hints.\n\nFirst obtain the number of times a guest appears each year.\nAdd a new column which includes the total number of times a guest appears across all years.\nPivot (longer or wider?). When you do, use values_fill = 0 to replace NA values with 0.\nArrange, then and keep the top 15.\nPart c\nLet’s recreate the first figure from the article. This groups all guests into 3 broader occupational categories. However, our current data has 18 categories:\n\ndaily |&gt; \n  count(group)\n\n# A tibble: 18 × 2\n   group              n\n   &lt;chr&gt;          &lt;int&gt;\n 1 Academic         103\n 2 Acting           930\n 3 Advocacy          24\n 4 Athletics         52\n 5 Business          25\n 6 Clergy             8\n 7 Comedy           150\n 8 Consultant        18\n 9 Government        40\n10 Media            751\n11 Military          16\n12 Misc              45\n13 Musician         123\n14 Political Aide    36\n15 Politician       308\n16 Science           28\n17 media              5\n18 &lt;NA&gt;              31\n\n\nLet’s define a new dataset that includes a new variable, broad_group, that buckets these 18 categories into the 3 bigger ones used in the article. And get rid of any rows missing information on broad_group. You’ll learn the code soon! For now, just run this chunk:\n\nplot_data &lt;- daily |&gt; \n  mutate(broad_group = case_when(\n    group %in% c(\"Acting\", \"Athletics\", \"Comedy\", \"Musician\") ~ \"Acting, Comedy & Music\",\n    group %in% c(\"Media\", \"media\", \"Science\", \"Academic\", \"Consultant\", \"Clergy\") ~ \"Media\",\n    group %in% c(\"Politician\", \"Political Aide\", \"Government\", \"Military\", \"Business\", \"Advocacy\") ~ \"Government and Politics\",\n    .default = NA\n  )) |&gt; \n  filter(!is.na(broad_group))\n\nNow, using the broad_group variable in plot_data, recreate the graphic from the article, with three different lines showing the fraction of guests in each group over time. Note: You’ll have to wrangle the data first.\n\nplot_data |&gt;\n  group_by(year, broad_group) |&gt;\n  summarise(n = n()) |&gt;\n  mutate(freq = n / sum(n)) |&gt; \n  ggplot(aes(y = freq, x = year, color = broad_group)) + \n    geom_line()\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html#footnotes",
    "href": "ica/ica-reshaping.html#footnotes",
    "title": "\n17  Reshaping\n",
    "section": "",
    "text": "Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.↩︎",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html",
    "href": "ica/ica-joining.html",
    "title": "\n18  Joining\n",
    "section": "",
    "text": "18.1 Exercises",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html#exercises",
    "href": "ica/ica-joining.html#exercises",
    "title": "\n18  Joining\n",
    "section": "",
    "text": "Exercise 1: Where are my keys?\nPart a\nDefine two new datasets, with different students and courses:\n\nstudents_2 &lt;- data.frame(\n  student = c(\"D\", \"E\", \"F\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\")\n)\n\n# Check it out\nstudents_2\n\n  student    class\n1       D COMP 101\n2       E BIOL 101\n3       F POLI 101\n\nenrollments_2 &lt;- data.frame(\n  course = c(\"ART 101\", \"BIOL 101\", \"COMP 101\"),\n  enrollment = c(18, 20, 19)\n)\n\n# Check it out\nenrollments_2\n\n    course enrollment\n1  ART 101         18\n2 BIOL 101         20\n3 COMP 101         19\n\n\nTo connect the course enrollments to the students’ courses, try do a left_join(). You get an error! Identify the problem by reviewing the error message and the datasets we’re trying to join.\n\n# eval = FALSE: don't evaluate this chunk when knitting. it produces an error.\n#students_2 |&gt; \n # left_join(enrollments_2)\n\nPart b\nThe problem is that course name, the key or variable that links these two datasets, is labeled differently: class in the students_2 data and course in the enrollments_2 data. Thus we have to specify these keys in our code:\n\nstudents_2 |&gt; \n  left_join(enrollments_2, join_by(class == course))\n\n  student    class enrollment\n1       D COMP 101         19\n2       E BIOL 101         20\n3       F POLI 101         NA\n\n\n\n# The order of the keys is important:\n# join_by(\"left data key\" == \"right data key\")\n# The order is mixed up here, thus we get an error:\n#students_2 |&gt; \n # left_join(enrollments_2, join_by(course == class))\n\nPart c\nDefine another set of fake data which adds grade information:\n\n# Add student grades in each course\nstudents_3 &lt;- data.frame(\n  student = c(\"Y\", \"Y\", \"Z\", \"Z\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\", \"COMP 101\"),\n  grade = c(\"B\", \"S\", \"C\", \"A\")\n)\n\n# Check it out\nstudents_3\n\n  student    class grade\n1       Y COMP 101     B\n2       Y BIOL 101     S\n3       Z POLI 101     C\n4       Z COMP 101     A\n\n# Add average grades in each course\nenrollments_3 &lt;- data.frame(\n  class = c(\"ART 101\", \"BIOL 101\",\"COMP 101\"),\n  grade = c(\"B\", \"A\", \"A-\"),\n  enrollment = c(20, 18, 19)\n)\n\n# Check it out\nenrollments_3\n\n     class grade enrollment\n1  ART 101     B         20\n2 BIOL 101     A         18\n3 COMP 101    A-         19\n\n\nTry doing a left_join() to link the students’ classes to their enrollment info. Did this work? Try and figure out the culprit by examining the output.\n\nstudents_3 |&gt; \n  left_join(enrollments_3)\n\nJoining with `by = join_by(class, grade)`\n\n\n  student    class grade enrollment\n1       Y COMP 101     B         NA\n2       Y BIOL 101     S         NA\n3       Z POLI 101     C         NA\n4       Z COMP 101     A         NA\n\n\nPart d\nThe issue here is that our datasets have 2 column names in common: class and grade. BUT grade is measuring 2 different things here: individual student grades in students_3 and average student grades in enrollments_3. Thus it doesn’t make sense to try to join the datasets with respect to this variable. We can again solve this by specifying that we want to join the datasets using the class variable as a key. What are grade.x and grade.y?\n\nstudents_3 |&gt; \n  left_join(enrollments_3, join_by(class == class))\n\n  student    class grade.x grade.y enrollment\n1       Y COMP 101       B      A-         19\n2       Y BIOL 101       S       A         18\n3       Z POLI 101       C    &lt;NA&gt;         NA\n4       Z COMP 101       A      A-         19\n\n\nExercise 2: More small practice\nBefore applying these ideas to bigger datasets, let’s practice identifying which join is appropriate in different scenarios. Define the following fake data on voters (people who have voted) and contact info for voting age adults (people who could vote):\n\n# People who have voted\nvoters &lt;- data.frame(\n  id = c(\"A\", \"D\", \"E\", \"F\", \"G\"),\n  times_voted = c(2, 4, 17, 6, 20)\n)\n\nvoters\n\n  id times_voted\n1  A           2\n2  D           4\n3  E          17\n4  F           6\n5  G          20\n\n# Contact info for voting age adults\ncontact &lt;- data.frame(\n  name = c(\"A\", \"B\", \"C\", \"D\"),\n  address = c(\"summit\", \"grand\", \"snelling\", \"fairview\"),\n  age = c(24, 89, 43, 38)\n)\n\ncontact\n\n  name  address age\n1    A   summit  24\n2    B    grand  89\n3    C snelling  43\n4    D fairview  38\n\n\nUse the appropriate join for each prompt below. In each case, think before you type:\n\nWhat dataset goes on the left?\nWhat do you want the resulting dataset to look like? How many rows and columns will it have?\n\n\n# 1. We want contact info for people who HAVEN'T voted\n\n\n# 2. We want contact info for people who HAVE voted\ncontact|&gt;\n  inner_join(voters, join_by(name==id))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    D fairview  38           4\n\n# 3. We want any data available on each person\ncontact|&gt;\n  full_join(voters, join_by(name==id))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n5    E     &lt;NA&gt;  NA          17\n6    F     &lt;NA&gt;  NA           6\n7    G     &lt;NA&gt;  NA          20\n\n# 4. When possible, we want to add contact info to the voting roster\ncontact|&gt;\n  left_join(voters, join_by(name==id))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n\n\nExercise 3: Bigger datasets\nLet’s apply these ideas to some bigger datasets. In grades, each row is a student-class pair with information on:\n\n\nsid = student ID\n\ngrade = student’s grade\n\nsessionID = an identifier of the class section\n\n\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nIn courses, each row corresponds to a class section with information on:\n\n\nsessionID = an identifier of the class section\n\ndept = department\n\nlevel = course level (eg: 100)\n\nsem = semester\n\nenroll = enrollment (number of students)\n\niid = instructor ID\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\nUse R code to take a quick glance at the data.\n\n# How many observations (rows) and variables (columns) are there in the grades data?\nnrow(grades)\n\n[1] 5844\n\nncol(grades)\n\n[1] 3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\nnrow(courses)\n\n[1] 1718\n\nncol(courses)\n\n[1] 6\n\n\nExercise 4: Class size\nHow big are the classes?\nPart a\nBefore digging in, note that some courses are listed twice in the courses data:\n\ncourses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\n     sessionID n\n1  session2047 2\n2  session2067 2\n3  session2448 2\n4  session2509 2\n5  session2541 2\n6  session2824 2\n7  session2826 2\n8  session2862 2\n9  session2897 2\n10 session3046 2\n11 session3057 2\n12 session3123 2\n13 session3243 2\n14 session3257 2\n15 session3387 2\n16 session3400 2\n17 session3414 2\n18 session3430 2\n19 session3489 2\n20 session3524 2\n21 session3629 2\n22 session3643 2\n23 session3821 2\n\n\nIf we pick out just 1 of these, we learn that some courses are cross-listed in multiple departments:\n\ncourses |&gt; \n  filter(sessionID == \"session2047\")\n\n    sessionID dept level    sem enroll     iid\n1 session2047    g   100 FA2001     12 inst436\n2 session2047    m   100 FA2001     28 inst436\n\n\nFor our class size exploration, obtain the total enrollments in each sessionID, combining any cross-listed sections. Save this as courses_combined. NOTE: There’s no joining to do here!\n\n courses_combined &lt;- courses |&gt; \n   group_by(sessionID) |&gt; \n   summarise(enroll = sum(enroll))\n\n# Check that this has 1695 rows and 2 columns\n dim(courses_combined)\n\n[1] 1695    2\n\n\nPart b\nLet’s first examine the question of class size from the administration’s viewpoint. To this end, calculate the median class size across all class sections. (The median is the middle or 50th percentile. Unlike the mean, it’s not skewed by outliers.) THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\nmedian_class_size &lt;- median(courses_combined$enroll)\n\nPart c\nBut how big are classes from the student perspective? To this end, calculate the median class size for each individual student. Once you have the correct output, store it as student_class_size. THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\nstudent_class_size&lt;- courses_combined |&gt;\n  left_join(grades, join_by(sessionID)) |&gt;\n  group_by(sid) |&gt;\n  summarise(median_class_size = median(enroll))\n\nPart d\nThe median class size varies from student to student. To get a sense for the typical student experience and range in student experiences, construct and discuss a histogram of the median class sizes experienced by the students.\n\nlibrary(ggplot2)\n\n\nggplot(student_class_size, aes(x = median_class_size)) + \n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Median Class Sizes (Student Perspective)\",\n       x = \"Median Class Size\",\n       y = \"Number of Students\") +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise 5: Narrowing in on classes\nPart a\nShow data on the students that enrolled in session1986. THINK FIRST: Which of the 2 datasets do you need to answer this question? One? Both?\n\nstudents_in_session1986 &lt;- courses_combined |&gt; \n  filter(sessionID == \"session1986\")\n\nhead(students_in_session1986)\n\n# A tibble: 1 × 2\n  sessionID   enroll\n  &lt;chr&gt;        &lt;int&gt;\n1 session1986     10\n\n\nPart b\nBelow is a dataset with all courses in department E:\n\ndept_E &lt;- courses |&gt; \n  filter(dept == \"E\")\n\nhead(dept_E)\n\n    sessionID dept level    sem enroll     iid\n1 session2326    E   100 SP2002     20 inst169\n2 session2835    E   100 SP2003     26 inst170\n3 session3104    E   100 SP2004     31 inst170\n4 session3658    E   100 FA2004     18 inst171\n5 session3798    E   100 FA2004     13 inst171\n6 session3799    E   100 FA2004     11 inst173\n\n\nWhat students enrolled in classes in department E? (We just want info on the students, not the classes.)\n\nstudents_departments &lt;- grades |&gt;\n  inner_join(dept_E, by = \"sessionID\") |&gt;\n  select(sid, dept)\n\nhead(students_departments)\n\n     sid dept\n1 S31245    E\n2 S31470    E\n3 S31470    E\n4 S31470    E\n5 S31938    E\n6 S31968    E\n\n\nExercise 6: All the wrangling\nUse all of your wrangling skills to answer the following prompts! THINK FIRST:\n\nThink about what tables you might need to join (if any). Identify the corresponding variables to match.\nYou’ll need an extra table to convert grades to grade point averages:\n\n\ngpa_conversion &lt;- tibble(\n  grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"AU\", \"S\"), \n  gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0, NA, NA)\n)\n\ngpa_conversion\n\n# A tibble: 15 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0  \n14 AU     NA  \n15 S      NA  \n\n\nPart a\nHow many total student enrollments are there in each department? Order from high to low.\n\nstudent_enrollments &lt;- courses |&gt;\n  select(dept, enroll) |&gt;\n  group_by(dept) |&gt;\n  summarise(total_enrollment = sum(enroll)) |&gt;\n  arrange(desc(total_enrollment))\n\nPart b\nWhat’s the grade-point average (GPA) for each student?\n\nstudent_gpa &lt;- grades |&gt;\n  left_join(gpa_conversion, by = \"grade\") |&gt;\n  group_by(sid) |&gt;\n  summarise(gpa = mean(gp))\n\nPart c\nWhat’s the median GPA across all students?\n\nstudent_gpa |&gt; \n  summarise(median_gpa = median(gpa, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median_gpa\n       &lt;dbl&gt;\n1       3.43\n\n\nPart d\nWhat fraction of grades are below B+?\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  mutate(below_b_plus = (gp &lt; 3.3)) |&gt; \n  summarize(mean(below_b_plus, na.rm = TRUE))\n\nJoining with `by = join_by(grade)`\n\n\n  mean(below_b_plus, na.rm = TRUE)\n1                        0.2834776\n\n\nPart e\nWhat’s the grade-point average for each instructor? Order from low to high.\n\ncross_listed &lt;- courses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\ngrades |&gt; \n  anti_join(cross_listed) |&gt; \n  inner_join(courses) |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(dept) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)\n\nJoining with `by = join_by(sessionID)`\nJoining with `by = join_by(sessionID)`\nJoining with `by = join_by(grade)`\n\n\n# A tibble: 39 × 2\n   dept    gpa\n   &lt;chr&gt; &lt;dbl&gt;\n 1 o      3.08\n 2 M      3.10\n 3 K      3.17\n 4 G      3.18\n 5 B      3.2 \n 6 J      3.22\n 7 T      3.23\n 8 b      3.25\n 9 F      3.30\n10 d      3.31\n# ℹ 29 more rows\n\n\nPart f\nCHALLENGE: Estimate the grade-point average for each department, and sort from low to high. NOTE: Don’t include cross-listed courses. Students in cross-listed courses could be enrolled under either department, and we do not know which department to assign the grade to. HINT: You’ll need to do multiple joins.\n\ncross_listed &lt;- courses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\ngrades |&gt; \n  anti_join(cross_listed) |&gt; \n  inner_join(courses) |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(dept) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html",
    "href": "ica/ica-factors.html",
    "title": "\n19  Factors\n",
    "section": "",
    "text": "19.1 Exercises\nThe exercises revisit our grades data:\n# Get rid of some duplicate rows!\n#grades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") |&gt; \n #distinct(sid, sessionID, .keep_all = TRUE)\n\n# Check it out\n#head(grades)\nWe’ll explore the number of times each grade was assigned:\n#grade_distribution &lt;- grades |&gt; \n # count(grade)\n\n#head(grade_distribution)",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html#exercises",
    "href": "ica/ica-factors.html#exercises",
    "title": "\n19  Factors\n",
    "section": "",
    "text": "Exercise 1: Changing Order\nCheck out a column plot of the number of times each grade was assigned during the study period. This is similar to a bar plot, but where we define the height of a bar according to variable in our dataset.\n\n#grade_distribution |&gt; \n  #ggplot(aes(x = grade, y = n)) +\n    #geom_col()\n\nThe order of the grades is goofy! Construct a new column plot, manually reordering the grades from high (A) to low (NC) with “S” and “AU” at the end:\n\n#grade_distribution |&gt;\n # mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n # ggplot(aes(x = grade, y = n)) +\n   # geom_col()\n\nConstruct a new column plot, reordering the grades in ascending frequency (i.e. how often the grades were assigned):\n\n# grade_distribution |&gt;\n   #mutate(grade = fct_reorder(grade, n)) |&gt;\n   #ggplot(aes(x = grade, y = n)) +\n     #geom_col()\n\nConstruct a new column plot, reordering the grades in descending frequency (i.e. how often the grades were assigned):\n\n# grade_distribution |&gt;\n # mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n # ggplot(aes(x = grade, y = n)) +\n #   geom_col()\n\nExercise 2: Changing Factor Level Labels\nIt may not be clear what “AU” and “S” stand for. Construct a new column plot that renames these levels “Audit” and “Satisfactory”, while keeping the other grade labels the same and in a meaningful order:\n\n #grade_distribution |&gt;\n  # mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  # mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;  # Multiple pieces go into the last 2 blanks\n # ggplot(aes(x = grade, y = n)) +\n   # geom_col()",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html",
    "href": "ica/ica-strings.html",
    "title": "\n20  Strings\n",
    "section": "",
    "text": "20.1 Exercises",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html#exercises",
    "href": "ica/ica-strings.html#exercises",
    "title": "\n20  Strings\n",
    "section": "",
    "text": "Exercise 1: Time slots\nThe courses data includes actual data scraped from Mac’s class schedule. (Thanks to Prof Leslie Myint for the scraping code!!)\nIf you want to learn how to scrape data, take COMP/STAT 212, Intermediate Data Science! NOTE: For simplicity, I removed classes that had “TBA” for the days.\n\ncourses &lt;- read.csv(\"https://mac-stat.github.io/data/registrar.csv\")\n\n# Check it out\nhead(courses)\n\n       number   crn                                                name  days\n1 AMST 112-01 10318         Introduction to African American Literature M W F\n2 AMST 194-01 10073              Introduction to Asian American Studies M W F\n3 AMST 194-F1 10072 What’s After White Empire - And Is It Already Here?  T R \n4 AMST 203-01 10646 Politics and Inequality: The American Welfare State M W F\n5 AMST 205-01 10842                         Trans Theories and Politics  T R \n6 AMST 209-01 10474                   Civil Rights in the United States   W  \n             time      room             instructor avail_max\n1 9:40 - 10:40 am  MAIN 009       Daylanne English    3 / 20\n2  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4 / 16\n3  3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0 / 14\n4 9:40 - 10:40 am  CARN 305          Lesley Lavery    3 / 25\n5  3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2 / 20\n6 7:00 - 10:00 pm  MAIN 010         Walter Greason   -1 / 15\n\n\nUse our more familiar wrangling tools to warm up.\n\n# Construct a table that indicates the number of classes offered in each day/time slot\n# Print only the 6 most popular time slots\n\n#courses |&gt;\n # count(days, time) |&gt;\n  #arrange(desc(n)) |&gt;\n #head()\n\n\n20.1.1 Exercise 2: Prep the data\nStore this as courses_clean so that you can use it later.\n\n#courses_clean &lt;- courses |&gt;\n # separate(avail_max,c(\"avail\", \"max\"),sep=\"/\") |&gt;\n  #mutate(enroll=as.numeric(max)-as.numeric(avail)) |&gt;\n  #separate(number,c(\"dept\", \"number\", \"section\"))\n  \n#head(courses_clean)\n\nExercise 3: Courses by department\nUsing courses_clean…\n\n# Identify the 6 departments that offered the most sections\n#courses_clean |&gt;\n # count(dept) |&gt;\n  #arrange(desc(n)) |&gt;\n  #head()\n\n# Identify the 6 departments with the longest average course titles\n#courses_clean |&gt; \n # mutate(length = str_length(name)) |&gt; \n # group_by(dept) |&gt; \n  #summarize(avg_length = mean(length)) |&gt; \n  #arrange(desc(avg_length)) |&gt; \n  #head()\n\nExercise 4: STAT courses\nPart a\nGet a subset of courses_clean that only includes courses taught by Alicia Johnson.\n\n#courses_clean |&gt;\n # filter(str_detect(instructor, \"Alicia Johnson\"))\n\n\n#courses_clean |&gt;\n # filter(str_detect(instructor, \"Bridgit Jordan\"))\n\nPart b\nCreate a new dataset from courses_clean, named stat, that only includes STAT sections. In this dataset:\n\n\nIn the course names:\n\nRemove “Introduction to” from any name.\nShorten “Statistical” to “Stat” where relevant.\n\n\nDefine a variable that records the start_time for the course.\nKeep only the number, name, start_time, enroll columns.\nThe result should have 19 rows and 4 columns.\n\n\n#stat&lt;- courses_clean |&gt;\n # filter(dept==\"STAT\") |&gt;\n  #mutate(name=str_replace(name, \"Introduction to\", \"\")) |&gt;\n  #mutate(name=str_replace(name,\"Statistical\", \"Stat\")) |&gt;\n  #mutate(start_time=str_sub(time,1,5)) |&gt;\n  #select(number, name, start_time, enroll)\n\n#dim(stat)\n\nExercise 5: More cleaning\nIn the next exercises, we’ll dig into enrollments. Let’s get the data ready for that analysis here. Make the following changes to the courses_clean data. Because they have different enrollment structures, and we don’t want to compare apples and oranges, remove the following:\n\nall sections in PE and INTD (interdisciplinary studies courses)\nall music ensembles and dance practicums, i.e. all MUSI and THDA classes with numbers less than 100. HINT: !(dept == \"MUSI\" & as.numeric(number) &lt; 100)\nall lab sections. Be careful which variable you use here. For example, you don’t want to search by “Lab” and accidentally eliminate courses with words such as “Labor”.\n\nSave the results as enrollments (don’t overwrite courses_clean).\n\n#enrollments &lt;- courses_clean |&gt;\n  #filter(dept!=\"PE\", dept!=\"INTD\") |&gt;\n  #filter(!(dept==\"MUSI\" &as.numeric(number)&lt;100)) |&gt;\n  #filter(!(dept==\"THDA\" &as.numeric(number)&lt;100)) |&gt;\n  #filter(!str_detect(section,\"L\"))\n\n#head(enrollments)\n\nExercise 6: Enrollment & departments\nExplore enrollments by department. You decide what research questions to focus on. Use both visual and numerical summaries.\nExercise 7: Enrollment & faculty\nLet’s now explore enrollments by instructor. In doing so, we have to be cautious of cross-listed courses that are listed under multiple different departments. Uncomment the code lines in the chunk below for an example.\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n# enrollments |&gt;\n#   filter(dept %in% c(\"STAT\", \"COMP\"), number == 112, section == \"01\")\n\nNotice that these are the exact same section! In order to not double count an instructor’s enrollments, we can keep only the courses that have distinct() combinations of days, time, instructor values. Uncomment the code lines in the chunk below.\n\n# enrollments_2 &lt;- enrollments |&gt; \n#   distinct(days, time, instructor, .keep_all = TRUE)\n\n# NOTE: By default this keeps the first department alphabetically\n# That's fine because we won't use this to analyze department enrollments!\n# enrollments_2 |&gt; \n#   filter(instructor == \"Brianna Heggeseth\", name == \"Introduction to Data Science\")\n\nNow, explore enrollments by instructor. You decide what research questions to focus on. Use both visual and numerical summaries.\nCAVEAT: The above code doesn’t deal with co-taught courses that have more than one instructor. Thus instructors that co-taught are recorded as a pair, and their co-taught enrollments aren’t added to their total enrollments. This is tough to get around with how the data were scraped as the instructor names are smushed together, not separated by a comma!\nOptional extra practice\n\n# Make a bar plot showing the number of night courses by day of the week\n# Use courses_clean\n\nDig Deeper: regex\nExample 4 gave 1 small example of a regular expression.\nThese are handy when we want process a string variable, but there’s no consistent pattern by which to do this. You must think about the structure of the string and how you can use regular expressions to capture the patterns you want (and exclude the patterns you don’t want).\nFor example, how would you describe the pattern of a 10-digit phone number? Limit yourself to just a US phone number for now.\n\nThe first 3 digits are the area code.\nThe next 3 digits are the exchange code.\nThe last 4 digits are the subscriber number.\n\nThus, a regular expression for a US phone number could be:\n\n\n[:digit:]{3}-[:digit:]{3}-[:digit:]{4} which limits you to XXX-XXX-XXXX pattern or\n\n\\\\([:digit:]{3}\\\\) [:digit:]{3}-[:digit:]{4} which limits you to (XXX) XXX-XXXX pattern or\n\n[:digit:]{3}\\\\.[:digit:]{3}\\\\.[:digit:]{4} which limits you to XXX.XXX.XXXX pattern\n\nThe following would include the three patterns above in addition to the XXXXXXXXXX pattern (no dashes or periods): - [\\\\(]*[:digit:]{3}[-.\\\\)]*[:digit:]{3}[-.]*[:digit:]{4}\nIn order to write a regular expression, you first need to consider what patterns you want to include and exclude.\nWork through the following examples, and the tutorial after them to learn about the syntax.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "ica/ica-data_import.html",
    "href": "ica/ica-data_import.html",
    "title": "\n21  Data Import\n",
    "section": "",
    "text": "21.1 Exercises\nSuppose our goal is to work with data on movie reviews, and that we’ve already gone through the work to find a dataset. The imdb_5000_messy.csv file is posted on Moodle. Let’s work with it!",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "ica/ica-data_import.html#exercises",
    "href": "ica/ica-data_import.html#exercises",
    "title": "\n21  Data Import\n",
    "section": "",
    "text": "Exercise 1: Save Data Locally\nPart a\nOn your laptop:\n\nDownload the “imdb_5000_messy.csv” file from Moodle\nMove it to the data folder in your portfolio repository\nPart b\nHot tip: After saving your data file, it’s important to record appropriate citations and info in either a new qmd (eg: “imdb_5000_messy_README.qmd”) or in the qmd where you’ll analyze the data. These citations should include:\n\nthe data source, i.e. where you found the data\nthe data creator, i.e. who / what group collected the original data\npossibly a data codebook, i.e. descriptions of the data variables\n\nTo this end, check out where we originally got our IMDB data:\nhttps://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata\nAfter visiting that website, take some quick notes here on the data source and creator.\nExercise 2: Import Data to RStudio\nNow that we have a local copy of our data file, let’s get it into RStudio! Remember that this process depends on 2 things: the file type and location. Since our file type is a csv, we can import it using read_csv(). But we have to supply the file location through a file path. To this end, we can either use an absolute file path or a relative file path.\nPart a\nAn absolute file path describes the location of a file starting from the root or home directory. How we refer to the user root directory depends upon your machine:\n\nOn a Mac: ~\n\nOn Windows: typically C:\\\n\n\nThen the complete file path to the IMDB data file in the data folder, depending on your machine an where you created your portfolio project, can be:\n\nOn a Mac: ~/Desktop/portfolio/data/imdb_5000_messy.csv\n\nOn Windows: C:\\Desktop\\portfolio\\data\\imdb_5000_messy.csv or C:\\\\Desktop\\\\portfolio\\\\data\\\\imdb_5000_messy.csv\n\n\nPutting this together, use read_csv() with the appropriate absolute file path to import your data into RStudio. Save this as imdb_messy.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n imdb_messy &lt;- read_csv(\"../data/imdb_5000_messy.csv\")\n\nNew names:\nRows: 5043 Columns: 29\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(12): color, director_name, actor_2_name, genres, actor_1_name, movie_ti... dbl\n(17): ...1, num_critic_for_reviews, duration, director_facebook_likes, a...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nPart b\nAbsolute file paths can get really long, depending upon our number of sub-folders, and they should not be used when sharing code with other and instead relative file paths should be used. A relative file path describes the location of a file from the current “working directory”, i.e. where RStudio would currently look for on your computer. Check what your working directory is inside this qmd:\n\n# This should be the folder where you stored this qmd!\ngetwd()\n\n[1] \"/Users/marthamiller/Desktop/Macalester Courses/Data Science 112/GitHub/portfolio-marthamiller14/ica\"\n\n\nNext, check what the working directory is for the console by typing getwd() in the console. This is probably different, meaning that the relative file paths that will work in your qmd won’t work in the console! You can either exclusively work inside your qmd, or change the working directory in your console, by navigating to the following in the upper toolbar: Session &gt; Set Working Directory &gt; To Source File location.\nPart c\nAs a good practice, we created a data folder and saved our data file (imdb_5000_messy.csv) into.\nSince our .qmd analysis and .csv data live in the same project, we don’t have to write out absolute file paths that go all the way to the root directory. We can use relative file paths that start from where our code file exists to where the data file exist:\n\nOn a Mac: ../data/imdb_5000_messy.csv\n\nOn Windows: ..\\data\\imdb_5000_messy.csv or ..\\\\data\\\\imdb_5000_messy.csv\n\n\nNOTE: .. means go up one level in the file hierarchy, ie, go to the parent folder/directory.\nPutting this together, use read_csv() with the appropriate relative file path to import your data into RStudio. Save this as imdb_temp (temp for “temporary”). Convince yourself that this worked, i.e. you get the same dataset as imdb_messy.\n\nimdb_temp &lt;- read_csv(\"../data/imdb_5000_messy.csv\")\n\nNew names:\nRows: 5043 Columns: 29\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(12): color, director_name, actor_2_name, genres, actor_1_name, movie_ti... dbl\n(17): ...1, num_critic_for_reviews, duration, director_facebook_likes, a...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\n\n\n\n\n\nFile Paths\n\n\n\nAbsolute file paths should be used when referring to files hosed on the web, eg, https://mac-stat.github.io/data/kiva_partners2.csv. In all other instances, relative file paths should be used.\n\n\nPart d: OPTIONAL\nSometimes, we don’t want to import the entire dataset. For example, we might want to…\n\nskips some rows, eg, if they’re just “filler”\nonly import the first “n” rows, eg, if the dataset is really large\nonly import a random subset of “n” rows, eg, if the dataset is really large\n\nThe “data import cheat sheet” at the top of this qmd, or Google, are handy resources here. As one example…\n\n# Try importing only the first 5 rows\n read_csv(\"../data/imdb_5000_messy.csv\", n_max = 10)\n\nNew names:\nRows: 10 Columns: 29\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(12): color, director_name, actor_2_name, genres, actor_1_name, movie_ti... dbl\n(17): ...1, num_critic_for_reviews, duration, director_facebook_likes, a...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n# A tibble: 10 × 29\n    ...1 color director_name     num_critic_for_reviews duration\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                              &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 Color James Cameron                        723      178\n 2     2 Color Gore Verbinski                       302      169\n 3     3 Color Sam Mendes                           602      148\n 4     4 Color Christopher Nolan                    813      164\n 5     5 &lt;NA&gt;  Doug Walker                           NA       NA\n 6     6 Color Andrew Stanton                       462      132\n 7     7 Color Sam Raimi                            392      156\n 8     8 Color Nathan Greno                         324      100\n 9     9 Color Joss Whedon                          635      141\n10    10 Color David Yates                          375      153\n# ℹ 24 more variables: director_facebook_likes &lt;dbl&gt;,\n#   actor_3_facebook_likes &lt;dbl&gt;, actor_2_name &lt;chr&gt;,\n#   actor_1_facebook_likes &lt;dbl&gt;, gross &lt;dbl&gt;, genres &lt;chr&gt;,\n#   actor_1_name &lt;chr&gt;, movie_title &lt;chr&gt;, num_voted_users &lt;dbl&gt;,\n#   cast_total_facebook_likes &lt;dbl&gt;, actor_3_name &lt;chr&gt;,\n#   facenumber_in_poster &lt;dbl&gt;, plot_keywords &lt;chr&gt;, movie_imdb_link &lt;chr&gt;,\n#   num_user_for_reviews &lt;dbl&gt;, language &lt;chr&gt;, country &lt;chr&gt;, …\n\n\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\nExercise 3: Check Data\nAfter importing new data into RStudio, you MUST do some quick checks of the data. Here are two first steps that are especially useful.\nPart a\nOpen imdb_messy in the spreadsheet-like viewer by typing View(imdb_messy) in the console. Sort this “spreadsheet” by different variables by clicking on the arrows next to the variable names. Do you notice anything unexpected?\nPart b\nDo a quick summary() of each variable in the dataset. One way to do this is below:\n\nimdb_messy |&gt;\n   mutate(across(where(is.character), as.factor)) |&gt;  # convert characters to factors in order to summarize\n   summary()\n\n      ...1                  color               director_name \n Min.   :   1   B&W            :  10   Steven Spielberg:  26  \n 1st Qu.:1262   Black and White: 199   Woody Allen     :  22  \n Median :2522   color          :  30   Clint Eastwood  :  20  \n Mean   :2522   Color          :4755   Martin Scorsese :  20  \n 3rd Qu.:3782   COLOR          :  30   Ridley Scott    :  17  \n Max.   :5043   NA's           :  19   (Other)         :4834  \n                                       NA's            : 104  \n num_critic_for_reviews    duration     director_facebook_likes\n Min.   :  1.0          Min.   :  7.0   Min.   :    0.0        \n 1st Qu.: 50.0          1st Qu.: 93.0   1st Qu.:    7.0        \n Median :110.0          Median :103.0   Median :   49.0        \n Mean   :140.2          Mean   :107.2   Mean   :  686.5        \n 3rd Qu.:195.0          3rd Qu.:118.0   3rd Qu.:  194.5        \n Max.   :813.0          Max.   :511.0   Max.   :23000.0        \n NA's   :50             NA's   :15      NA's   :104            \n actor_3_facebook_likes          actor_2_name  actor_1_facebook_likes\n Min.   :    0.0        Morgan Freeman :  20   Min.   :     0        \n 1st Qu.:  133.0        Charlize Theron:  15   1st Qu.:   614        \n Median :  371.5        Brad Pitt      :  14   Median :   988        \n Mean   :  645.0        James Franco   :  11   Mean   :  6560        \n 3rd Qu.:  636.0        Meryl Streep   :  11   3rd Qu.: 11000        \n Max.   :23000.0        (Other)        :4959   Max.   :640000        \n NA's   :23             NA's           :  13   NA's   :7             \n     gross                            genres             actor_1_name \n Min.   :      162   Drama               : 236   Robert De Niro:  49  \n 1st Qu.:  5340988   Comedy              : 209   Johnny Depp   :  41  \n Median : 25517500   Comedy|Drama        : 191   Nicolas Cage  :  33  \n Mean   : 48468408   Comedy|Drama|Romance: 187   J.K. Simmons  :  31  \n 3rd Qu.: 62309438   Comedy|Romance      : 158   Bruce Willis  :  30  \n Max.   :760505847   Drama|Romance       : 152   (Other)       :4852  \n NA's   :884         (Other)             :3910   NA's          :   7  \n                    movie_title   num_voted_users   cast_total_facebook_likes\n Ben-Hur                  :   3   Min.   :      5   Min.   :     0           \n Halloween                :   3   1st Qu.:   8594   1st Qu.:  1411           \n Home                     :   3   Median :  34359   Median :  3090           \n King Kong                :   3   Mean   :  83668   Mean   :  9699           \n Pan                      :   3   3rd Qu.:  96309   3rd Qu.: 13756           \n The Fast and the Furious :   3   Max.   :1689764   Max.   :656730           \n (Other)                  :5025                                              \n         actor_3_name  facenumber_in_poster\n Ben Mendelsohn:   8   Min.   : 0.000      \n John Heard    :   8   1st Qu.: 0.000      \n Steve Coogan  :   8   Median : 1.000      \n Anne Hathaway :   7   Mean   : 1.371      \n Jon Gries     :   7   3rd Qu.: 2.000      \n (Other)       :4982   Max.   :43.000      \n NA's          :  23   NA's   :13          \n                                                                           plot_keywords \n based on novel                                                                   :   4  \n 1940s|child hero|fantasy world|orphan|reference to peter pan                     :   3  \n alien friendship|alien invasion|australia|flying car|mother daughter relationship:   3  \n animal name in title|ape abducts a woman|gorilla|island|king kong                :   3  \n assistant|experiment|frankenstein|medical student|scientist                      :   3  \n (Other)                                                                          :4874  \n NA's                                                                             : 153  \n                                             movie_imdb_link\n http://www.imdb.com/title/tt0077651/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0232500/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0360717/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt1976009/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2224026/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2638144/?ref_=fn_tt_tt_1:   3  \n (Other)                                             :5025  \n num_user_for_reviews     language       country       content_rating\n Min.   :   1.0       English :4704   USA    :3807   R        :2118  \n 1st Qu.:  65.0       French  :  73   UK     : 448   PG-13    :1461  \n Median : 156.0       Spanish :  40   France : 154   PG       : 701  \n Mean   : 272.8       Hindi   :  28   Canada : 126   Not Rated: 116  \n 3rd Qu.: 326.0       Mandarin:  26   Germany:  97   G        : 112  \n Max.   :5060.0       (Other) : 160   (Other): 406   (Other)  : 232  \n NA's   :21           NA's    :  12   NA's   :   5   NA's     : 303  \n     budget            title_year   actor_2_facebook_likes   imdb_score   \n Min.   :2.180e+02   Min.   :1916   Min.   :     0         Min.   :1.600  \n 1st Qu.:6.000e+06   1st Qu.:1999   1st Qu.:   281         1st Qu.:5.800  \n Median :2.000e+07   Median :2005   Median :   595         Median :6.600  \n Mean   :3.975e+07   Mean   :2002   Mean   :  1652         Mean   :6.442  \n 3rd Qu.:4.500e+07   3rd Qu.:2011   3rd Qu.:   918         3rd Qu.:7.200  \n Max.   :1.222e+10   Max.   :2016   Max.   :137000         Max.   :9.500  \n NA's   :492         NA's   :108    NA's   :13                            \n  aspect_ratio   movie_facebook_likes\n Min.   : 1.18   Min.   :     0      \n 1st Qu.: 1.85   1st Qu.:     0      \n Median : 2.35   Median :   166      \n Mean   : 2.22   Mean   :  7526      \n 3rd Qu.: 2.35   3rd Qu.:  3000      \n Max.   :16.00   Max.   :349000      \n NA's   :329                         \n\n\nFollow-up:\n\nWhat type of info is provided on quantitative variables? The color, director, actor names, genres.\nWhat type of info is provided on categorical variables? The duration of movies, critic reviews, facebook likes, content rating, budget.\nWhat stands out to you in these summaries? Is there anything you’d need to clean before using this data? I would like to clean out some of the categories like maybe condensing the actors facebooks likes or aspect ratio. Also there are multiple “color” categories.\nExercise 4: Clean Data: Factor Variables 1\nIf you didn’t already in Exercise 3, check out the color variable in the imdb_messy dataset.\n\nWhat’s goofy about this / what do we need to fix?\nMore specifically, what different categories does the color variable take, and how many movies fall into each of these categories?\n\n\nimdb_messy |&gt; \n  count(color)\n\n# A tibble: 6 × 2\n  color               n\n  &lt;chr&gt;           &lt;int&gt;\n1 B&W                10\n2 Black and White   199\n3 COLOR              30\n4 Color            4755\n5 color              30\n6 &lt;NA&gt;               19\n\n\nExercise 5: Clean Data: Factor Variables 2\nWhen working with categorical variables like color, the categories must be “clean”, i.e. consistent and in the correct format. Let’s make that happen.\nPart a\nWe could open the .csv file in, say, Excel or Google sheets, clean up the color variable, save a clean copy, and then reimport that into RStudio. BUT that would be the wrong thing to do. Why is it important to use R code, which we then save inside this qmd, to clean our data?\nPart b\nLet’s use R code to change the color variable so that it appropriately combines the various categories into only 2: Color and Black_White. We’ve learned a couple sets of string-related tools that could be handy here. First, starting with the imdb_messy data, change the color variable using one of the functions we learned in the Factors lesson.\nfct_relevel(), fct_recode(), fct_reorder()\nStore your results in imdb_temp (don’t overwrite imdb_messy). To check your work, print out a count() table of the color variable in imdb_temp.\n\nimbd_temp &lt;- imdb_messy |&gt;\n  mutate(color = fct_recode(color, \n                             \"Color\" = \"COLOR\",\n                             \"Color\" = \"color\",\n                             \"Black_White\" = \"B&W\",\n                             \"B;ack_White\" = \"Black and White\"))\n\nPart c\nRepeat Part b using one of our string functions from the String lesson:\nstr_replace(), str_replace_all(), str_to_lower(), str_sub(), str_length(), str_detect()\n\nimdb_temp &lt;- imdb_messy |&gt;\n  mutate(color = str_replace(color, \"COLOR\", \"Color\"),\n         color = str_replace(color, \"color\", \"Color\"),\n         color = str_replace(color, \"B&W\", \"Black_White\"),\n         color = str_replace(color, \"Black and White\", \"Black_White\"))\n\nimdb_temp |&gt;\n  count(color)\n\n# A tibble: 3 × 2\n  color           n\n  &lt;chr&gt;       &lt;int&gt;\n1 Black_White   209\n2 Color        4815\n3 &lt;NA&gt;           19\n\n\nExercise 6: Clean Data: Missing Data 1\nThroughout these exercises, you’ve probably noticed that there’s a bunch of missing data. This is encoded as NA (not available) in R. There are a few questions to address about missing data:\n\n\nHow many values are missing data? What’s the volume of the missingness?\n\nWhy are some values missing?\n\nWhat should we do about the missing values?\n\nLet’s consider the first 2 questions in this exercise.\nPart a\nAs a first step, let’s simply understand the volume of NAs. Specifically:\n\n# Count the total number of rows in imdb_messy\nnrow(imdb_messy)\n\n[1] 5043\n\n# Then count the number of NAs in each column\ncolSums(is.na(imdb_messy))\n\n                     ...1                     color             director_name \n                        0                        19                       104 \n   num_critic_for_reviews                  duration   director_facebook_likes \n                       50                        15                       104 \n   actor_3_facebook_likes              actor_2_name    actor_1_facebook_likes \n                       23                        13                         7 \n                    gross                    genres              actor_1_name \n                      884                         0                         7 \n              movie_title           num_voted_users cast_total_facebook_likes \n                        0                         0                         0 \n             actor_3_name      facenumber_in_poster             plot_keywords \n                       23                        13                       153 \n          movie_imdb_link      num_user_for_reviews                  language \n                        0                        21                        12 \n                  country            content_rating                    budget \n                        5                       303                       492 \n               title_year    actor_2_facebook_likes                imdb_score \n                      108                        13                         0 \n             aspect_ratio      movie_facebook_likes \n                      329                         0 \n\n# Then count the number of NAs in a specific column\nsum(is.na(imdb_messy$actor_1_facebook_likes))\n\n[1] 7\n\n\nPart b\nAs a second step, let’s think about why some values are missing. Study the individual observations with NAs carefully. Why do you think they are missing? Are certain films more likely to have more NAs than others?\nPart c\nConsider a more specific example. Obtain a dataset of movies that are missing data on actor_1_facebook_likes. Then explain why you think there are NAs. HINT: is.na(___)\n\nimdb_messy |&gt; \n  filter(is.na(actor_1_facebook_likes))\n\n# A tibble: 7 × 29\n   ...1 color director_name     num_critic_for_reviews duration\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                              &lt;dbl&gt;    &lt;dbl&gt;\n1  4503 Color Léa Pool                              23       97\n2  4520 Color Harry Gantz                           12      105\n3  4721 Color U. Roberto Romano                      3       80\n4  4838 Color Pan Nalin                             15      102\n5  4946 Color Amal Al-Agroobi                       NA       62\n6  4947 Color Andrew Berends                        12       90\n7  4991 Color Jem Cohen                             12      111\n# ℹ 24 more variables: director_facebook_likes &lt;dbl&gt;,\n#   actor_3_facebook_likes &lt;dbl&gt;, actor_2_name &lt;chr&gt;,\n#   actor_1_facebook_likes &lt;dbl&gt;, gross &lt;dbl&gt;, genres &lt;chr&gt;,\n#   actor_1_name &lt;chr&gt;, movie_title &lt;chr&gt;, num_voted_users &lt;dbl&gt;,\n#   cast_total_facebook_likes &lt;dbl&gt;, actor_3_name &lt;chr&gt;,\n#   facenumber_in_poster &lt;dbl&gt;, plot_keywords &lt;chr&gt;, movie_imdb_link &lt;chr&gt;,\n#   num_user_for_reviews &lt;dbl&gt;, language &lt;chr&gt;, country &lt;chr&gt;, …\n\n\nThe actor may not have had facebook or the movie not popular enough to have it be a large social media presence.\nExercise 7: Clean Data: Missing Data 2\nNext, let’s think about what to do about the missing values. There is no perfect or universal approach here. Rather, we must think carefully about…\n\nWhy the values are missing?\nWhat we want to do with our data?\nWhat is the impact of removing or replacing missing data on our work / conclusions?\n\nPart a\nCalculate the average duration of a film. THINK: How can we deal with the NA’s?\n\nimdb_messy |&gt; \n  summarize(mean(duration, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  `mean(duration, na.rm = TRUE)`\n                           &lt;dbl&gt;\n1                           107.\n\n\nFollow-up:\nHow are the NAs dealt with here? Did we have to create and save a new dataset in order to do this analysis?\nPart b\nTry out the drop_na() function:\n\n imdb_temp &lt;- drop_na(imdb_messy)\n\ncount(imdb_temp)\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  3756\n\n\nFollow-up questions:\n\nWhat did drop_na() do? How many data points are left? It is down to 3756 points of data.\nIn what situations might this function be a good idea? It shows where ones actually have quantitative or qualitative points.\nIn what situations might this function be a bad idea? If it is reloevant to know where information was not able to be found.\nPart c\ndrop_na() removes data points that have any NA values, even if we don’t care about the variable(s) for which data is missing. This can result in losing a lot of data points that do have data on the variables we actually care about! For example, suppose we only want to explore the relationship between film duration and whether it’s in color. Check out a plot:\n\n ggplot(imdb_messy, aes(x = duration, fill = color)) + \n   geom_density()\n\nWarning: Removed 15 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nFollow-up:\n\nCreate a new dataset with only and all movies that have complete info on duration and color. HINT: You could use !is.na(___) or drop_na() (differently than above)\nUse this new dataset to create a new and improved plot.\nHow many movies remain in your new dataset? Hence why this is better than using the dataset from part b?\n\n\n# Approach 2\nimdb_temp &lt;- imdb_messy |&gt; \n  filter(!is.na(duration), !is.na(color))\ndim(imdb_temp)\n\n[1] 5010   29\n\n# Plot\nggplot(imdb_temp, aes(x = duration, fill = color)) +\n  geom_density()\n\n\n\n\n\n\n\nPart d\nIn some cases, missing data is more non-data than unknown data. For example, the films with NAs for actor_1_facebook_likes actually have 0 Facebook likes–they don’t even have actors! In these cases, we can replace the NAs with a 0. Use the replace_na() function to create a new dataset (imdb_temp) that replaces the NAs in actor_1_facebook_likes with 0. You’ll have to check out the help file for this function.\n\nimdb_temp &lt;- imdb_messy |&gt; \n  mutate(actor_1_facebook_likes =\n         replace_na(actor_1_facebook_likes, 0))\n\nimdb_temp |&gt; \n  summarize(sum(is.na(actor_1_facebook_likes)))\n\n# A tibble: 1 × 1\n  `sum(is.na(actor_1_facebook_likes))`\n                                 &lt;int&gt;\n1                                    0\n\n\nExercise 8: New Data + Projects\nLet’s practice the above ideas while also planting some seeds for the course project. Each group will pick and analyze their own dataset. The people you’re sitting with today aren’t necessarily your project groups! BUT do some brainstorming together:\n\nShare with each other: What are some personal hobbies or passions or things you’ve been thinking about or things you’d like to learn more about? Don’t think too hard about this! Just share what’s at the top of mind today.\nEach individual: Find a dataset online that’s related to one of the topics you shared in the above prompt.\nDiscuss what data you found with your group!\nLoad the data into RStudio, perform some basic checks, and perform some preliminary cleaning, as necessary.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Data Import</span>"
    ]
  }
]